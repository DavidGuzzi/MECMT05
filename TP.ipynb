{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNIVERSIDAD TORCUATO DI TELLA**\n",
    "## **MAESTRÍA EN ECONOMETRÍA**\n",
    "\n",
    "---\n",
    "\n",
    "### **TRABAJO PRÁCTICO DE ECONOMETRÍA**\n",
    "\n",
    "- **Profesor:** González-Rozada, Martín  \n",
    "- **Ayudante:** Lening, Iara  \n",
    "- **Alumno:** Guzzi, David Alexander  (Legajo n°: 24H1970)  \n",
    "\n",
    "**Ciclo Lectivo:** Tercer Trimestre, 2024  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. PROPIEDADES DE MUESTRA FINITA DE FGLS (MCGE).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>u</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44.691948</td>\n",
       "      <td>0.394367</td>\n",
       "      <td>33.147926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.945501</td>\n",
       "      <td>-2.739349</td>\n",
       "      <td>21.417052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.364044</td>\n",
       "      <td>-5.117829</td>\n",
       "      <td>21.773406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>37.251050</td>\n",
       "      <td>1.502358</td>\n",
       "      <td>28.303198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>34.525174</td>\n",
       "      <td>6.038023</td>\n",
       "      <td>30.658162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.409759</td>\n",
       "      <td>0.030947</td>\n",
       "      <td>-0.241246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.352028</td>\n",
       "      <td>-0.504023</td>\n",
       "      <td>7.977600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30.112575</td>\n",
       "      <td>2.001504</td>\n",
       "      <td>23.091564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.797610</td>\n",
       "      <td>3.095406</td>\n",
       "      <td>4.733495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>20.579967</td>\n",
       "      <td>8.232626</td>\n",
       "      <td>21.696599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample  index          x         u          y\n",
       "0       0      0  44.691948  0.394367  33.147926\n",
       "1       0      1  33.945501 -2.739349  21.417052\n",
       "2       0      2  37.364044 -5.117829  21.773406\n",
       "3       0      3  37.251050  1.502358  28.303198\n",
       "4       0      4  34.525174  6.038023  30.658162\n",
       "5       1      0   3.409759  0.030947  -0.241246\n",
       "6       1      1  14.352028 -0.504023   7.977600\n",
       "7       1      2  30.112575  2.001504  23.091564\n",
       "8       1      3   5.797610  3.095406   4.733495\n",
       "9       1      4  20.579967  8.232626  21.696599"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parámetros del modelo\n",
    "beta_0 = -3\n",
    "beta_1 = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas y covarianzas de u\n",
    "n_observations = 5  # Tamaño de cada muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "\n",
    "# Generación de muestras\n",
    "samples = []\n",
    "np.random.seed(3649)  #Últimos 4 números de mi documento de identidad.\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1 * x + u  # Generar y\n",
    "    samples.append(pd.DataFrame({'x': x, 'u': u, 'y': y}))\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples, keys=range(n_samples), names=['sample', 'index']).reset_index()\n",
    "\n",
    "# Mostrar algunas filas\n",
    "all_samples.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma_hat: 1.0190342558308039\n",
      "beta1 (coeficiente de x): 1.049125141002301\n",
      "beta0 (intercepto): 1.1894337904253731\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "\n",
    "x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "x2 = x**2\n",
    "u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "# Creamos un DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'x2': x2,\n",
    "    'u': u,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "# 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "# 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "df['u_hat'] = ols_results.resid\n",
    "df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "# 3. Dada la forma funcional de la heterocedasticidad de White para este modelo, sigma2 ~ x x2, se estima por OLS usando u_hat2 como proxy de sigma2\n",
    "ols_model_aux = sm.OLS(df['u_hat2'], df[['x', 'x2']]) #Ver de agregar intercepto.\n",
    "ols_model_aux_results = ols_model_aux.fit()\n",
    "gamma_hat1 = ols_model_aux_results.params['x']\n",
    "gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "# 4. Usar las estimaciones de la regresión auxiliar y obtener las varianzas ajustadas (no consistentes)\n",
    "df['sigma2_hat'] = ols_model_aux_results.predict(df[['x', 'x2']])\n",
    "\n",
    "# 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por la raíz de sigma2_hat y estimar por OLS\n",
    "df['u_hat2_over_sigma2_hat'] = df['u_hat2'] / df['sigma2_hat']\n",
    "df['x_over_sigma2_hat'] = df['x'] / df['sigma2_hat']\n",
    "df['x2_over_sigma2_hat'] = df['x2'] / df['sigma2_hat']\n",
    "\n",
    "ols_model_aux2 = sm.OLS(df['u_hat2_over_sigma2_hat'], df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']]) #Ver de agregar intercepto.\n",
    "ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "# 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "df['sigma2_tilde'] = ols_model_aux_results2.predict(df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']])\n",
    "df['sigma2_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "# 7. Usar uno sobre sigma tilde como ponderador en la regresión de y sobre x\n",
    "df['y_estrella'] = df['y'] / df['sigma_tilde']\n",
    "df['x_estrella'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "X_estrella = sm.add_constant(df['x_over_sigma_tilde']) \n",
    "final_ols_model = sm.OLS(df['y_estrella'], X_estrella)\n",
    "final_ols_results = final_ols_model.fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated beta_0: 19.1022, beta_1: 0.1914\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "#n_samples = 5000\n",
    "n_observations = 5\n",
    "\n",
    "x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "x2 = x**2\n",
    "u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "# Creamos un DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'x2': x2,\n",
    "    'u': u,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "# 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "# 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "df['u_hat'] = ols_results.resid\n",
    "df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "# 3. Regresión auxiliar: u_hat2 ~ x + x2 (incluyendo intercepto)\n",
    "aux_X = sm.add_constant(df[['x', 'x2']])\n",
    "ols_model_aux = sm.OLS(df['u_hat2'], aux_X)\n",
    "ols_model_aux_results = ols_model_aux.fit()\n",
    "gamma_hat1 = ols_model_aux_results.params['x']\n",
    "gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "# 4. Usar las estimaciones para obtener las varianzas ajustadas\n",
    "df['sigma2_hat'] = ols_model_aux_results.predict(aux_X)\n",
    "\n",
    "# 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por la raíz de sigma2_hat y estimar por OLS\n",
    "df['u_hat2_over_sigma2_hat'] = df['u_hat2'] / df['sigma2_hat']\n",
    "df['x_over_sigma2_hat'] = df['x'] / df['sigma2_hat']\n",
    "df['x2_over_sigma2_hat'] = df['x2'] / df['sigma2_hat']\n",
    "\n",
    "aux_X_2 = sm.add_constant(df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']])\n",
    "ols_model_aux2 = sm.OLS(df['u_hat2_over_sigma2_hat'], aux_X_2)\n",
    "ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "# 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "df['sigma2_tilde'] = ols_model_aux_results2.predict(aux_X_2)\n",
    "df['sigma_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "# 7. Usar uno sobre sigma tilde como ponderador en la regresión de y sobre x\n",
    "df['y_star'] = df['y'] / df['sigma_tilde']\n",
    "df['x_star'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "aux_X_star = sm.add_constant(df['x_star']) \n",
    "final_ols_model = sm.OLS(df['y_star'], aux_X_star)\n",
    "final_ols_results = final_ols_model.fit() \n",
    "\n",
    "# Estimaciones finales\n",
    "beta_0_hat = final_ols_results.params['const']\n",
    "beta_1_hat = final_ols_results.params['x_star']\n",
    "print(f\"Estimated beta_0: {beta_0_hat:.4f}, beta_1: {beta_1_hat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de all_samples:\n",
      "   sample          x          y         u\n",
      "0       0  44.691948  33.147926  0.394367\n",
      "1       0  33.945501  21.417052 -2.739349\n",
      "2       0  37.364044  21.773406 -5.117829\n",
      "3       0  37.251050  28.303198  1.502358\n",
      "4       0  34.525174  30.658162  6.038023\n",
      "\n",
      "Ejemplo de results_df:\n",
      "   sample  beta_0_hat  beta_1_hat  se_beta1_hat\n",
      "0       0   19.102219    0.191446      1.192612\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 1\n",
    "n_observations = 5\n",
    "\n",
    "# Función para realizar FGLS\n",
    "def fgls_estimation(x, y):\n",
    "    # DataFrame para almacenar las variables intermedias\n",
    "    df = pd.DataFrame({'x': x, 'y': y})\n",
    "    df['x2'] = df['x'] ** 2\n",
    "\n",
    "    # 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "    ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "    # 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "    df['u_hat'] = ols_results.resid\n",
    "    df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "    # 3. Dada la forma funcional de la heterocedasticidad de White para este modelo, sigma2 ~ x + x2, se estima por OLS usando u_hat2 como proxy de sigma2\n",
    "    aux_X = sm.add_constant(df[['x', 'x2']])\n",
    "    ols_model_aux = sm.OLS(df['u_hat2'], aux_X)\n",
    "    ols_model_aux_results = ols_model_aux.fit()\n",
    "    gamma_hat1 = ols_model_aux_results.params['x']\n",
    "    gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "    # 4. Usar las estimaciones de la regresión auxiliar y obtener las varianzas ajustadas (no consistentes)\n",
    "    df['sigma2_hat'] = ols_model_aux_results.predict(aux_X)\n",
    "\n",
    "    # 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por la raíz de sigma2_hat y estimar por OLS\n",
    "    df['u_hat2_over_sigma2_hat'] = df['u_hat2'] / df['sigma2_hat']\n",
    "    df['x_over_sigma2_hat'] = df['x'] / df['sigma2_hat']\n",
    "    df['x2_over_sigma2_hat'] = df['x2'] / df['sigma2_hat']\n",
    "\n",
    "    aux_X_2 = sm.add_constant(df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']])\n",
    "    ols_model_aux2 = sm.OLS(df['u_hat2_over_sigma2_hat'], aux_X_2)\n",
    "    ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "    gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "    gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "    # 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "    df['sigma2_tilde'] = ols_model_aux_results2.predict(aux_X_2)\n",
    "    \n",
    "    # Corregir valores negativos o muy pequeños\n",
    "    df['sigma2_tilde'] = df['sigma2_tilde'].clip(lower=1e-10)\n",
    "    df['sigma_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "    # Verificar si hay valores NaN\n",
    "    if df['sigma_tilde'].isna().any():\n",
    "        raise ValueError(\"Se encontraron valores NaN en sigma_tilde\")\n",
    "\n",
    "    # 7. Usar uno sobre sigma tilde como ponderador en la regresión de y ~ x\n",
    "    df['y_star'] = df['y'] / df['sigma_tilde']\n",
    "    df['x_star'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "    aux_X_star = sm.add_constant(df['x_star']) \n",
    "    final_ols_model = sm.OLS(df['y_star'], aux_X_star)\n",
    "    final_ols_results = final_ols_model.fit() \n",
    "\n",
    "    # Estimaciones finales\n",
    "    beta_0_hat = final_ols_results.params['const']\n",
    "    beta_1_hat = final_ols_results.params['x_star']\n",
    "    se_beta1_hat = final_ols_results.bse.iloc[1]\n",
    "\n",
    "    return beta_0_hat, beta_1_hat, se_beta1_hat\n",
    "\n",
    "# Inicializar listas para almacenar resultados\n",
    "samples = []\n",
    "results = []\n",
    "\n",
    "# Generar muestras y aplicar FGLS\n",
    "for i in range(n_samples):\n",
    "    # Generar datos\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "    # Almacenar muestra\n",
    "    samples.append(pd.DataFrame({'sample': i, 'x': x, 'y': y, 'u': u}))\n",
    "\n",
    "    # Aplicar FGLS\n",
    "    beta_0_hat, beta_1_hat, se_beta1_hat = fgls_estimation(x, y)\n",
    "    # t_stat = (beta_1_hat - beta_1_true) / se_beta[1]  # Estadístico t para beta_1\n",
    "    # p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_observations - 2))  # Valor p\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append([i, beta_0_hat, beta_1_hat, se_beta1_hat])\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "# Crear DataFrame con los resultados de FGLS\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    'sample', 'beta_0_hat', 'beta_1_hat', 'se_beta1_hat'\n",
    "])\n",
    "\n",
    "# Mostrar ejemplos de los DataFrames generados\n",
    "print(\"Ejemplo de all_samples:\")\n",
    "print(all_samples.head())\n",
    "print(\"\\nEjemplo de results_df:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>beta_0_hat</th>\n",
       "      <th>beta_1_hat</th>\n",
       "      <th>se_beta1_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19.102219</td>\n",
       "      <td>0.191446</td>\n",
       "      <td>1.192612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample  beta_0_hat  beta_1_hat  se_beta1_hat\n",
       "0       0   19.102219    0.191446      1.192612"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7429979282613803"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['beta_1_hat'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación por MCC (beta_0, beta_1):\n",
      "[-0.10411783 -0.04700603  0.00813422  0.03736534  0.06757768]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definimos parámetros\n",
    "beta_0 = -3\n",
    "beta_1 = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "N = 5  # Observaciones por muestra\n",
    "M = 5000  # Cantidad de muestras\n",
    "\n",
    "# Generación de datos\n",
    "np.random.seed(3649)\n",
    "x = np.random.uniform(1, 50, size=(M, N))  # x ~ U[1, 50]\n",
    "chol_omega = np.linalg.cholesky(omega)     # P = Cholesky de omega\n",
    "u = chol_omega @ np.random.randn(N, M)    # u ~ N(0, omega)\n",
    "u = u.T  # Transpuesta para mantener dimensiones MxN\n",
    "y = beta_0 + beta_1 * x + u               # y_i = beta_0 + beta_1 * x_i + u_i\n",
    "\n",
    "# Transformación para MCC\n",
    "P_inv = np.linalg.inv(chol_omega)         # Inversa de P\n",
    "y_star = y @ P_inv.T                      # y* = P^-1 * y\n",
    "x_star = x @ P_inv.T                      # X* = P^-1 * X\n",
    "x_star = np.hstack([np.ones((M, 1)), x_star])  # Agregar constante a X*\n",
    "\n",
    "# Estimación MCC\n",
    "beta_mcc = np.linalg.inv(x_star.T @ x_star) @ x_star.T @ y_star\n",
    "\n",
    "# Resultados\n",
    "print(\"Estimación por MCC (beta_0, beta_1):\")\n",
    "print(beta_mcc.mean(axis=0))  # Promedio de betas estimadas en todas las muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.41230844, -1.08667462, -0.74686156, -0.56526369, -0.38246838])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_mcc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44.69194777, 33.9455005 , 37.36404406, 37.25104984, 34.52517416])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([np.ones((M, 1)), x]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  0,  0,  0,  0],\n",
       "       [ 0,  9,  0,  0,  0],\n",
       "       [ 0,  0, 16,  0,  0],\n",
       "       [ 0,  0,  0, 25,  0],\n",
       "       [ 0,  0,  0,  0, 36]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0., 0., 0.],\n",
       "       [0., 3., 0., 0., 0.],\n",
       "       [0., 0., 4., 0., 0.],\n",
       "       [0., 0., 0., 5., 0.],\n",
       "       [0., 0., 0., 0., 6.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(omega, chol_omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'beta_0_hat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     beta_hat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(X_tilde\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m X_tilde) \u001b[38;5;241m@\u001b[39m (X_tilde\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m y_tilde)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Almacenar resultados\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend([i, \u001b[43mbeta_0_hat\u001b[49m, beta_1_hat, se_beta1_hat])\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Combinar todas las muestras en un único DataFrame\u001b[39;00m\n\u001b[0;32m     48\u001b[0m all_samples \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(samples)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'beta_0_hat' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "n_observations = 5  # Observaciones por muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "# Matriz omega (varianzas)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "\n",
    "# Descomposición de Cholesky de omega\n",
    "P = np.linalg.cholesky(omega)\n",
    "\n",
    "# Inversa de P\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "# Contenedores para almacenar muestras y estimadores\n",
    "samples = []\n",
    "results = []\n",
    "\n",
    "# Simulación de M muestras\n",
    "for sample_id in range(n_samples):\n",
    "    # 1. Generar datos\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "    # Almacenar muestra\n",
    "    samples.append(pd.DataFrame({'sample': i, 'x': x, 'y': y, 'u': u}))\n",
    "\n",
    "    # 2. Transformar para MCG\n",
    "    y_tilde = P_inv @ y\n",
    "    X = np.column_stack((np.ones(n_observations), x))  # Matriz de diseño original\n",
    "    X_tilde = P_inv @ X\n",
    "\n",
    "    # 3. Ajustar modelo transformado con MCO\n",
    "    beta_hat = np.linalg.inv(X_tilde.T @ X_tilde) @ (X_tilde.T @ y_tilde)\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append([i, beta_0_hat, beta_1_hat, se_beta1_hat])\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "# Crear DataFrame con los resultados de FGLS\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    'sample', 'beta_0_hat', 'beta_1_hat', 'se_beta1_hat'\n",
    "])\n",
    "\n",
    "# Mostrar ejemplos de los DataFrames generados\n",
    "print(\"Ejemplo de all_samples:\")\n",
    "print(all_samples.head())\n",
    "print(\"\\nEjemplo de results_df:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de all_samples:\n",
      "   sample          x          y         u\n",
      "0       0  44.691948  33.147926  0.394367\n",
      "1       0  33.945501  21.417052 -2.739349\n",
      "2       0  37.364044  21.773406 -5.117829\n",
      "3       0  37.251050  28.303198  1.502358\n",
      "4       0  34.525174  30.658162  6.038023\n",
      "\n",
      "Ejemplo de results_df:\n",
      "   sample  beta_0_hat  beta_1_hat  se_beta1_hat\n",
      "0       0  -11.213024    0.989465      0.301298\n",
      "1       1   -3.104349    0.893636      0.119634\n",
      "2       2    1.983310    0.630313      0.192682\n",
      "3       3   -1.290987    0.670113      0.142292\n",
      "4       4   -0.085546    0.737440      0.122292\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "n_observations = 5  # Observaciones por muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "# Matriz omega (varianzas)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "\n",
    "# Descomposición de Cholesky de omega\n",
    "P = np.linalg.cholesky(omega)\n",
    "\n",
    "# Inversa de P\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "# Contenedores para almacenar muestras y estimadores\n",
    "samples = []\n",
    "results = []\n",
    "\n",
    "# Simulación de M muestras\n",
    "for i in range(n_samples):\n",
    "    # 1. Generar datos\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "    # Almacenar muestra\n",
    "    samples.append(pd.DataFrame({'sample': i, 'x': x, 'y': y, 'u': u}))\n",
    "\n",
    "    # 2. Transformar para MCG\n",
    "    y_tilde = P_inv @ y\n",
    "    X = sm.add_constant(x)  # Agregar columna de unos automáticamente\n",
    "    X_tilde = P_inv @ X\n",
    "\n",
    "    # 3. Ajustar modelo transformado con statsmodels\n",
    "    model = sm.OLS(y_tilde, X_tilde)\n",
    "    results_model = model.fit()\n",
    "\n",
    "    # Extraer coeficientes y error estándar\n",
    "    beta_0_hat, beta_1_hat = results_model.params\n",
    "    se_beta1_hat = results_model.bse[1]  # Error estándar del coeficiente beta_1\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append([i, beta_0_hat, beta_1_hat, se_beta1_hat])\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "# Crear DataFrame con los resultados de FGLS\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    'sample', 'beta_0_hat', 'beta_1_hat', 'se_beta1_hat'\n",
    "])\n",
    "\n",
    "# Mostrar ejemplos de los DataFrames generados\n",
    "print(\"Ejemplo de all_samples:\")\n",
    "print(all_samples.head())\n",
    "print(\"\\nEjemplo de results_df:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test: {0.01: 0.0, 0.05: 0.0012}\n",
      "Poder del test: {0: 0.9014, 0.4: 0.452}\n",
      "Estadísticas descriptivas:\n",
      "           beta_0    beta_1\n",
      "mean   -2.958325  0.798089\n",
      "median -2.890889  0.796406\n",
      "std     4.649156  0.162026\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para estimar por FGLS\n",
    "def fgls_estimation(x, y, omega):\n",
    "    W = np.linalg.inv(omega)  # Matriz de pesos inversos\n",
    "    X = np.column_stack((np.ones(len(x)), x))  # Matriz de diseño\n",
    "    beta_hat = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ y)  # Estimación FGLS\n",
    "    var_beta = np.linalg.inv(X.T @ W @ X)  # Varianza de los estimadores\n",
    "    return beta_hat, np.sqrt(np.diag(var_beta))  # Estimadores y errores estándar\n",
    "\n",
    "# Simulaciones\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    beta_hat, se_beta = fgls_estimation(x, y, omega)  # Estimación FGLS\n",
    "    t_stat = (beta_hat[1] - beta_1_true) / se_beta[1]  # Estadístico t para beta_1\n",
    "    p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_observations - 2))  # Valor p\n",
    "    results.append([beta_hat[0], beta_hat[1], se_beta[0], se_beta[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        beta_hat, se_beta = fgls_estimation(x, y, omega)\n",
    "        t_stat = (beta_hat[1] - beta_1_true) / se_beta[1]\n",
    "        p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_observations - 2))\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test: {0.01: 0.6018, 0.05: 0.8908}\n",
      "Poder del test: {0: 0.0532, 0.4: 0.5276}\n",
      "Estadísticas descriptivas:\n",
      "           beta_0    beta_1\n",
      "mean   -2.958325  0.798089\n",
      "median -2.890889  0.796406\n",
      "std     4.649156  0.162026\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para estimar por FGLS usando statsmodels\n",
    "def fgls_estimation(x, y, omega):\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    model = sm.GLS(y, X, sigma=omega)  # Estimación FGLS con sigma = omega\n",
    "    results = model.fit()  # Ajustar el modelo\n",
    "    return results\n",
    "\n",
    "# Simulaciones\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    result = fgls_estimation(x, y, omega)  # Estimación FGLS\n",
    "    t_stat = result.tvalues[1]  # Estadístico t para beta_1\n",
    "    p_value = result.pvalues[1]  # Valor p para beta_1\n",
    "    results.append([result.params[0], result.params[1], result.bse[0], result.bse[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        result = fgls_estimation(x, y, omega)\n",
    "        t_stat = result.tvalues[1]\n",
    "        p_value = result.pvalues[1]\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta_0_sm</th>\n",
       "      <th>beta_1_sm</th>\n",
       "      <th>se_beta_0_sm</th>\n",
       "      <th>se_beta_1_sm</th>\n",
       "      <th>beta_0_manual</th>\n",
       "      <th>beta_1_manual</th>\n",
       "      <th>se_beta_0_manual</th>\n",
       "      <th>se_beta_1_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.124395</td>\n",
       "      <td>0.803626</td>\n",
       "      <td>3.777375</td>\n",
       "      <td>0.136698</td>\n",
       "      <td>-3.124395</td>\n",
       "      <td>0.803626</td>\n",
       "      <td>3.777375</td>\n",
       "      <td>0.136698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>-3.050941</td>\n",
       "      <td>0.804804</td>\n",
       "      <td>3.135215</td>\n",
       "      <td>0.118106</td>\n",
       "      <td>-3.050941</td>\n",
       "      <td>0.804804</td>\n",
       "      <td>3.135215</td>\n",
       "      <td>0.118106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.543793</td>\n",
       "      <td>0.162838</td>\n",
       "      <td>2.673090</td>\n",
       "      <td>0.088475</td>\n",
       "      <td>4.543793</td>\n",
       "      <td>0.162838</td>\n",
       "      <td>2.673090</td>\n",
       "      <td>0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        beta_0_sm  beta_1_sm  se_beta_0_sm  se_beta_1_sm  beta_0_manual  \\\n",
       "mean    -3.124395   0.803626      3.777375      0.136698      -3.124395   \n",
       "median  -3.050941   0.804804      3.135215      0.118106      -3.050941   \n",
       "std      4.543793   0.162838      2.673090      0.088475       4.543793   \n",
       "\n",
       "        beta_1_manual  se_beta_0_manual  se_beta_1_manual  \n",
       "mean         0.803626          3.777375          0.136698  \n",
       "median       0.804804          3.135215          0.118106  \n",
       "std          0.162838          2.673090          0.088475  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Parámetros\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_observations = 5  # Pequeña muestra\n",
    "n_samples = 5000  # Número de simulaciones\n",
    "\n",
    "# Generar datos\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_data():\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    return x, y\n",
    "\n",
    "# Método 1: Estimación con FGLS usando statsmodels\n",
    "def fgls_statsmodels(x, y):\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    model = sm.GLS(y, X, sigma=omega)  # Estimación FGLS con sigma = omega\n",
    "    results = model.fit()  # Ajustar el modelo\n",
    "    return results\n",
    "\n",
    "# Método 2: Estimación manual FGLS\n",
    "def fgls_manual(x, y, omega):\n",
    "    X = np.column_stack([np.ones(n_observations), x])  # Matriz de diseño con intercepto\n",
    "    # Calcular la inversa de Omega\n",
    "    omega_inv = np.linalg.inv(omega)\n",
    "    # Estimar Beta usando la fórmula FGLS\n",
    "    beta_hat = np.linalg.inv(X.T @ omega_inv @ X) @ (X.T @ omega_inv @ y)\n",
    "    # Estimación de errores estándar (desviación estándar)\n",
    "    residuals = y - X @ beta_hat\n",
    "    sigma_hat = (residuals.T @ omega_inv @ residuals) / (n_observations - 2)  # Varianza de los errores\n",
    "    cov_beta_hat = np.linalg.inv(X.T @ omega_inv @ X) * sigma_hat  # Varianza de los coeficientes\n",
    "    se_beta_hat = np.sqrt(np.diag(cov_beta_hat))  # Desviación estándar de los coeficientes\n",
    "    return beta_hat, se_beta_hat\n",
    "\n",
    "# Comparación de ambos métodos\n",
    "results_fgls_sm = []\n",
    "results_fgls_manual = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x, y = generate_data()\n",
    "    \n",
    "    # Método 1: FGLS con statsmodels\n",
    "    result_sm = fgls_statsmodels(x, y)\n",
    "    results_fgls_sm.append([result_sm.params[0], result_sm.params[1], result_sm.bse[0], result_sm.bse[1]])\n",
    "    \n",
    "    # Método 2: Estimación manual\n",
    "    beta_hat_manual, se_beta_hat_manual = fgls_manual(x, y, omega)\n",
    "    results_fgls_manual.append([beta_hat_manual[0], beta_hat_manual[1], se_beta_hat_manual[0], se_beta_hat_manual[1]])\n",
    "\n",
    "# Convertir resultados a DataFrame para comparar\n",
    "results_fgls_sm_df = pd.DataFrame(results_fgls_sm, columns=['beta_0_sm', 'beta_1_sm', 'se_beta_0_sm', 'se_beta_1_sm'])\n",
    "results_fgls_manual_df = pd.DataFrame(results_fgls_manual, columns=['beta_0_manual', 'beta_1_manual', 'se_beta_0_manual', 'se_beta_1_manual'])\n",
    "\n",
    "# Comparar medias, medianas y desviaciones estándar\n",
    "comparison = pd.concat([\n",
    "    results_fgls_sm_df[['beta_0_sm', 'beta_1_sm', 'se_beta_0_sm', 'se_beta_1_sm']].agg(['mean', 'median', 'std']),\n",
    "    results_fgls_manual_df[['beta_0_manual', 'beta_1_manual', 'se_beta_0_manual', 'se_beta_1_manual']].agg(['mean', 'median', 'std'])\n",
    "], axis=1)\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test: {0.01: 0.298, 0.05: 0.6176}\n",
      "Poder del test: {0: 0.0282, 0.4: 0.2734}\n",
      "Estadísticas descriptivas:\n",
      "           beta_0    beta_1\n",
      "mean   -2.958325  0.798089\n",
      "median -2.890889  0.796406\n",
      "std     4.649156  0.162026\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para estimar por FGLS usando statsmodels con errores robustos\n",
    "def fgls_estimation(x, y, omega):\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    model = sm.GLS(y, X, sigma=omega)  # Estimación FGLS con sigma = omega\n",
    "    results = model.fit()  # Ajustar el modelo\n",
    "    robust_results = results.get_robustcov_results(cov_type='HC3')  # Errores estándar robustos (HC3)\n",
    "    return robust_results\n",
    "\n",
    "# Simulaciones\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    robust_results = fgls_estimation(x, y, omega)  # Estimación FGLS con errores robustos\n",
    "    t_stat = robust_results.tvalues[1]  # Estadístico t para beta_1\n",
    "    p_value = robust_results.pvalues[1]  # Valor p para beta_1\n",
    "    results.append([robust_results.params[0], robust_results.params[1], robust_results.bse[0], robust_results.bse[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        robust_results = fgls_estimation(x, y, omega)\n",
    "        t_stat = robust_results.tvalues[1]\n",
    "        p_value = robust_results.pvalues[1]\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sigma must be a scalar, 1d of length 5 or a 2d array of shape 5 x 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m u \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmultivariate_normal(mean\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(n_observations), cov\u001b[38;5;241m=\u001b[39momega)  \u001b[38;5;66;03m# u ~ N(0, omega)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m y \u001b[38;5;241m=\u001b[39m beta_0 \u001b[38;5;241m+\u001b[39m beta_1_true \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m u  \u001b[38;5;66;03m# Generar y\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m fgls_result \u001b[38;5;241m=\u001b[39m \u001b[43mfgls_white_estimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Estimación FGLS White\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Obtener estadísticos de interés\u001b[39;00m\n\u001b[0;32m     43\u001b[0m t_stat \u001b[38;5;241m=\u001b[39m fgls_result\u001b[38;5;241m.\u001b[39mtvalues[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Estadístico t para beta_1\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m, in \u001b[0;36mfgls_white_estimation\u001b[1;34m(x, y, omega)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Paso 3: Estimación FGLS con la matriz robusta\u001b[39;00m\n\u001b[0;32m     26\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(robust_cov)  \u001b[38;5;66;03m# Matriz de pesos (inversa de la matriz de varianzas-covarianzas)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m fgls_model \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Estimación FGLS usando la matriz robusta\u001b[39;00m\n\u001b[0;32m     28\u001b[0m fgls_results \u001b[38;5;241m=\u001b[39m fgls_model\u001b[38;5;241m.\u001b[39mfit()  \u001b[38;5;66;03m# Ajuste FGLS\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fgls_results\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:534\u001b[0m, in \u001b[0;36mGLS.__init__\u001b[1;34m(self, endog, exog, sigma, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_kwargs(kwargs)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# TODO: add options igls, for iterative fgls if sigma is None\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# TODO: default if sigma is none should be two-step GLS\u001b[39;00m\n\u001b[1;32m--> 534\u001b[0m sigma, cholsigmainv \u001b[38;5;241m=\u001b[39m \u001b[43m_get_sigma\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    537\u001b[0m                           hasconst\u001b[38;5;241m=\u001b[39mhasconst, sigma\u001b[38;5;241m=\u001b[39msigma,\n\u001b[0;32m    538\u001b[0m                           cholsigmainv\u001b[38;5;241m=\u001b[39mcholsigmainv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# store attribute names for data arrays\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:181\u001b[0m, in \u001b[0;36m_get_sigma\u001b[1;34m(sigma, nobs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sigma\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (nobs, nobs):\n\u001b[1;32m--> 181\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSigma must be a scalar, 1d of length \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m or a 2d \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray of shape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m x \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (nobs, nobs, nobs))\n\u001b[0;32m    183\u001b[0m     cholsigmainv, info \u001b[38;5;241m=\u001b[39m dtrtri(cholesky(sigma, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    184\u001b[0m                                 lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, overwrite_c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Sigma must be a scalar, 1d of length 5 or a 2d array of shape 5 x 5"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Parámetros del modelo\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas (heterocedasticidad)\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para realizar la estimación FGLS con errores robustos de White\n",
    "def fgls_white_estimation(x, y, omega):\n",
    "    # Paso 1: Estimación OLS inicial\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "    ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "    \n",
    "    # Paso 2: Estimación de la matriz de varianzas-covarianzas de los errores\n",
    "    residuals = ols_results.resid  # Residuos de la estimación OLS\n",
    "    # Usamos la matriz de varianzas-covarianzas robusta (White)\n",
    "    robust_cov = ols_results.get_robustcov_results(cov_type='HC3').cov_params()  # Matriz robusta de varianzas-covarianzas\n",
    "    \n",
    "    # Paso 3: Estimación FGLS con la matriz robusta\n",
    "    W = np.linalg.inv(robust_cov)  # Matriz de pesos (inversa de la matriz de varianzas-covarianzas)\n",
    "    fgls_model = sm.GLS(y, X, sigma=W)  # Estimación FGLS usando la matriz robusta\n",
    "    fgls_results = fgls_model.fit()  # Ajuste FGLS\n",
    "    \n",
    "    return fgls_results\n",
    "\n",
    "# Simulación de los datos\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    fgls_result = fgls_white_estimation(x, y, omega)  # Estimación FGLS White\n",
    "    \n",
    "    # Obtener estadísticos de interés\n",
    "    t_stat = fgls_result.tvalues[1]  # Estadístico t para beta_1\n",
    "    p_value = fgls_result.pvalues[1]  # Valor p para beta_1\n",
    "    results.append([fgls_result.params[0], fgls_result.params[1], fgls_result.bse[0], fgls_result.bse[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        fgls_result = fgls_white_estimation(x, y, omega)\n",
    "        p_value = fgls_result.pvalues[1]\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
