{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNIVERSIDAD TORCUATO DI TELLA**\n",
    "## **MAESTRÍA EN ECONOMETRÍA**\n",
    "\n",
    "---\n",
    "\n",
    "### **TRABAJO PRÁCTICO DE ECONOMETRÍA**\n",
    "\n",
    "- **Profesor:** González-Rozada, Martín  \n",
    "- **Ayudante:** Lening, Iara  \n",
    "- **Alumno:** Guzzi, David Alexander  (Legajo n°: 24H1970)  \n",
    "\n",
    "**Ciclo Lectivo:** Tercer Trimestre, 2024  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. PROPIEDADES DE MUESTRA FINITA DE FGLS (MCGE).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con un nivel de significatividad del 5%, el tamaño del test es 1.0000\n",
      "Con un nivel de significatividad del 1%, el tamaño del test es 1.0000\n",
      "Con un nivel de significatividad del 5%, cuando el valor verdadero de b es 0.5, la potencia del test es 1.0000\n",
      "Con un nivel de significatividad del 1%, cuando el valor verdadero de b es 0.5, la potencia del test es 1.0000\n",
      "Con un nivel de significatividad del 5%, cuando el valor verdadero de b es 0, la potencia del test es 0.0522\n",
      "Con un nivel de significatividad del 1%, cuando el valor verdadero de b es 0, la potencia del test es 0.0118\n",
      "             beta0        beta1    gamma_hat\n",
      "count  5000.000000  5000.000000  5000.000000\n",
      "mean      0.994148     1.001113     0.992565\n",
      "std       0.244381     0.076791     0.098951\n",
      "min      -0.034371     0.728956     0.670566\n",
      "50%       0.989151     1.003207     0.986854\n",
      "max       1.899646     1.343259     1.406991\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "np.random.seed(144)\n",
    "n_reps = 5000\n",
    "n_obs = 500\n",
    "\n",
    "# Función que ejecuta el procedimiento una vez\n",
    "def run_simulation():\n",
    "    x = np.random.uniform(1, 10, n_obs)\n",
    "    x2 = x ** 2\n",
    "    u = np.random.normal(scale=x)\n",
    "    \n",
    "    y0 = 1 + x + u\n",
    "    y1 = 1 + 0.5 * x + u\n",
    "    y2 = 1 + u\n",
    "    \n",
    "    def fgls(y, x):\n",
    "        ols = sm.OLS(y, sm.add_constant(x)).fit()\n",
    "        resid2 = ols.resid ** 2\n",
    "        gamma_hat = sm.OLS(resid2, x2).fit().params[0]\n",
    "        \n",
    "        uhat2_hat = gamma_hat * x2\n",
    "        weights = np.sqrt(uhat2_hat)\n",
    "        x_gls = x / weights\n",
    "        y_gls = y / weights\n",
    "        cons_gls = 1 / weights\n",
    "        gls = sm.OLS(y_gls, np.column_stack((cons_gls, x_gls))).fit()\n",
    "        \n",
    "        p_value = gls.pvalues[1]  # Para la prueba H0: beta = 1\n",
    "        return p_value, gls.params[0], gls.params[1], gamma_hat\n",
    "    \n",
    "    pv0, beta0, beta1, gamma_hat = fgls(y0, x)\n",
    "    pv1, _, _, _ = fgls(y1, x)\n",
    "    pv2, _, _, _ = fgls(y2, x)\n",
    "    \n",
    "    return pv0, pv1, pv2, beta0, beta1, gamma_hat\n",
    "\n",
    "# Ejecutamos la simulación\n",
    "results = np.array([run_simulation() for _ in range(n_reps)])\n",
    "\n",
    "# Extraemos los resultados\n",
    "pv0, pv1, pv2, beta0_vals, beta1_vals, gamma_hat_vals = results.T\n",
    "\n",
    "# Calculamos los tamaños del test y la potencia\n",
    "pv0_5 = np.mean(pv0 < 0.05)\n",
    "pv0_1 = np.mean(pv0 < 0.01)\n",
    "pv1_5 = np.mean(pv1 < 0.05)\n",
    "pv1_1 = np.mean(pv1 < 0.01)\n",
    "pv2_5 = np.mean(pv2 < 0.05)\n",
    "pv2_1 = np.mean(pv2 < 0.01)\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(f\"Con un nivel de significatividad del 5%, el tamaño del test es {pv0_5:.4f}\")\n",
    "print(f\"Con un nivel de significatividad del 1%, el tamaño del test es {pv0_1:.4f}\")\n",
    "print(f\"Con un nivel de significatividad del 5%, cuando el valor verdadero de b es 0.5, la potencia del test es {pv1_5:.4f}\")\n",
    "print(f\"Con un nivel de significatividad del 1%, cuando el valor verdadero de b es 0.5, la potencia del test es {pv1_1:.4f}\")\n",
    "print(f\"Con un nivel de significatividad del 5%, cuando el valor verdadero de b es 0, la potencia del test es {pv2_5:.4f}\")\n",
    "print(f\"Con un nivel de significatividad del 1%, cuando el valor verdadero de b es 0, la potencia del test es {pv2_1:.4f}\")\n",
    "\n",
    "# Estadísticas de resumen\n",
    "summary_df = pd.DataFrame({\n",
    "    'beta0': beta0_vals,\n",
    "    'beta1': beta1_vals,\n",
    "    'gamma_hat': gamma_hat_vals\n",
    "})\n",
    "print(summary_df.describe(percentiles=[0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Función para realizar FGLS\n",
    "def fgls_estimation(x, y):\n",
    "    # DataFrame para almacenar las variables intermedias\n",
    "    df = pd.DataFrame({'x': x, 'y': y})\n",
    "    df['x2'] = df['x'] ** 2\n",
    "\n",
    "    # 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "    ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "    # 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "    df['u_hat'] = ols_results.resid\n",
    "    df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "    # 3. Dada la forma funcional de la heterocedasticidad de White para este modelo, sigma2 ~ x + x2, se estima por OLS usando u_hat2 como proxy de sigma2\n",
    "    aux_X = sm.add_constant(df[['x', 'x2']])\n",
    "    ols_model_aux = sm.OLS(df['u_hat2'], aux_X)\n",
    "    ols_model_aux_results = ols_model_aux.fit()\n",
    "    gamma_hat1 = ols_model_aux_results.params['x']\n",
    "    gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "    # 4. Usar las estimaciones de la regresión auxiliar y obtener las varianzas ajustadas (no consistentes)\n",
    "    df['sigma2_hat'] = ols_model_aux_results.predict(aux_X)\n",
    "\n",
    "    # 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por sigma2_hat y estimar por OLS\n",
    "    df['u_hat2_over_sigma2_hat'] = df['u_hat2'] / df['sigma2_hat']\n",
    "    df['const1'] = 1 / df['sigma2_hat']\n",
    "    df['x_over_sigma2_hat'] = df['x'] / df['sigma2_hat']\n",
    "    df['x2_over_sigma2_hat'] = df['x2'] / df['sigma2_hat']\n",
    "    \n",
    "\n",
    "    # Crear la matriz de diseño constante, x_over_sigma2_hat y x2_over_sigma2_hat\n",
    "    aux_X_2 = df[['const1', 'x_over_sigma2_hat', 'x2_over_sigma2_hat']]\n",
    "\n",
    "    ols_model_aux2 = sm.OLS(df['u_hat2_over_sigma2_hat'], aux_X_2)\n",
    "    ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "    gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "    gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "    # 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "    df['sigma2_tilde'] = ols_model_aux_results2.predict(aux_X_2)\n",
    "    \n",
    "    # Corregir valores negativos o muy pequeños\n",
    "    df['sigma2_tilde'] = df['sigma2_tilde'].clip(lower=1e-10)\n",
    "    df['sigma_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "    # Verificar si hay valores NaN\n",
    "    if df['sigma_tilde'].isna().any():\n",
    "        raise ValueError(\"Se encontraron valores NaN en sigma_tilde\")\n",
    "\n",
    "    # 7. Usar uno sobre sigma tilde como ponderador en la regresión de y ~ x\n",
    "    df['y_star'] = df['y'] / df['sigma_tilde']\n",
    "    df['const2'] = 1 / df['sigma_tilde']\n",
    "    df['x_star'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "    # Crear la matriz de diseño con constante y x_star\n",
    "    aux_X_star = df[['const2', 'x_star']]\n",
    "\n",
    "    # Ajustar el modelo OLS\n",
    "    final_ols_model = sm.OLS(df['y_star'], aux_X_star)\n",
    "    final_ols_results = final_ols_model.fit() \n",
    "\n",
    "    # Estimaciones finales\n",
    "    beta_0_hat = final_ols_results.params['const2']\n",
    "    beta_1_hat = final_ols_results.params['x_star']\n",
    "    se_beta1_hat = final_ols_results.bse.iloc[1]\n",
    "\n",
    "    return beta_0_hat, beta_1_hat, se_beta1_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Función para realizar FGLS\n",
    "def fgls_estimation2(x, y):\n",
    "    # DataFrame para almacenar las variables intermedias\n",
    "    df = pd.DataFrame({'x': x, 'y': y})\n",
    "    df['x2'] = df['x'] ** 2\n",
    "\n",
    "    # 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "    ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "    # 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "    df['u_hat'] = ols_results.resid\n",
    "    df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "    # 3. Dada la forma funcional de la heterocedasticidad de White para este modelo, sigma2 ~ x + x2, se estima por OLS usando u_hat2 como proxy de sigma2\n",
    "    aux_X = sm.add_constant(df[['x', 'x2']])\n",
    "    ols_model_aux = sm.OLS(df['u_hat2'], aux_X)\n",
    "    ols_model_aux_results = ols_model_aux.fit()\n",
    "    gamma_hat1 = ols_model_aux_results.params['x']\n",
    "    gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "    # 4. Usar las estimaciones de la regresión auxiliar y obtener las varianzas ajustadas (no consistentes)\n",
    "    df['sigma2_hat'] = ols_model_aux_results.predict(aux_X).clip(lower=1e-10)\n",
    "    df['sigma_hat'] = np.sqrt(df['sigma2_hat'])\n",
    "\n",
    "    # 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por sigma2_hat y estimar por OLS\n",
    "    df['y_over_sigma2_hat'] = df['y'] / df['sigma_hat']\n",
    "    df['const1'] = 1 / df['sigma_hat']\n",
    "    df['x_over_sigma2_hat'] = df['x'] / df['sigma_hat']\n",
    "    # df['x2_over_sigma2_hat'] = df['x2'] / df['sigma_hat']\n",
    "    \n",
    "\n",
    "    # Crear la matriz de diseño constante, x_over_sigma2_hat y x2_over_sigma2_hat\n",
    "    aux_X_2 = df[['const1', 'x_over_sigma2_hat']]\n",
    "\n",
    "    ols_model_aux2 = sm.OLS(df['y_over_sigma2_hat'], aux_X_2)\n",
    "    ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "    # gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "    # gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "    # Estimaciones finales\n",
    "    beta_0_hat = ols_model_aux_results2.params['const1']\n",
    "    beta_1_hat = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "    se_beta1_hat = ols_model_aux_results2.bse.iloc[1]\n",
    "\n",
    "    # # 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "    # df['sigma2_tilde'] = ols_model_aux_results2.predict(aux_X_2)\n",
    "    \n",
    "    # # Corregir valores negativos o muy pequeños\n",
    "    # df['sigma2_tilde'] = df['sigma2_tilde'].clip(lower=1e-10)\n",
    "    # df['sigma_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "    # # Verificar si hay valores NaN\n",
    "    # if df['sigma_tilde'].isna().any():\n",
    "    #     raise ValueError(\"Se encontraron valores NaN en sigma_tilde\")\n",
    "\n",
    "    # # 7. Usar uno sobre sigma tilde como ponderador en la regresión de y ~ x\n",
    "    # df['y_star'] = df['y'] / df['sigma_tilde']\n",
    "    # df['const2'] = 1 / df['sigma_tilde']\n",
    "    # df['x_star'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "    # # Crear la matriz de diseño con constante y x_star\n",
    "    # aux_X_star = df[['const2', 'x_star']]\n",
    "\n",
    "    # # Ajustar el modelo OLS\n",
    "    # final_ols_model = sm.OLS(df['y_star'], aux_X_star)\n",
    "    # final_ols_results = final_ols_model.fit() \n",
    "\n",
    "    # # Estimaciones finales\n",
    "    # beta_0_hat = final_ols_results.params['const2']\n",
    "    # beta_1_hat = final_ols_results.params['x_star']\n",
    "    # se_beta1_hat = final_ols_results.bse.iloc[1]\n",
    "\n",
    "    return beta_0_hat, beta_1_hat, se_beta1_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgls_estimation_numpy(x, y):\n",
    "    # 1. Estimación inicial por OLS\n",
    "    X = np.column_stack((np.ones_like(x), x))  # Agregar intercepto\n",
    "    beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y  # Estimación OLS\n",
    "    u_hat = y - X @ beta_ols  # Residuos\n",
    "    u_hat2 = u_hat ** 2  # Cuadrado de los residuos\n",
    "    \n",
    "    # 2. Regresión auxiliar para estimar la heterocedasticidad\n",
    "    X_aux = np.column_stack((np.ones_like(x), x, x**2))\n",
    "    gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ u_hat2\n",
    "    sigma2_hat = np.maximum(X_aux @ gamma_hat, 1e-10)  # Asegurar positividad\n",
    "    sigma_hat = np.sqrt(sigma2_hat)\n",
    "    \n",
    "    # 3. Transformación de variables y segunda estimación por OLS\n",
    "    y_trans = y / sigma_hat\n",
    "    X_trans = np.column_stack((np.ones_like(x) / sigma_hat, x / sigma_hat))\n",
    "    beta_fgls = np.linalg.inv(X_trans.T @ X_trans) @ X_trans.T @ y_trans\n",
    "    \n",
    "    # 4. Cálculo de errores estándar\n",
    "    residuals_fgls = y_trans - X_trans @ beta_fgls\n",
    "    sigma2_fgls = np.sum(residuals_fgls**2) / (len(x) - X_trans.shape[1])\n",
    "    var_beta_fgls = sigma2_fgls * np.linalg.inv(X_trans.T @ X_trans)\n",
    "    se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]\n",
    "    \n",
    "    return beta_fgls[0], beta_fgls[1], se_beta1_fgls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "def fgls_estimation_numpy2(x, y):\n",
    "    # 1. Estimación inicial por OLS\n",
    "    X = np.column_stack((np.ones_like(x), x))  # Agregar intercepto\n",
    "    beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y  # Estimación OLS\n",
    "    u_hat = y - X @ beta_ols  # Residuos\n",
    "    u_hat2 = u_hat ** 2  # Cuadrado de los residuos\n",
    "    \n",
    "    # 2. Regresión auxiliar para estimar la heterocedasticidad\n",
    "    X_aux = np.column_stack((np.ones_like(x), x, x**2))\n",
    "    gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ u_hat2\n",
    "    sigma2_hat = X_aux @ gamma_hat\n",
    "\n",
    "    X_aux2 = np.column_stack((np.ones_like(x) / sigma2_hat, x / sigma2_hat,  x**2 / sigma2_hat))\n",
    "    u_hat2_over_sigma2_hat = u_hat2 / sigma2_hat\n",
    "    gamma_hat2 = np.linalg.inv(X_aux2.T @ X_aux2) @ X_aux2.T @ u_hat2_over_sigma2_hat\n",
    "\n",
    "    sigma2_tilde = X_aux2 @ gamma_hat2  # Asegurar positividad\n",
    "    sigma_tilde = np.sqrt(sigma2_tilde)\n",
    "    \n",
    "    # 3. Transformación de variables y segunda estimación por OLS\n",
    "    y_trans = y / sigma_tilde\n",
    "    X_trans = np.column_stack((np.ones_like(x) / sigma_tilde, x / sigma_tilde))\n",
    "    beta_fgls = np.linalg.inv(X_trans.T @ X_trans) @ X_trans.T @ y_trans\n",
    "    \n",
    "    # 4. Cálculo de errores estándar\n",
    "    residuals_fgls = y_trans - X_trans @ beta_fgls\n",
    "    sigma2_fgls = np.sum(residuals_fgls**2) / (len(x) - X_trans.shape[1])\n",
    "    var_beta_fgls = sigma2_fgls * np.linalg.inv(X_trans.T @ X_trans)\n",
    "    se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]\n",
    "    \n",
    "    return beta_fgls[0], beta_fgls[1], se_beta1_fgls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "def fgls_estimation_numpy3(x, y):\n",
    "    # 1. Estimación inicial por OLS\n",
    "    X = np.column_stack((np.ones_like(x), x))  # Agregar intercepto\n",
    "    beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y  # Estimación OLS\n",
    "    u_hat = y - X @ beta_ols  # Residuos\n",
    "    u_hat2 = u_hat ** 2  # Cuadrado de los residuos\n",
    "    \n",
    "    # 2. Regresión auxiliar para estimar la heterocedasticidad\n",
    "    X_aux = np.column_stack((np.ones_like(x), x, x**2))\n",
    "    gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ u_hat2\n",
    "    sigma2_hat = X_aux @ gamma_hat\n",
    "\n",
    "    X_aux2 = np.column_stack((np.ones_like(x) / sigma2_hat, x / sigma2_hat,  x**2 / sigma2_hat))\n",
    "    u_hat2_over_sigma2_hat = u_hat2 / sigma2_hat\n",
    "    gamma_hat2 = np.linalg.inv(X_aux2.T @ X_aux2) @ X_aux2.T @ u_hat2_over_sigma2_hat\n",
    "\n",
    "    sigma2_tilde = X_aux @ gamma_hat2  # Asegurar positividad\n",
    "    # sigma_tilde = np.sqrt(sigma2_tilde)\n",
    "    omega_tilde_inv = np.linalg.inv(np.diag(sigma2_tilde))\n",
    "    \n",
    "    # # 3. Transformación de variables y segunda estimación por OLS\n",
    "    # y_trans = y / sigma_tilde\n",
    "    # X_trans = np.column_stack((np.ones_like(x) / sigma_tilde, x / sigma_tilde))\n",
    "    beta_fgls = np.linalg.inv(X.T @ omega_tilde_inv @ X) @ X.T @ omega_tilde_inv @ y\n",
    "    \n",
    "    # 4. Cálculo de errores estándar\n",
    "    residuals_fgls = y - X @ beta_fgls\n",
    "    s2_fgls = (residuals_fgls.T @ omega_tilde_inv @ residuals_fgls) / (len(x) - X.shape[1])\n",
    "    var_beta_fgls = s2_fgls * np.linalg.inv(X.T @ omega_tilde_inv @ X)\n",
    "    se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]   \n",
    "    \n",
    "    return beta_fgls[0], beta_fgls[1], se_beta1_fgls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "def fgls_estimation_numpy4(x, y):\n",
    "    \n",
    "    X = np.column_stack((np.ones_like(x), x)) \n",
    "    beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y \n",
    "    u_hat = y - X @ beta_ols\n",
    "    u_hat2 = u_hat ** 2  \n",
    "    log_u_hat2 = np.log(u_hat2)\n",
    "    \n",
    "    X_aux = np.column_stack((x, x**2))\n",
    "    gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ log_u_hat2\n",
    "    log_sigma2_hat = X_aux @ gamma_hat\n",
    "    sigma2_hat = np.exp(log_sigma2_hat)\n",
    "\n",
    "    X_aux2 = np.column_stack((x / sigma2_hat,  x**2 / sigma2_hat))\n",
    "    u_hat2_over_sigma2_hat = u_hat2 / sigma2_hat\n",
    "    gamma_hat2 = np.linalg.inv(X_aux2.T @ X_aux2) @ X_aux2.T @ u_hat2_over_sigma2_hat\n",
    "\n",
    "    sigma2_tilde = X_aux @ gamma_hat2 \n",
    "    sigma2_tilde = np.maximum(sigma2_tilde, 1e-6)\n",
    "    omega_tilde_inv = np.linalg.inv(np.diag(sigma2_tilde))\n",
    "    \n",
    "    \n",
    "    beta_fgls = np.linalg.inv(X.T @ omega_tilde_inv @ X) @ X.T @ omega_tilde_inv @ y\n",
    "    \n",
    "    # 4. Cálculo de errores estándar\n",
    "    residuals_fgls = y - X @ beta_fgls\n",
    "    s2_fgls = (residuals_fgls.T @ omega_tilde_inv @ residuals_fgls) / ( X.shape[0] - X.shape[1])\n",
    "    var_beta_fgls = s2_fgls * np.linalg.inv(X.T @ omega_tilde_inv @ X)\n",
    "    se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]   \n",
    "    \n",
    "    return beta_fgls[0], beta_fgls[1], se_beta1_fgls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_hat2: [3.72274068e+03 4.93029693e+02 1.76274518e+03 8.95859030e+02\n",
      " 1.20397116e+03 1.45748562e+01 1.71183514e+00 3.68287155e+01\n",
      " 4.80798104e+01 7.16184304e+02 1.81838223e+02 1.75854284e+01\n",
      " 4.21535431e+02 2.78471371e+02 1.56941745e+01 3.68948185e+01\n",
      " 1.26841166e+03 2.42232780e+02 4.98376119e-02 1.07450608e+02\n",
      " 9.47909850e+02 1.19316035e+00 3.75589602e+02 7.94265394e+01\n",
      " 5.36467879e+02 8.97070726e+01 4.95216021e+00 5.98619171e+01\n",
      " 6.65142815e+01 8.83710516e+01 1.33054545e+01 5.32474385e-01\n",
      " 1.07664898e+03 1.12914411e+03 1.23142937e+02 2.32139660e-02\n",
      " 9.08496571e+00 1.48158691e+02 2.92085565e+03 4.44394536e+01\n",
      " 1.61321095e+01 1.22198614e+03 3.16575760e+00 1.20380979e+01\n",
      " 2.43842096e+03 2.24704849e+03 7.79691426e+02 1.03845107e+01\n",
      " 4.14678838e+02 2.66252607e-02 3.54182652e+03 1.36365565e+01\n",
      " 4.09515181e+01 9.91182818e+01 1.96331907e+02 3.55326325e+03\n",
      " 3.59422321e+02 5.33557790e+01 1.06220202e+01 1.88030655e+01\n",
      " 9.85238312e+01 6.52230599e+02 1.06748002e+01 9.11034282e+02\n",
      " 1.23823306e+03 5.19859549e+01 4.94153858e+02 1.05762404e+03\n",
      " 1.65200260e+03 6.78519058e+00 5.09271590e-01 4.14018582e+01\n",
      " 4.80235350e+02 9.24015990e+02 4.35890168e+02 4.50791099e+03\n",
      " 7.70596928e+01 9.18230697e+03 1.22033055e+03 1.39982296e+00\n",
      " 1.76643076e+03 4.85733628e+01 6.22785191e+00 1.91285997e+02\n",
      " 7.89742658e+00 2.53693383e+01 1.47270687e+01 9.39152951e+00\n",
      " 1.52635549e+02 9.88362748e+00 1.74248582e+03 1.56909670e+02\n",
      " 3.15240951e+01 3.67825928e+03 5.33055116e+03 1.78051413e+02\n",
      " 1.87099647e+02 2.97649735e+01 5.37721666e+02 4.01428080e+01\n",
      " 1.54907776e+00 8.55190274e+03 3.89117583e+02 8.29491248e+01\n",
      " 1.31787393e+03 1.03258512e+01 4.42218771e-01 2.20740224e+01\n",
      " 3.13373269e+00 2.79875637e+03 6.49611040e+01 5.34178411e+02\n",
      " 2.01272042e+03 1.10541834e+03 1.10648784e-02 3.57264675e+02\n",
      " 1.51091301e+02 4.43548996e+00 1.48075071e+02 4.67914235e+02\n",
      " 9.16092721e+01 1.99319622e+01 1.58690489e+03 2.12954206e+01\n",
      " 7.96246043e+03 2.87022470e+01 5.40529583e-01 5.91294743e+02\n",
      " 6.29192278e+02 8.98090161e+01 1.00395380e+00 2.70463207e+00\n",
      " 7.30160136e+01 1.02377114e+03 1.70908383e+02 2.83857628e+02\n",
      " 1.45135813e+03 1.08185004e+01 1.47306017e+02 5.50128711e+01\n",
      " 1.12183337e+03 2.79109124e+01 5.20024492e+00 9.02382694e+01\n",
      " 2.23316769e+03 6.93638666e+02 9.74171151e+01 5.11561661e+02\n",
      " 3.04895265e+00 6.15659611e+01 1.55119239e+00 6.52618250e+02\n",
      " 4.06810551e+01 2.08326671e-01 7.08577622e+01 2.64951057e+01\n",
      " 1.37166934e+00 2.11830300e+01 4.03498632e+02 2.63200222e+01\n",
      " 1.54424737e+02 1.95883669e+02 3.54179127e+02 4.61307892e-01\n",
      " 3.57666319e+01 6.14429969e+02 1.21684847e+00 8.67449978e+01\n",
      " 6.53555771e-03 1.43575655e+00 4.16425150e+01 4.88175289e+03\n",
      " 7.38966818e+02 6.75643100e+02 2.79607918e+01 7.52582879e-03\n",
      " 1.27398835e+03 1.79853499e+03 5.33800322e+01 6.17035695e-01\n",
      " 3.19770732e+01 1.31452989e+01 1.97807398e+03 1.38955239e+03\n",
      " 6.16597021e+02 3.11481628e+03 1.09706083e+03 3.31218757e+02\n",
      " 6.09760473e+03 1.93071099e+02 4.93326583e+03 8.56300320e+01\n",
      " 2.21006998e+03 2.95557279e+01 2.89636352e+02 3.88061295e+02\n",
      " 7.20046405e+02 1.69400055e+03 8.11304081e+02 1.00243146e+04\n",
      " 1.42033709e+02 2.21381467e+01 1.85298822e-01 3.10961826e+01\n",
      " 3.82122964e+02 2.60818893e+03 4.79430039e+02 2.40833908e+00\n",
      " 1.56622400e+00 2.20082536e+02 4.92427024e+02 3.61312722e+03\n",
      " 7.17472051e+00 2.47446927e+02 6.51909728e+01 1.35191334e+03\n",
      " 2.25827853e+03 5.93321754e+00 5.59983988e+02 2.43656803e+02\n",
      " 6.62622339e+02 4.14520275e+02 1.96190086e+02 8.77910178e+01\n",
      " 1.13047533e+01 5.83716404e+02 7.48412482e+01 1.02198878e+01\n",
      " 3.95670359e+02 1.96765408e+02 1.31281292e+02 1.99378646e+02\n",
      " 4.24520068e+03 1.81217263e+02 2.98367869e+01 1.11556624e+01\n",
      " 7.09355781e+02 1.15752823e+03 1.47236934e+01 7.16153083e-01\n",
      " 1.14486049e+01 5.94294955e+02 1.61882587e+03 2.67956103e+02\n",
      " 1.29844970e+02 1.71175694e+02 4.17967084e+00 3.48526409e+03\n",
      " 6.13645622e+02 1.09468611e+01 7.38643225e+02 5.67618884e+01\n",
      " 5.16210202e+00 2.01968281e-01 9.51893661e+02 1.42293508e+02\n",
      " 1.59267474e+00 5.70775607e+00 5.63610626e+03 2.45919252e+00\n",
      " 3.20381741e+00 7.24978501e+02 6.21078924e+01 2.17347939e+03\n",
      " 7.20832020e+01 3.82349289e+02 2.38507775e+03 2.24603454e+00\n",
      " 6.15711871e+01 5.49293847e+01 8.42080303e+01 9.41188555e+02\n",
      " 3.96121116e+03 1.66486756e+03 1.96201856e+03 2.58174669e+03\n",
      " 3.40301495e+01 3.76913030e+02 1.44353548e+02 1.51062351e+00\n",
      " 9.40939143e+02 5.81630171e+03 1.02815291e+04 6.34181529e+02\n",
      " 9.30440317e+02 1.24450218e+01 5.63070093e+00 1.35814633e+02\n",
      " 7.44280109e+02 1.47771390e+01 1.35482048e+01 4.82590762e+02\n",
      " 2.42806827e+02 3.94125513e-01 6.90730548e+00 2.55707618e+01\n",
      " 5.39901369e+01 2.08480183e-01 2.24260110e+01 1.08299510e+02\n",
      " 1.24525677e+03 3.87231102e+01 9.56903606e+01 1.88487511e+02\n",
      " 1.39514672e+01 1.80595795e+03 3.67426887e+02 1.51250197e+02\n",
      " 7.44035035e-01 6.81395534e+01 8.62934236e+01 5.28892913e+00\n",
      " 3.66858518e+02 7.85366208e+01 2.77154301e+03 1.59397518e-01\n",
      " 4.78919615e+01 5.47177927e+01 4.84665222e+02 9.88820073e+01\n",
      " 1.34271764e+01 9.37870653e+01 3.08934454e+02 1.49419911e+03\n",
      " 1.12666649e+03 1.38451890e+02 1.05471439e+02 4.81248904e+02\n",
      " 4.34887764e+00 4.99660726e+02 7.18706376e-02 2.04498565e+03\n",
      " 6.27616331e+02 9.40308641e+02 1.60833011e+02 3.65963846e+00\n",
      " 1.63004940e+03 1.19132145e+03 6.83177451e+00 2.55151768e+03\n",
      " 1.39684364e+02 2.65136147e+01 1.99388085e+02 2.57513005e+01\n",
      " 3.64125930e+02 1.04189470e+03 3.49615627e+03 1.50395987e+00\n",
      " 4.20237619e+00 5.72338598e+02 1.06746053e+04 2.20726061e+01\n",
      " 7.25902768e+03 1.52200391e-01 6.56708250e+00 2.05379452e+01\n",
      " 3.46281013e+02 1.11217943e+02 4.44160303e+02 2.25437527e-01\n",
      " 1.67669326e+02 3.86904332e+03 2.96576813e+02 1.85452263e+03\n",
      " 1.77651521e+02 9.78123931e+00 2.97173905e+02 3.96569959e+02\n",
      " 3.51009379e-02 1.91174156e+01 2.43157530e+02 1.55997224e+03\n",
      " 2.77568281e-01 2.03758932e+02 8.59660419e+00 1.39395683e+02\n",
      " 8.05942693e-01 4.70680781e+01 6.16619987e+03 5.76195928e+02\n",
      " 6.41405297e+02 2.18906287e+02 5.42087106e+00 3.85714438e+03\n",
      " 8.36529453e+02 6.21323552e+00 2.32718667e+01 3.75574343e+02\n",
      " 1.71999736e+03 1.22907989e+03 3.25818167e+02 1.47826019e+03\n",
      " 1.48896627e+01 1.28387452e+03 8.15285609e+01 3.03435576e+00\n",
      " 8.10428592e+01 3.03181194e+02 3.41232650e+03 8.31383835e-01\n",
      " 1.43497204e+02 8.58347659e+03 4.55421637e+02 1.79622124e-03\n",
      " 3.73982537e+02 2.03387373e+01 4.12204976e-02 3.97076176e-03\n",
      " 7.97347030e+02 2.07010582e+02 1.58773969e+02 5.86017244e+02\n",
      " 1.70294626e+02 4.65876424e+02 6.32580280e+01 3.26897602e+03\n",
      " 7.66661133e+00 6.56817235e+01 5.84006958e+03 7.40793313e+01\n",
      " 3.12557386e+03 3.16041059e+01 4.68171724e+01 1.29833123e-02\n",
      " 8.55080760e+02 8.32341439e+02 1.18817391e+02 6.25350740e+01\n",
      " 1.04033735e+01 4.84962519e+00 9.20565850e-02 3.67939414e+03\n",
      " 1.78871189e+00 2.16840572e+02 4.41045245e+02 1.11971259e+03\n",
      " 7.27357709e+01 3.42381454e+02 6.70243435e+01 1.24672984e+02\n",
      " 1.33186776e+03 2.81807213e+03 8.66290236e+03 3.85451984e+02\n",
      " 6.11127095e+00 5.06369392e+02 2.55015708e+02 6.09641850e-01\n",
      " 2.45807639e+02 4.03640999e+02 7.05897164e+02 2.38507309e+02\n",
      " 1.19991723e+02 2.66471120e+02 2.18784092e+02 4.63116818e+02\n",
      " 3.01356708e+02 3.18505899e+02 1.93599509e+01 1.41843570e+00\n",
      " 3.99358984e+00 4.72652449e+03 3.89914756e+03 3.15927236e+01\n",
      " 2.65031475e+02 2.35398778e+02 3.60526969e+00 1.68837803e+03\n",
      " 9.27804829e+01 5.67950638e+02 3.56820942e+01 3.53144786e+00\n",
      " 6.17704806e+01 2.28819065e+01 3.54511674e+02 4.03328595e+02\n",
      " 4.09920200e+01 3.19150485e+01 7.09592336e-01 4.26939803e+03\n",
      " 3.01713910e+03 1.26783624e+02 4.34895299e+00 1.55745910e+00\n",
      " 3.95768990e+03 2.66735907e-01 2.51773843e+03 3.15827630e+02\n",
      " 3.35698519e+00 4.20160657e+01 1.00458330e+02 2.09652035e+01\n",
      " 1.86292393e+00 1.45226643e+03 5.81320669e+02 8.24705162e+01\n",
      " 1.89402140e+02 3.90850648e+03 4.23946847e+03 1.15936265e+01], log_u_hat2:[ 8.22221542e+00  6.20056940e+00  7.47462763e+00  6.79778307e+00\n",
      "  7.09338067e+00  2.67929787e+00  5.37565974e-01  3.60627785e+00\n",
      "  3.87286235e+00  6.57393754e+00  5.20311741e+00  2.86707063e+00\n",
      "  6.04390383e+00  5.62931526e+00  2.75328959e+00  3.60807112e+00\n",
      "  7.14552073e+00  5.48989917e+00 -2.99898532e+00  4.67703128e+00\n",
      "  6.85425940e+00  1.76605543e-01  5.92849706e+00  4.37483256e+00\n",
      "  6.28500669e+00  4.49654961e+00  1.59982389e+00  4.09204053e+00\n",
      "  4.19741668e+00  4.48154445e+00  2.58817407e+00 -6.30220486e-01\n",
      "  6.98160870e+00  7.02921520e+00  4.81334577e+00 -3.76300120e+00\n",
      "  2.20662093e+00  4.99828394e+00  7.97963188e+00  3.79412767e+00\n",
      "  2.78081167e+00  7.10823280e+00  1.15239240e+00  2.48807644e+00\n",
      "  7.79910596e+00  7.71737285e+00  6.65889823e+00  2.34031534e+00\n",
      "  6.02750434e+00 -3.62589486e+00  8.17239784e+00  2.61275416e+00\n",
      "  3.71238888e+00  4.59631390e+00  5.27980663e+00  8.17562168e+00\n",
      "  5.88449808e+00  3.97698229e+00  2.36292922e+00  2.93401991e+00\n",
      "  4.59029846e+00  6.48039818e+00  2.36788584e+00  6.81458053e+00\n",
      "  7.12144069e+00  3.95097358e+00  6.20284692e+00  6.96378020e+00\n",
      "  7.40974353e+00  1.91474238e+00 -6.74773830e-01  3.72332576e+00\n",
      "  6.17427630e+00  6.82872938e+00  6.07739030e+00  8.41358913e+00\n",
      "  4.34458035e+00  9.12503376e+00  7.10687704e+00  3.36345775e-01\n",
      "  7.47671627e+00  3.88307529e+00  1.82903148e+00  5.25376968e+00\n",
      "  2.06653696e+00  3.23354129e+00  2.68968721e+00  2.23980817e+00\n",
      "  5.02805304e+00  2.29087960e+00  7.46306800e+00  5.05567029e+00\n",
      "  3.45075218e+00  8.21019490e+00  8.58120992e+00  5.18207235e+00\n",
      "  5.23164135e+00  3.39333232e+00  6.28734108e+00  3.69244329e+00\n",
      "  4.37659762e-01  9.05390908e+00  5.96388157e+00  4.41822747e+00\n",
      "  7.18377506e+00  2.33465057e+00 -8.15950562e-01  3.09440146e+00\n",
      "  1.14222485e+00  7.93693044e+00  4.17378869e+00  6.28072989e+00\n",
      "  7.60724253e+00  7.00797913e+00 -4.50397929e+00  5.87847689e+00\n",
      "  5.01788430e+00  1.48963808e+00  4.99771938e+00  6.14828502e+00\n",
      "  4.51753249e+00  2.99232458e+00  7.36954079e+00  3.05849205e+00\n",
      "  8.98249333e+00  3.35697541e+00 -6.15205911e-01  6.38231461e+00\n",
      "  6.44443690e+00  4.49768537e+00  3.94600284e-03  9.94965884e-01\n",
      "  4.29067878e+00  6.93124829e+00  5.14112764e+00  5.64847280e+00\n",
      "  7.28025504e+00  2.38125767e+00  4.99251217e+00  4.00756718e+00\n",
      "  7.02271956e+00  3.32901774e+00  1.64870572e+00  4.50245361e+00\n",
      "  7.71117635e+00  6.54195117e+00  4.57900192e+00  6.23746813e+00\n",
      "  1.11479814e+00  4.12010914e+00  4.39023919e-01  6.48099235e+00\n",
      "  3.70576251e+00 -1.56864790e+00  4.26067452e+00  3.27696002e+00\n",
      "  3.16028495e-01  3.05320039e+00  6.00017310e+00  3.27032995e+00\n",
      "  5.03970684e+00  5.27752096e+00  5.86980279e+00 -7.73689580e-01\n",
      "  3.57701539e+00  6.42069496e+00  1.96264296e-01  4.46297275e+00\n",
      " -5.03049759e+00  3.61691925e-01  3.72912164e+00  8.49325963e+00\n",
      "  6.60525302e+00  6.51566498e+00  3.33080324e+00 -4.88941434e+00\n",
      "  7.14990769e+00  7.49472772e+00  3.97743675e+00 -4.82828405e-01\n",
      "  3.46501918e+00  2.57606420e+00  7.58987891e+00  7.23673695e+00\n",
      "  6.42421568e+00  8.04392545e+00  7.00038991e+00  5.80277905e+00\n",
      "  8.71565131e+00  5.26305851e+00  8.50375649e+00  4.45003606e+00\n",
      "  7.70077946e+00  3.38627756e+00  5.66862618e+00  5.96116330e+00\n",
      "  6.57931566e+00  7.43484820e+00  6.69864293e+00  9.21276888e+00\n",
      "  4.95606442e+00  3.09730222e+00 -1.68578550e+00  3.43708507e+00\n",
      "  5.94574245e+00  7.86641136e+00  6.17259798e+00  8.78937330e-01\n",
      "  4.48667630e-01  5.39400264e+00  6.19934627e+00  8.19232894e+00\n",
      "  1.97056381e+00  5.51119612e+00  4.17732101e+00  7.20927616e+00\n",
      "  7.72235809e+00  1.78056665e+00  6.32790819e+00  5.49576069e+00\n",
      "  6.49620520e+00  6.02712189e+00  5.27908402e+00  4.47495919e+00\n",
      "  2.42522329e+00  6.36941525e+00  4.31536918e+00  2.32433560e+00\n",
      "  5.98058144e+00  5.28201220e+00  4.87734228e+00  5.29520576e+00\n",
      "  8.35354437e+00  5.19969666e+00  3.39574209e+00  2.41194721e+00\n",
      "  6.56435721e+00  7.05404217e+00  2.68945799e+00 -3.33861332e-01\n",
      "  2.43786788e+00  6.38737575e+00  7.38945639e+00  5.59082317e+00\n",
      "  4.86634120e+00  5.14269048e+00  1.43023250e+00  8.15629910e+00\n",
      "  6.41941760e+00  2.39305276e+00  6.60481502e+00  4.03886512e+00\n",
      "  1.64134387e+00 -1.59964462e+00  6.85845333e+00  4.95789188e+00\n",
      "  4.65414829e-01  1.74182596e+00  8.63694873e+00  8.99833052e-01\n",
      "  1.16434304e+00  6.58614200e+00  4.12887307e+00  7.68408457e+00\n",
      "  4.27782104e+00  5.94633456e+00  7.77698700e+00  8.09166233e-01\n",
      "  4.12019402e+00  4.00604845e+00  4.43329029e+00  6.84714350e+00\n",
      "  8.28430511e+00  7.41750086e+00  7.58172910e+00  7.85622146e+00\n",
      "  3.52724688e+00  5.93201447e+00  4.97226548e+00  4.12522487e-01\n",
      "  6.84687847e+00  8.66841989e+00  9.23810427e+00  6.45233524e+00\n",
      "  6.83565793e+00  2.52132069e+00  1.72823393e+00  4.91129096e+00\n",
      "  6.61241745e+00  2.69308133e+00  2.60625405e+00  6.17916901e+00\n",
      "  5.49226618e+00 -9.31085859e-01  1.93257962e+00  3.24144958e+00\n",
      "  3.98880138e+00 -1.56791129e+00  3.11022149e+00  4.68490063e+00\n",
      "  7.12709703e+00  3.65643658e+00  4.56111757e+00  5.23903175e+00\n",
      "  2.63558468e+00  7.49884645e+00  5.90652435e+00  5.01893540e+00\n",
      " -2.95667155e-01  4.22155786e+00  4.45775339e+00  1.66561579e+00\n",
      "  5.90497627e+00  4.36356502e+00  7.92715949e+00 -1.83635408e+00\n",
      "  3.86894767e+00  4.00218893e+00  6.18345839e+00  4.59392729e+00\n",
      "  2.59728074e+00  4.54102695e+00  5.73312913e+00  7.30934563e+00\n",
      "  7.02701855e+00  4.93052290e+00  4.65844020e+00  6.17638461e+00\n",
      "  1.46991780e+00  6.21392932e+00 -2.63288748e+00  7.62314605e+00\n",
      "  6.44192904e+00  6.84620816e+00  5.08036663e+00  1.29736436e+00\n",
      "  7.39636560e+00  7.08281843e+00  1.92158445e+00  7.84444363e+00\n",
      "  4.93938534e+00  3.27765836e+00  5.29525310e+00  3.24848513e+00\n",
      "  5.89749977e+00  6.94879616e+00  8.15941944e+00  4.08101544e-01\n",
      "  1.43565012e+00  6.34973077e+00  9.27562286e+00  3.09433730e+00\n",
      "  8.89000117e+00 -1.88255727e+00  1.88206967e+00  3.02227416e+00\n",
      "  5.84725062e+00  4.71149173e+00  6.09618554e+00 -1.48971220e+00\n",
      "  5.12199374e+00  8.26076255e+00  5.69230625e+00  7.52538260e+00\n",
      "  5.17982388e+00  2.28046619e+00  5.69431751e+00  5.98285247e+00\n",
      " -3.34952743e+00  2.95059973e+00  5.49370951e+00  7.35242330e+00\n",
      " -1.28168832e+00  5.31693759e+00  2.15136726e+00  4.93731653e+00\n",
      " -2.15742639e-01  3.85159502e+00  8.72683802e+00  6.35644775e+00\n",
      "  6.46366155e+00  5.38864372e+00  1.69025651e+00  8.25768239e+00\n",
      "  6.72926173e+00  1.82668178e+00  3.14724519e+00  5.92845644e+00\n",
      "  7.45007803e+00  7.11402111e+00  5.78633946e+00  7.29862113e+00\n",
      "  2.70066720e+00  7.15763776e+00  4.40095340e+00  1.10999913e+00\n",
      "  4.39497814e+00  5.71433063e+00  8.13514959e+00 -1.84663695e-01\n",
      "  4.96631555e+00  9.05759431e+00  6.12122366e+00 -6.32207013e+00\n",
      "  5.92420910e+00  3.01252731e+00 -3.18881963e+00 -5.52879732e+00\n",
      "  6.68129000e+00  5.33276991e+00  5.06748161e+00  6.37334922e+00\n",
      "  5.13753003e+00  6.14392041e+00  4.14722204e+00  8.09223207e+00\n",
      "  2.03687471e+00  4.18482071e+00  8.67249799e+00  4.30513656e+00\n",
      "  8.04737318e+00  3.45328705e+00  3.84625007e+00 -4.34409041e+00\n",
      "  6.75119592e+00  6.72424274e+00  4.77758778e+00  4.13572758e+00\n",
      "  2.34213012e+00  1.57890142e+00 -2.38535184e+00  8.21050338e+00\n",
      "  5.81495748e-01  5.37916239e+00  6.08914747e+00  7.02082732e+00\n",
      "  4.28683330e+00  5.83592548e+00  4.20505589e+00  4.82569418e+00\n",
      "  7.19433757e+00  7.94380829e+00  9.06680509e+00  5.95441663e+00\n",
      "  1.81013476e+00  6.22726643e+00  5.54132514e+00 -4.94883625e-01\n",
      "  5.50454927e+00  6.00052587e+00  6.55946957e+00  5.47439996e+00\n",
      "  4.78742277e+00  5.58526587e+00  5.38808536e+00  6.13797933e+00\n",
      "  5.70829464e+00  5.76364100e+00  2.96320655e+00  3.49554648e-01\n",
      "  1.38469053e+00  8.46094543e+00  8.26851323e+00  3.45292683e+00\n",
      "  5.57984859e+00  5.46128100e+00  1.28239658e+00  7.43152360e+00\n",
      "  4.53023630e+00  6.34203451e+00  3.57464900e+00  1.26170795e+00\n",
      "  4.12342559e+00  3.13034649e+00  5.87074128e+00  5.99975160e+00\n",
      "  3.71337741e+00  3.46307764e+00 -3.43064648e-01  8.35922812e+00\n",
      "  8.01206434e+00  4.84248188e+00  1.46993513e+00  4.43055713e-01\n",
      "  8.28341578e+00 -1.32149622e+00  7.83111633e+00  5.75519659e+00\n",
      "  1.21104331e+00  3.73805206e+00  4.60974301e+00  3.04286409e+00\n",
      "  6.22147259e-01  7.28088067e+00  6.36530253e+00  4.41244085e+00\n",
      "  5.24387248e+00  8.27091060e+00  8.35219318e+00  2.45045551e+00]\n",
      "log_sigma2_hat:[6.01819966 5.97098891 6.07806757 6.07590089 5.99519354 5.79793532\n",
      " 6.05948024 4.30870583 3.77400631 6.10075649 5.54956507 1.20425385\n",
      " 3.70028019 5.74878754 1.82401057 4.72794523 6.09606389 6.08947997\n",
      " 0.72301954 5.75338112 5.55633214 0.61506187 6.06605214 6.01897833\n",
      " 5.83571461 4.58339438 6.1002146  6.07775685 4.16877691 5.22016319\n",
      " 1.49778306 5.78947768 5.76316981 6.03819256 6.06222371 5.09296838\n",
      " 1.97331081 3.41838867 6.10165197 6.09660944 3.77346126 4.82451463\n",
      " 1.56299413 2.77812879 5.34732927 6.07933327 4.96505954 4.12742833\n",
      " 5.97495925 1.36893058 6.00787409 6.01616669 5.4325621  6.02993072\n",
      " 4.46300448 5.84015367 5.95809921 5.8000199  2.37361231 2.06724261\n",
      " 3.5201154  5.04169867 3.5625261  5.88762477 6.10112156 5.1997707\n",
      " 3.66489089 5.83364784 5.03442085 2.24497291 1.66566123 5.34052082\n",
      " 5.93466161 5.36644092 5.46677741 6.06096182 5.20299898 6.00122582\n",
      " 6.01812723 2.34265978 6.09131536 2.68612642 3.4503173  3.82266169\n",
      " 1.98334772 2.25621794 6.10066268 3.64184688 2.71566429 5.17385443\n",
      " 5.63331807 5.09572173 2.08554769 5.90014051 6.08931189 3.17191725\n",
      " 5.25837114 4.32006194 6.06716067 4.50638149 0.55238676 6.06394597\n",
      " 5.69160556 3.70123558 5.85386598 2.62581452 3.58351227 4.36449697\n",
      " 1.69124673 5.87964721 1.09318333 4.01612956 5.85586451 3.25843345\n",
      " 2.44587342 3.48176395 3.87136537 1.82837437 4.52152443 5.80586875\n",
      " 6.10188681 3.14134709 5.80302525 5.78652243 6.04086839 2.3493087\n",
      " 4.27265045 6.0908283  5.74783363 2.28789926 1.41796964 1.09456078\n",
      " 4.73931229 6.10121132 6.05963208 6.09790969 5.97720397 1.36131032\n",
      " 6.08951265 3.89375421 5.19638383 2.83978668 1.67727585 6.09606382\n",
      " 6.10067566 6.07698332 2.98469388 5.59937548 0.96517988 4.30586069\n",
      " 6.07253056 6.10090037 3.11126281 5.28245048 1.44326724 2.81664958\n",
      " 1.72436013 1.20657648 6.08134455 2.32516316 6.05044446 5.18983593\n",
      " 6.0390512  2.8341152  3.17561187 5.55609916 5.89424378 6.06447584\n",
      " 4.20310765 5.96407411 5.1116773  6.03064345 6.10190736 4.62103745\n",
      " 5.45008803 5.81544984 4.19119709 3.88865579 5.40982296 0.79403604\n",
      " 0.85403668 5.34694992 6.05614963 5.86908773 5.68722063 6.01905075\n",
      " 5.85795553 5.87539375 5.92445709 6.09669278 5.96089878 6.02106677\n",
      " 6.01164587 4.06345043 4.80947349 4.66853834 5.61607365 6.09805038\n",
      " 6.09034681 6.10164544 4.11698071 5.02784231 2.21301233 2.78508419\n",
      " 5.87118563 5.99878899 2.35498551 1.28093851 1.13079786 5.98500885\n",
      " 4.14279858 6.07798565 1.97206077 6.0777859  6.0879817  5.41735735\n",
      " 5.85968586 0.63725683 5.52476189 4.72826817 6.09616387 6.10200508\n",
      " 6.01585271 5.11296766 0.80760891 5.98626203 2.2583017  3.33597228\n",
      " 5.55328076 5.22498362 5.47137546 4.38736687 5.87758394 2.46672198\n",
      " 5.32943014 1.3173075  5.25505613 5.35546022 4.61034289 4.7151932\n",
      " 3.2032216  6.05284008 5.99047448 2.49820574 5.79415763 2.93498851\n",
      " 3.95648733 6.10200651 6.06468717 3.10138581 6.08489975 2.38651811\n",
      " 6.1016711  0.90633249 4.20389842 6.03624891 2.70735505 1.36690953\n",
      " 6.01884968 3.47614347 5.88030166 3.98926588 3.26046624 5.32488402\n",
      " 4.48760991 6.09605742 5.80328486 6.10192033 4.55546018 3.78500069\n",
      " 5.40861219 6.00234129 5.99685021 5.94783075 6.08058167 6.10137925\n",
      " 2.86646092 2.60473232 6.02583494 6.0020953  6.04041133 5.69738302\n",
      " 5.94626059 5.55213403 3.96865573 3.12909413 1.012698   3.91462612\n",
      " 6.07012807 3.41314829 2.78378232 4.78006063 3.2422632  0.9865096\n",
      " 1.8747615  5.87592235 4.41354865 2.76498322 6.08016257 5.82118717\n",
      " 5.95830661 3.11950385 4.87430118 5.44912104 5.96019896 5.77591003\n",
      " 2.95615454 3.56221197 0.68669551 6.10176856 6.09584426 0.95715103\n",
      " 4.06016293 5.90946687 6.08738811 2.07088666 3.72746171 4.57414531\n",
      " 5.88471377 3.5835737  3.22224476 1.58687682 6.09863613 6.06525304\n",
      " 5.42448366 5.64908021 6.0732285  5.21459944 5.87903446 4.39858228\n",
      " 1.8971909  5.90791817 5.3566574  5.73369501 4.06243329 0.87110718\n",
      " 5.01458123 6.10200859 6.02288264 6.06653162 5.18300252 5.8325998\n",
      " 3.21326193 1.73761433 5.93902831 5.90399096 5.40036907 0.65630737\n",
      " 1.40059422 5.79544887 6.09998799 2.41408693 6.1018459  1.4063374\n",
      " 1.55237382 2.24488256 3.99909725 6.05624656 5.12661269 5.93205604\n",
      " 5.17865171 5.84993449 5.93197396 6.03468414 4.20773519 2.36229331\n",
      " 3.81886283 5.90382362 1.6015962  2.05595959 5.07012101 6.03140196\n",
      " 0.62598862 5.35905643 6.02159799 5.13013264 0.93732692 3.40041482\n",
      " 6.10106522 5.91074504 5.58277694 6.01184755 3.42591269 6.01553605\n",
      " 5.09170765 6.06971958 3.1623984  5.92639519 6.08461671 6.06098933\n",
      " 5.87228263 6.0722437  4.10604651 6.07242941 1.87100868 2.53471228\n",
      " 1.91046546 6.01973453 6.0930792  1.19499114 3.37584514 6.05938108\n",
      " 6.01628378 1.19524903 6.05207778 1.30601731 0.78611453 4.86573855\n",
      " 5.75184258 5.34168123 5.64987164 6.08992704 5.73312319 5.23489056\n",
      " 6.10089715 6.04965356 1.84522404 2.08947136 5.76402546 6.10093146\n",
      " 5.94967169 5.54621405 2.19837267 2.938211   5.86670667 4.54147354\n",
      " 1.84531701 2.79409982 2.80968428 6.04766884 2.06147888 6.05751621\n",
      " 1.66408449 4.51683071 5.9917843  5.22803285 3.54770339 5.03247963\n",
      " 4.98021958 5.68373208 6.06000552 5.74840109 6.10184023 5.14646827\n",
      " 6.06748281 5.8416873  3.77918717 6.08314852 6.10154818 3.39964461\n",
      " 4.14577177 4.49428709 4.2683176  3.49800078 4.26627998 6.06915562\n",
      " 3.38101653 3.0656605  2.55162048 5.47615967 2.39493198 6.10199883\n",
      " 6.04527011 1.21424935 6.01988705 5.36873773 3.74729393 5.80377324\n",
      " 1.94951933 5.76469287 2.30553383 5.91323079 6.07844513 2.61955481\n",
      " 5.70735148 6.00345503 5.79560937 4.90757548 3.73033964 5.76947475\n",
      " 6.05324533 3.92014814 1.03539834 2.06471582 6.02429371 4.95556278\n",
      " 5.94267525 3.72947514 2.32259054 6.06444225 5.72125475 5.21349128\n",
      " 0.70534897 5.91119993 5.85760651 6.05519044 3.4577755  5.96631748\n",
      " 5.96190006 2.55392666], sigma2_hat:[410.83827936 391.89302518 436.18548075 435.24142985 401.49438125\n",
      " 329.61830059 428.15284178  74.34421242  43.55420756 446.19518385\n",
      " 257.12570046   3.33427029  40.4586387  313.80994609   6.1966608\n",
      " 113.06300552 444.10627538 441.19192006   2.06064603 315.25477418\n",
      " 258.87158707   1.84977104 430.97588557 411.1583148  342.30926383\n",
      "  97.84595741 445.95345941 436.04997049  64.63634775 184.96436613\n",
      "   4.47176443 326.84226349 318.35585861 419.13478999 429.32907741\n",
      " 162.8726126    7.19445658  30.52019739 446.59492273 444.34862366\n",
      "  43.53047484 124.52601209   4.77309114  16.088887   210.04657042\n",
      " 436.73790947 143.31708335  62.0182272  393.45206799   3.93114442\n",
      " 406.6179687  410.0039064  228.73453609 415.68622894  86.74775026\n",
      " 343.83217233 386.87406023 330.3061333   10.73610444   7.90300137\n",
      "  33.78832733 154.7326317   35.25213529 360.54788111 446.35810662\n",
      " 181.23068051  39.05187514 341.60252027 153.61060283   9.44015983\n",
      "   5.28916944 208.62133535 377.91209116 214.09951187 236.69618787\n",
      " 428.78765463 181.81669019 403.92362816 410.80852347  10.40888517\n",
      " 442.00242295  14.67472192  31.51038906  45.72575407   7.2670303\n",
      "   9.54691379 446.15332782  38.16225263  15.11464727 176.59419718\n",
      " 279.5882733  163.32167687   8.04899863 365.08876465 441.11777001\n",
      "  23.85317305 192.16822143  75.19328527 431.4538987   90.59341197\n",
      "   1.73739482 430.06913384 296.36907665  40.49731117 348.57938112\n",
      "  13.8158228   35.99975995  78.60984709   5.42624155 357.68303069\n",
      "   2.98375725  55.48593488 349.2767237   26.00876125  11.54062499\n",
      "  32.51702978  48.00788997   6.22376091  91.97570139 332.24370522\n",
      " 446.69981428  23.13501075 331.30031149 325.87778981 420.2578222\n",
      "  10.47832357  71.71145139 441.78719399 313.51074456   9.85421479\n",
      "   4.12872912   2.98787007 114.35553069 446.39817477 428.21785712\n",
      " 444.92676219 394.33624787   3.90130192 441.20633475  49.09485343\n",
      " 180.61791495  17.11211486   5.35095929 444.10624294 446.15912058\n",
      " 435.71280202  19.7804462  270.25757261   2.62525984  74.13299342\n",
      " 433.77699219 446.25938607  22.44937585 196.85166594   4.23450841\n",
      "  16.72073523   5.6089309    3.34202357 437.61719552  10.22834879\n",
      " 424.30157224 179.43910958 419.49482913  17.01533848  23.94146454\n",
      " 258.81128402 362.94226857 430.29707473  66.8938913  389.19251243\n",
      " 165.9484666  415.98260812 446.70899489 101.59938179 232.77865728\n",
      " 335.44226128  66.10187342  48.84518408 223.59199888   2.21230739\n",
      "   2.34911035 209.96690537 426.72920533 353.92595855 295.07236555\n",
      " 411.18809124 350.00783051 356.1648712  374.07529018 444.38565708\n",
      " 387.95865542 412.0178894  408.15453714  58.17469289 122.6670155\n",
      " 106.54190106 274.80826824 444.98936344 441.57452874 446.59200561\n",
      "  61.37365763 152.60338713   9.14321733  16.20118181 354.66923781\n",
      " 402.94053383  10.53797619   3.60001681   3.0981274  397.42603958\n",
      "  62.9788265  436.14975273   7.18546886 436.0626372  440.53139041\n",
      " 225.28299036 350.61398456   1.89128563 250.82660759 113.09952319\n",
      " 444.15067643 446.75264711 409.87519331 166.16273879   2.24253945\n",
      " 397.92439672   9.56682801  28.1056966  258.08287687 185.85812601\n",
      " 237.78703453  80.42836151 356.9457952   11.78375613 206.32036741\n",
      "   3.73335579 191.53223676 211.76140973 100.51861046 111.63037575\n",
      "  24.6116917  425.31925726 399.60417027  12.16065504 328.37545342\n",
      "  18.82128689  52.27338386 446.75328504 430.38801814  22.22873465\n",
      " 439.1757847   10.87556047 446.60346611   2.47522795  66.94680971\n",
      " 418.32092689  14.98957638   3.9232074  411.10542219  32.33478133\n",
      " 357.91719492  54.0152211   26.06168531 205.38453822  88.9086919\n",
      " 444.10340059 331.38633214 446.71478662  95.15053138  44.03570112\n",
      " 223.32144383 404.37444582 402.16007789 382.92178291 437.28347714\n",
      " 446.47314367  17.5747097   13.52760372 413.98715112 404.2749834\n",
      " 420.06578661 298.0862939  382.32100653 257.78709461  52.91335334\n",
      "  22.85326804   2.75301866  50.13032509 432.73609708  30.36067828\n",
      "  16.18010377 119.11157135  25.59157508   2.68185737   6.51926406\n",
      " 356.35319175  82.56192733  15.87877353 437.10024771 337.37233449\n",
      " 386.95430593  22.63514634 130.8826574  232.5536711  387.68725086\n",
      " 322.43772806  19.22390461  35.2410633    1.98713819 446.64699394\n",
      " 444.00874609   2.60426641  57.98375766 368.50963962 440.26997247\n",
      "   7.93185282  41.57344856  96.94514573 359.4998548   36.00197148\n",
      "  25.0843655    4.88845753 445.25009425 430.63163285 226.89416116\n",
      " 284.03009794 434.07984765 183.93812844 357.4639306   81.33547601\n",
      "   6.66713946 367.9393705  212.01507882 309.10932429  58.11555106\n",
      "   2.38955506 150.59306012 446.75421344 412.76673912 431.18257968\n",
      " 178.2171087  341.24469337  24.86004592   5.68376762 379.56593138\n",
      " 366.49723012 221.48814656   1.92766104   4.05761036 328.799738\n",
      " 445.85241571  11.17955793 446.68153681   4.08098099   4.72266767\n",
      "   9.4393069   54.54888377 426.770569   168.44557338 376.92869708\n",
      " 177.44340504 347.21163235 376.89776105 417.66686348  67.20416252\n",
      "  10.61526761  45.5523781  366.43590448   4.96094478   7.81433284\n",
      " 159.19358963 416.29825379   1.87009386 212.52432024 412.23681919\n",
      " 169.03953727   2.55314752  29.97653244 446.33295785 368.98095987\n",
      " 265.80871598 408.23686258  30.7506979  409.74542402 162.6674046\n",
      " 432.55936542  23.62719549 374.80099108 439.05149472 428.79945333\n",
      " 355.0585228  433.65257832  60.70624113 433.73311959   6.49484432\n",
      "  12.61280135   6.75623282 411.46934947 442.78273179   3.3035285\n",
      "  29.24899282 428.11038731 410.0519182    3.30438054 424.99516008\n",
      "   3.69144251   2.1948518  129.76674206 314.7701166  208.86356228\n",
      " 284.25497563 441.38920732 308.93261878 187.70856161 446.25795308\n",
      " 423.9661246    6.3295177    8.08064227 318.62837623 446.27326335\n",
      " 383.62736814 256.26550982   9.01033878  18.88203605 353.08424092\n",
      "  93.82895943   6.33010618  16.34790606  16.60467491 423.125506\n",
      "   7.85758166 427.31276257   5.28083638  91.54500482 400.12791991\n",
      " 186.42571436  34.73345677 153.31270051 145.50632834 294.04478441\n",
      " 428.37780337 313.68869839 446.67900605 171.82358268 431.592913\n",
      " 344.35988959  43.78044119 438.40735807 446.54857204  29.95345312\n",
      "  63.16635281  89.50433756  71.40140906  33.04931316  71.2560681\n",
      " 432.31548681  29.40064258  21.44862423  12.82787431 238.92738439\n",
      "  10.96745206 446.74985439 422.11176047   3.3677651  411.53211099\n",
      " 214.59182461  42.40617249 331.54821246   7.0253099  318.84110386\n",
      "  10.02953092 369.89929317 436.35019849  13.72960989 301.0726136\n",
      " 404.8250633  328.85251422 135.31095197  41.69326655 320.36941532\n",
      " 425.4916544   50.40791156   2.81622782   7.88305735 413.34959217\n",
      " 141.96247796 380.95271079  41.65723826  10.20206897 430.2826195\n",
      " 305.28774294 183.73440877   2.02455307 369.14884123 349.88569334\n",
      " 426.32008726  31.74627837 390.06659356 388.34730716  12.85749182]\n",
      "gamma_hat2:[ 2.08039078 -0.59482435  0.89757836]\n",
      "sigma2_tilde: [1768.29279128 1016.16563298 1232.93917015 1225.43883227 1051.44629436\n",
      "  836.34227154 1660.59579246  276.70307672  188.52047195 1450.25810807\n",
      "  672.13020651   10.48783901  178.42728843  798.06326262   28.80149025\n",
      "  369.99480939 1319.53826408 1542.7378098     3.56223216 2185.74973357\n",
      "  675.8300724     2.78324207 1194.9718984  1766.52244111  868.60156953\n",
      "  334.8310095  1359.25286223 1231.84186648  250.78651607  523.06305576\n",
      "   17.73423891  829.48009173 2173.57298678 1720.179155   1651.99858585\n",
      "  477.41517964   35.01940837  143.6240856  1430.70119583 1323.6518183\n",
      "  188.44423445  395.57327171   19.68282501   83.17126612  574.46149045\n",
      " 1237.4895294   436.25018426  243.54782181 1857.32440474   14.25234264\n",
      " 1791.11247499 1772.88054454  612.87112809 1111.36366425  308.06816217\n",
      "  872.58037931  998.93196483  838.05262126   55.65951334   39.3278939\n",
      "  155.52566389  460.39334279  160.7010317  2003.58539191 1443.6030928\n",
      "  515.38874221  173.73572133  866.76348873  458.03547239   48.36247411\n",
      "   23.01290896  571.54119978 1928.87317637  582.77051713  629.34558869\n",
      " 1655.98582639  516.59404394 1060.94805388 1089.45474095   53.84348557\n",
      " 1532.29856724   76.26697118  147.28341839  195.42581367   35.46488855\n",
      "   48.97433705 1365.78797065  170.73276285   78.44003488  505.84027733\n",
      "  720.43135975  478.35031156   40.20431752  931.09914647 1543.65604967\n",
      "  117.57669316  537.84221872  278.90760967 1198.15433929  317.46538453\n",
      "    2.45343289 1189.06895897  757.75234083  178.55530271 2053.00258748\n",
      "   71.95511618  163.30994107  287.68782446   23.89381015 2015.53998882\n",
      "    8.36518318  224.94524923  887.01625056  126.29116331   60.05182978\n",
      "  150.95513579  202.53994504   28.97256395  320.80982208  842.8926944\n",
      " 1395.54728169  114.60305806  840.5319663  2143.93610221 1133.55227011\n",
      "   54.23032205  269.80732895 1535.14736342  797.35616382   50.72466971\n",
      "   15.52073077    8.38951009  372.9138866  1441.76791103 1177.51061035\n",
      " 1484.52075258 1024.83353903   14.06141662 1542.55864119  205.87987506\n",
      "  514.12806397   88.02645114   23.41024171 1319.53772676 1365.99270816\n",
      " 1600.01921967  100.19124737  700.1705141     6.31832739  276.15322811\n",
      " 1616.92912754 1369.71109807  111.72917773  547.43768915   16.20216271\n",
      "   86.18266488   25.06466676   10.53574784 1245.00839372   52.83400558\n",
      " 1687.2472587   511.70180613 1129.72026655   87.57204048  117.93975983\n",
      "  675.70214552 1993.52439563 1190.53694796  256.93802039 1006.80181339\n",
      "  483.81297292 1739.04843155 1396.68274306  343.65288927  621.22836828\n",
      "  850.95659047  254.78906987  205.11538876  602.27202002    4.22345272\n",
      "    4.87620457  574.29823873 1670.69672338 2031.09154782  754.82222553\n",
      " 1766.35742005  888.9810503   905.79420641 1945.76966863 1324.29879396\n",
      " 1883.27218485 1761.73735129 1078.1655201   232.70142836  391.47005159\n",
      "  355.11500894  710.01323024 1335.68346788 1537.90574071 1386.21003415\n",
      "  241.74761305  455.91627106   46.65007891   83.70966616  901.66474861\n",
      " 1057.07117258   54.56201124   12.14732748    9.04737017 1838.00454896\n",
      "  246.21722358 1596.02281311   34.96417319 1596.82541849 1550.72759212\n",
      "  605.75412928  890.61496874    2.92143276  658.84416918  370.07741327\n",
      " 1320.27702211 1410.75422777 1085.43813488  484.25803564    4.36350988\n",
      " 1037.93208692   49.08825731  134.48957593  674.15766058  524.89844701\n",
      "  631.61000722  292.30472083 2018.60250965   61.35949298  566.82782673\n",
      "   12.99104711  536.5385953   577.976199    341.12349942  366.74803216\n",
      "  120.67834011 1160.5498709  1827.19223424   63.36909913 2134.05523693\n",
      "   95.89637121  215.47896504 1410.27412971 1644.04249271  110.79687032\n",
      " 1259.31919651   56.4282078  1430.09486306    5.51666032  257.08125447\n",
      " 1725.12958671   77.82461007   14.20154422 1090.74347851  150.29390861\n",
      "  910.67125638  220.63954567  126.50133904  564.91119549  313.36576039\n",
      " 1319.49065209  840.7468965  1397.45954798  328.42837161  190.06353065\n",
      "  601.7151537  1802.88224667 1054.02424398 1906.40215686 1242.11249173\n",
      " 1379.20857199   90.18517057   70.48691124 1750.60072347 1062.34441728\n",
      " 1714.44386526  761.64576343 1909.12346193  673.53089268  217.38291404\n",
      "  113.42632193    7.02920153  209.03381682 1625.5567028   143.03001137\n",
      "   83.60871375  383.57589222  124.62824689    6.63034376   30.83031063\n",
      "  906.31630521  297.67410754   82.16008493 1587.0637041   855.86880185\n",
      "  999.20185978  112.51129177  409.48714988  620.76286324 1001.67517947\n",
      " 2157.51042172   97.70937498  160.66222376    3.26997151 1390.35526293\n",
      " 1502.0108312     6.20384991  232.1553888   941.1313431  1270.29937353\n",
      "   39.50139762  182.09759301  332.6977159   915.11328318  163.31762498\n",
      "  122.59152641   20.42856431 1477.36913498 1192.71070777  609.07465628\n",
      "  730.19127191 1614.36183014  520.95486189  909.40564208  294.5937142\n",
      "   31.75452949  939.44511583  578.49622251  787.02129039  232.53236932\n",
      "    5.07788582  451.67888157 1407.83403281 1098.0571576  1637.92528013\n",
      "  509.18513487 2082.70407537  121.68541927   25.54313119  975.04143421\n",
      "  935.20579461  597.94360274    3.04905563   15.06342184 2132.37443224\n",
      " 1356.26594748   58.09313371 1393.54322902   15.21361565   19.35676518\n",
      "   48.3575775   222.20725512 1168.88353554  488.99492641  966.7279352\n",
      "  507.59088401 2058.57042473  966.63130518 1729.06715002  257.77721584\n",
      "   54.99091401  194.8794935   935.02630228   20.89684759   38.79371873\n",
      "  469.73917383 1114.23276915    2.8498665   579.54026169 1095.7049854\n",
      "  490.22602316    5.92811758  141.59408057 1444.70841486 1967.83698075\n",
      "  690.61388044 1782.46963603  144.48024589 1084.88375497  476.98772777\n",
      " 1626.99309455  116.64495677  960.12834958 1567.30758288 1181.07490301\n",
      " 2026.41784103 1213.50075652  239.87580888 1617.29881904   30.67733354\n",
      "   65.75250829   32.30956239 1764.79609312 1521.42040007   10.29822682\n",
      "  138.85357877 1660.90167533 1086.19469774   10.30347451 1682.59865975\n",
      "   12.72518278    4.14375262  407.05700538 2187.65105599  572.03747175\n",
      "  730.6875181  1282.5724151  2210.52269787  528.69664589 1447.82468388\n",
      " 1689.47404468   29.6390563    40.39376249  809.53445749 1447.20850691\n",
      "  988.15489937  670.31018789   45.87877355   96.17088837  897.32089404\n",
      "  325.26745025   29.64276      84.41103638   85.63277477 1694.99539679\n",
      "   39.05444624 1666.59442777   22.95929574  319.76958229 1046.21308647\n",
      "  526.0637249   158.87757889  457.40893971  440.90855248  752.5061729\n",
      " 1658.969951   2191.8919719  1393.28672219  495.98932721 1199.08945167\n",
      "  873.96496527  189.24635709 1574.05901859 1433.74139479  141.50756934\n",
      "  246.73649106  314.81820968  268.98910811  152.87762319  268.6050869\n",
      " 1204.0229284   139.42711205  107.47042886   66.87598526  633.97924269\n",
      "   56.93302181 1404.75479059 1143.10571207   10.69505401 1764.44704622\n",
      "  583.78037973  184.81280664 2121.46868912   33.97747671  810.04463976\n",
      "   51.71602225 1963.8876317  1594.1647067    71.51708849  768.45332217\n",
      " 1064.54221939 2132.16531     419.08215585  182.4896454   813.7196047\n",
      " 1679.2329296   209.87482477    7.38909366   39.20786799 1100.66586468\n",
      "  433.36039917  979.47531308  182.37180724   52.68660933 1644.84415988\n",
      "  778.14519788  520.53626205    3.41620382  943.02816663  888.65236545\n",
      " 1673.54265134  148.14812345 1009.80871019 1003.91530061   67.0301894 ], omega_tilde_inv:[[0.00056552 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.00098409 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.00081107 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.00099029 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.0009961  0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.01491865]]\n",
      "beta_fgls: [-2.93734454  0.80733604]\n",
      "se_beta1_fgls:0.05494447910720293\n",
      "t_stat: 0.13351727209454856, tp_value:0.8938382416141735, np_value:0.8937843054089667, chip_value:0.7148127267639317\n",
      "beta_0 = -2.9373445414769748, beta_1 = 0.8073360369670497, se_beta1 = 0.05494447910720292\n",
      "pval_beta1 = 7.321807610616539e-41, t_test_res = 0.8938382416141735, t_statistic = 0.1335172720945486\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm, t, chi2\n",
    "\n",
    "np.random.seed(3649)\n",
    "\n",
    "\n",
    "n_obs = 500\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "\n",
    "x = np.random.uniform(1, 50, n_obs)\n",
    "u = np.random.normal(scale=x)\n",
    "y = beta_0_true + beta_1_true * x + u\n",
    "\n",
    "X = np.column_stack((np.ones_like(x), x)) \n",
    "beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y \n",
    "u_hat = y - X @ beta_ols\n",
    "u_hat2 = u_hat ** 2  \n",
    "log_u_hat2 = np.log(u_hat2)\n",
    "\n",
    "print(f\"u_hat2: {u_hat2}, log_u_hat2:{log_u_hat2}\")\n",
    "\n",
    "\n",
    "X_aux = np.column_stack((np.ones_like(x), x, x**2))\n",
    "gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ log_u_hat2\n",
    "log_sigma2_hat = X_aux @ gamma_hat\n",
    "sigma2_hat = np.exp(log_sigma2_hat)\n",
    "\n",
    "print(f\"log_sigma2_hat:{log_sigma2_hat}, sigma2_hat:{sigma2_hat}\")\n",
    "\n",
    "X_aux2 = np.column_stack((np.ones_like(x) / sigma2_hat, x / sigma2_hat,  x**2 / sigma2_hat))\n",
    "u_hat2_over_sigma2_hat = u_hat2 / sigma2_hat\n",
    "gamma_hat2 = np.linalg.inv(X_aux2.T @ X_aux2) @ X_aux2.T @ u_hat2_over_sigma2_hat\n",
    "\n",
    "print(f\"gamma_hat2:{gamma_hat2}\")\n",
    "\n",
    "sigma2_tilde = X_aux @ gamma_hat2 \n",
    "sigma2_tilde = np.maximum(sigma2_tilde, 1e-6)\n",
    "omega_tilde_inv = np.linalg.inv(np.diag(sigma2_tilde))\n",
    "\n",
    "print(f\"sigma2_tilde: {sigma2_tilde}, omega_tilde_inv:{omega_tilde_inv}\")\n",
    "\n",
    "beta_fgls = np.linalg.inv(X.T @ omega_tilde_inv @ X) @ X.T @ omega_tilde_inv @ y\n",
    "\n",
    "print(f\"beta_fgls: {beta_fgls}\")\n",
    "\n",
    "residuals_fgls = y - X @ beta_fgls\n",
    "s2_fgls = (residuals_fgls.T @ omega_tilde_inv @ residuals_fgls) / ( X.shape[0] - X.shape[1])\n",
    "var_beta_fgls = s2_fgls * np.linalg.inv(X.T @ omega_tilde_inv @ X)\n",
    "se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]\n",
    "\n",
    "print(f\"se_beta1_fgls:{se_beta1_fgls}\")\n",
    "\n",
    "t_stat = (beta_fgls[1] - 0.8) / se_beta1_fgls\n",
    "tp_value = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "np_value = 2 * (1 - norm.cdf(abs(t_stat)))\n",
    "chip_value = 1 - chi2.cdf(t_stat, df=1)\n",
    "\n",
    "print(f\"t_stat: {t_stat}, tp_value:{tp_value}, np_value:{np_value}, chip_value:{chip_value}\")\n",
    "\n",
    "\n",
    "# Ajustar el modelo GLS (equivalente a WLS si Omega es diagonal)\n",
    "model = sm.GLS(y, X, sigma=sigma2_tilde).fit()  # sigma es la inversa de los pesos\n",
    "\n",
    "# Extraer coeficientes y errores estándar\n",
    "beta_0, beta_1 = model.params\n",
    "se_beta1 = model.bse[1]\n",
    "pval_beta1 = model.pvalues[1]\n",
    "hypothesis = 'x1 = 0.8'  # Aquí 'x1' es el nombre de la variable correspondiente a beta1 en el modelo\n",
    "\n",
    "# Realizar la prueba t\n",
    "t_test = model.t_test(hypothesis)\n",
    "\n",
    "# Obtener el estadístico t y el p-valor\n",
    "t_statistic = t_test.tvalue[0][0]\n",
    "p_value_res = t_test.pvalue\n",
    "\n",
    "print(f\"beta_0 = {beta_0}, beta_1 = {beta_1}, se_beta1 = {se_beta1}\")\n",
    "\n",
    "print(f\"pval_beta1 = {pval_beta1}, t_test_res = {p_value_res}, t_statistic = {t_statistic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reject_1pct_list: 0.04\n",
      "reject_5pct_list: 0.12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "np.random.seed(3649)\n",
    "\n",
    "reject_1pct_list = []\n",
    "reject_5pct_list = []\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    n_obs = 10\n",
    "    beta_0_true = -3\n",
    "    beta_1_true = 0.8\n",
    "\n",
    "    x = np.random.uniform(1, 50, n_obs)\n",
    "    u = np.random.normal(scale=x)\n",
    "    y = beta_0_true + beta_1_true * x + u\n",
    "\n",
    "    X = np.column_stack((np.ones_like(x), x)) \n",
    "    beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y \n",
    "    u_hat = y - X @ beta_ols\n",
    "    u_hat2 = u_hat ** 2  \n",
    "    log_u_hat2 = np.log(u_hat2)\n",
    "\n",
    "    X_aux = np.column_stack((np.ones_like(x), x, x**2))\n",
    "    gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ log_u_hat2\n",
    "    log_sigma2_hat = X_aux @ gamma_hat\n",
    "    sigma2_hat = np.exp(log_sigma2_hat)\n",
    "\n",
    "    X_aux2 = np.column_stack((np.ones_like(x) / sigma2_hat, x / sigma2_hat,  x**2 / sigma2_hat))\n",
    "    u_hat2_over_sigma2_hat = u_hat2 / sigma2_hat\n",
    "    gamma_hat2 = np.linalg.inv(X_aux2.T @ X_aux2) @ X_aux2.T @ u_hat2_over_sigma2_hat\n",
    "\n",
    "    sigma2_tilde = X_aux @ gamma_hat2 \n",
    "    sigma2_tilde = np.maximum(sigma2_tilde, 1e-6)\n",
    "    omega_tilde_inv = np.linalg.inv(np.diag(sigma2_tilde))\n",
    "\n",
    "    beta_fgls = np.linalg.inv(X.T @ omega_tilde_inv @ X) @ X.T @ omega_tilde_inv @ y\n",
    "\n",
    "    residuals_fgls = y - X @ beta_fgls\n",
    "    s2_fgls = (residuals_fgls.T @ omega_tilde_inv @ residuals_fgls) / ( X.shape[0] - X.shape[1])\n",
    "    var_beta_fgls = s2_fgls * np.linalg.inv(X.T @ omega_tilde_inv @ X)\n",
    "    se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]\n",
    "\n",
    "    t_stat = (beta_fgls[1] - 0.8) / se_beta1_fgls\n",
    "    p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "    reject_1pct = p_value < 0.01\n",
    "    reject_5pct = p_value < 0.05\n",
    "\n",
    "    reject_1pct_list.append(reject_1pct)\n",
    "    reject_5pct_list.append(reject_5pct)\n",
    "\n",
    "print(f\"reject_1pct_list: {np.mean(reject_1pct_list)}\")\n",
    "print(f\"reject_5pct_list: {np.mean(reject_5pct_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-valor OLS (Python): 0.084999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Datos definidos manualmente\n",
    "X_values = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 50])\n",
    "y_values = np.array([2.5, 70.1, 3.8, 4.5, 5.2, 5.9, 6.5, 7.2, 7.8, 8.5])\n",
    "\n",
    "# Agregar constante\n",
    "X = sm.add_constant(X_values)\n",
    "\n",
    "# Ajustar modelo OLS\n",
    "model = sm.OLS(y_values, X)\n",
    "results = model.fit()\n",
    "\n",
    "hypothesis = 'x1 = 0.8'  # Aquí 'x1' es el nombre de la variable correspondiente a beta1 en el modelo\n",
    "\n",
    "# Realizar la prueba t\n",
    "t_test = results.t_test(hypothesis)\n",
    "\n",
    "# Obtener el estadístico t y el p-valor\n",
    "t_statistic = t_test.tvalue[0][0]\n",
    "p_value_res = t_test.pvalue\n",
    "\n",
    "print(f\"P-valor OLS (Python): {p_value_res:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estadístico F: 5.610111\n",
      "P-valor (distribución F): 0.045348\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f\n",
    "# Obtener estimaciones\n",
    "beta_1_hat = results.params[1]          # Coeficiente estimado\n",
    "se_beta1 = results.bse[1]                # Error estándar de beta_1\n",
    "n_obs = len(y_values)                     # Número de observaciones\n",
    "df_num = 1                                 # Grados de libertad del numerador\n",
    "df_den = n_obs - 2                         # Grados de libertad del denominador\n",
    "\n",
    "# Estadístico F\n",
    "H0 = 1  # Hipótesis nula beta_1 = 1\n",
    "F_stat = ((beta_1_hat - H0) ** 2) / (se_beta1 ** 2)\n",
    "\n",
    "# P-valor usando distribución F\n",
    "p_value_F = 1 - f.cdf(F_stat, df_num, df_den)\n",
    "\n",
    "print(f\"Estadístico F: {F_stat:.6f}\")\n",
    "print(f\"P-valor (distribución F): {p_value_F:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgls_estimation_numpy5(x, y):\n",
    "    \n",
    "    X = np.column_stack((np.ones_like(x), x)) \n",
    "    beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y \n",
    "    u_hat = y - X @ beta_ols\n",
    "    u_hat2 = u_hat ** 2  \n",
    "    log_u_hat2 = np.log(np.maximum(u_hat2, 1e-6))  # Evita log(0)\n",
    "    \n",
    "    # Estimación de la varianza condicional con un modelo cuadrático\n",
    "    X_aux = np.column_stack((np.ones_like(x), x, x**2))\n",
    "    gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ log_u_hat2\n",
    "    log_sigma2_hat = X_aux @ gamma_hat\n",
    "    sigma2_hat = np.exp(log_sigma2_hat)\n",
    "\n",
    "    # Asegurar que sigma2_hat no sea cero ni negativo\n",
    "    sigma2_hat = np.maximum(sigma2_hat, 1e-6)\n",
    "\n",
    "    # Matriz de pesos (diagonal)\n",
    "    omega_inv = np.diag(1 / sigma2_hat)\n",
    "\n",
    "    # Estimación FGLS\n",
    "    beta_fgls = np.linalg.inv(X.T @ omega_inv @ X) @ X.T @ omega_inv @ y\n",
    "\n",
    "    # Cálculo de errores estándar\n",
    "    residuals_fgls = y - X @ beta_fgls\n",
    "    s2_fgls = (residuals_fgls.T @ omega_inv @ residuals_fgls) / (len(x) - X.shape[1])\n",
    "    var_beta_fgls = s2_fgls * np.linalg.inv(X.T @ omega_inv @ X)\n",
    "    se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]   \n",
    "\n",
    "    return beta_fgls[0], beta_fgls[1], se_beta1_fgls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>mean_beta0</th>\n",
       "      <th>median_beta0</th>\n",
       "      <th>std_beta0</th>\n",
       "      <th>mean_beta1</th>\n",
       "      <th>median_beta1</th>\n",
       "      <th>std_beta1</th>\n",
       "      <th>test_size_1pct</th>\n",
       "      <th>test_size_5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>9.258171</td>\n",
       "      <td>-2.623278</td>\n",
       "      <td>893.446389</td>\n",
       "      <td>0.459207</td>\n",
       "      <td>0.786858</td>\n",
       "      <td>22.237986</td>\n",
       "      <td>0.1124</td>\n",
       "      <td>0.2204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>18.481732</td>\n",
       "      <td>-2.940235</td>\n",
       "      <td>994.586445</td>\n",
       "      <td>0.314504</td>\n",
       "      <td>0.795749</td>\n",
       "      <td>23.291711</td>\n",
       "      <td>0.1118</td>\n",
       "      <td>0.1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>-14.900211</td>\n",
       "      <td>-2.985833</td>\n",
       "      <td>630.791366</td>\n",
       "      <td>1.004283</td>\n",
       "      <td>0.801668</td>\n",
       "      <td>13.521860</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.1512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>-4.380422</td>\n",
       "      <td>-3.001054</td>\n",
       "      <td>88.507895</td>\n",
       "      <td>0.839335</td>\n",
       "      <td>0.800465</td>\n",
       "      <td>3.178552</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.1088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>-2.960183</td>\n",
       "      <td>-2.996994</td>\n",
       "      <td>8.543328</td>\n",
       "      <td>0.805510</td>\n",
       "      <td>0.799963</td>\n",
       "      <td>2.897765</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>-2.996185</td>\n",
       "      <td>-2.979176</td>\n",
       "      <td>2.097564</td>\n",
       "      <td>0.809875</td>\n",
       "      <td>0.798362</td>\n",
       "      <td>1.788327</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>700</td>\n",
       "      <td>-3.019977</td>\n",
       "      <td>-3.011490</td>\n",
       "      <td>1.855264</td>\n",
       "      <td>0.811589</td>\n",
       "      <td>0.799311</td>\n",
       "      <td>1.683692</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000</td>\n",
       "      <td>-3.005423</td>\n",
       "      <td>-2.993798</td>\n",
       "      <td>1.609359</td>\n",
       "      <td>0.808850</td>\n",
       "      <td>0.799982</td>\n",
       "      <td>1.456669</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.0606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1200</td>\n",
       "      <td>-2.985904</td>\n",
       "      <td>-2.994522</td>\n",
       "      <td>1.102379</td>\n",
       "      <td>0.791900</td>\n",
       "      <td>0.800001</td>\n",
       "      <td>0.993060</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  mean_beta0  median_beta0   std_beta0  mean_beta1  \\\n",
       "0               5    9.258171     -2.623278  893.446389    0.459207   \n",
       "1              10   18.481732     -2.940235  994.586445    0.314504   \n",
       "2              30  -14.900211     -2.985833  630.791366    1.004283   \n",
       "3             100   -4.380422     -3.001054   88.507895    0.839335   \n",
       "4             200   -2.960183     -2.996994    8.543328    0.805510   \n",
       "5             500   -2.996185     -2.979176    2.097564    0.809875   \n",
       "6             700   -3.019977     -3.011490    1.855264    0.811589   \n",
       "7            1000   -3.005423     -2.993798    1.609359    0.808850   \n",
       "8            1200   -2.985904     -2.994522    1.102379    0.791900   \n",
       "\n",
       "   median_beta1  std_beta1  test_size_1pct  test_size_5pct  \n",
       "0      0.786858  22.237986          0.1124          0.2204  \n",
       "1      0.795749  23.291711          0.1118          0.1968  \n",
       "2      0.801668  13.521860          0.0750          0.1512  \n",
       "3      0.800465   3.178552          0.0578          0.1088  \n",
       "4      0.799963   2.897765          0.0480          0.0952  \n",
       "5      0.798362   1.788327          0.0370          0.0768  \n",
       "6      0.799311   1.683692          0.0292          0.0690  \n",
       "7      0.799982   1.456669          0.0236          0.0606  \n",
       "8      0.800001   0.993060          0.0166          0.0584  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(3649)\n",
    "def simulate_fgls(n_samples, n_observations_list, beta_0_true, beta_1_true, beta1_H0):\n",
    "    \n",
    "    \"\"\"Realiza simulaciones de FGLS para distintos tamaños de muestra y reporta estadísticas de interés.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for n_obs in n_observations_list:\n",
    "        sample_results = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Generación de datos\n",
    "            x = np.random.uniform(1, 50, n_obs)\n",
    "            u = np.random.normal(scale=x)\n",
    "            y = beta_0_true + beta_1_true * x + u\n",
    "            \n",
    "            # Estimación FGLS\n",
    "            beta_0_hat, beta_1_hat, se_beta1_hat = fgls_estimation_numpy4(x, y)\n",
    "            \n",
    "            # Test de hipótesis para beta_1 = 0.8\n",
    "            t_stat = (beta_1_hat - beta1_H0) / se_beta1_hat\n",
    "            p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "            reject_1pct = p_value < 0.01\n",
    "            reject_5pct = p_value < 0.05\n",
    "            \n",
    "            sample_results.append([beta_0_hat, beta_1_hat, reject_1pct, reject_5pct])\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        df_results = pd.DataFrame(sample_results, columns=['beta_0_hat', 'beta_1_hat', 'reject_1pct', 'reject_5pct'])\n",
    "        \n",
    "        # Calcular estadísticas\n",
    "        mean_beta0 = df_results['beta_0_hat'].mean()\n",
    "        mean_beta1 = df_results['beta_1_hat'].mean()\n",
    "        median_beta0 = df_results['beta_0_hat'].median()\n",
    "        median_beta1 = df_results['beta_1_hat'].median()\n",
    "        std_beta0 = df_results['beta_0_hat'].std()\n",
    "        std_beta1 = df_results['beta_1_hat'].std()\n",
    "        test_size_1pct = df_results['reject_1pct'].mean()\n",
    "        test_size_5pct = df_results['reject_5pct'].mean()\n",
    "        \n",
    "        results.append([n_obs, mean_beta0, median_beta0, std_beta0, mean_beta1, median_beta1, std_beta1, test_size_1pct, test_size_5pct])\n",
    "    \n",
    "    # Crear DataFrame final\n",
    "    results_df = pd.DataFrame(results, columns=['n_observations', 'mean_beta0', 'median_beta0', 'std_beta0', 'mean_beta1', 'median_beta1', 'std_beta1', 'test_size_1pct', 'test_size_5pct'])\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Parámetros iniciales\n",
    "test_results = simulate_fgls(n_samples=5000, n_observations_list=[5, 10, 30, 100, 200, 500, 700, 1000, 1200], beta_0_true=-3, beta_1_true=0.8, beta1_H0=0.8)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>mean_beta0</th>\n",
       "      <th>median_beta0</th>\n",
       "      <th>std_beta0</th>\n",
       "      <th>mean_beta1</th>\n",
       "      <th>median_beta1</th>\n",
       "      <th>std_beta1</th>\n",
       "      <th>test_size_1pct</th>\n",
       "      <th>test_size_5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>9.258171</td>\n",
       "      <td>-2.623278</td>\n",
       "      <td>893.446389</td>\n",
       "      <td>0.059207</td>\n",
       "      <td>0.386858</td>\n",
       "      <td>22.237986</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.2364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>18.481732</td>\n",
       "      <td>-2.940235</td>\n",
       "      <td>994.586445</td>\n",
       "      <td>-0.085496</td>\n",
       "      <td>0.395749</td>\n",
       "      <td>23.291711</td>\n",
       "      <td>0.1356</td>\n",
       "      <td>0.2486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>-14.900211</td>\n",
       "      <td>-2.985833</td>\n",
       "      <td>630.791366</td>\n",
       "      <td>0.604283</td>\n",
       "      <td>0.401668</td>\n",
       "      <td>13.521860</td>\n",
       "      <td>0.2002</td>\n",
       "      <td>0.3870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>-4.380422</td>\n",
       "      <td>-3.001054</td>\n",
       "      <td>88.507895</td>\n",
       "      <td>0.439335</td>\n",
       "      <td>0.400465</td>\n",
       "      <td>3.178552</td>\n",
       "      <td>0.6858</td>\n",
       "      <td>0.8460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>-2.960183</td>\n",
       "      <td>-2.996994</td>\n",
       "      <td>8.543328</td>\n",
       "      <td>0.405510</td>\n",
       "      <td>0.399963</td>\n",
       "      <td>2.897765</td>\n",
       "      <td>0.9572</td>\n",
       "      <td>0.9864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  mean_beta0  median_beta0   std_beta0  mean_beta1  \\\n",
       "0               5    9.258171     -2.623278  893.446389    0.059207   \n",
       "1              10   18.481732     -2.940235  994.586445   -0.085496   \n",
       "2              30  -14.900211     -2.985833  630.791366    0.604283   \n",
       "3             100   -4.380422     -3.001054   88.507895    0.439335   \n",
       "4             200   -2.960183     -2.996994    8.543328    0.405510   \n",
       "\n",
       "   median_beta1  std_beta1  test_size_1pct  test_size_5pct  \n",
       "0      0.386858  22.237986          0.1230          0.2364  \n",
       "1      0.395749  23.291711          0.1356          0.2486  \n",
       "2      0.401668  13.521860          0.2002          0.3870  \n",
       "3      0.400465   3.178552          0.6858          0.8460  \n",
       "4      0.399963   2.897765          0.9572          0.9864  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(3649)\n",
    "def simulate_fgls(n_samples, n_observations_list, beta_0_true, beta_1_true, beta1_H0):\n",
    "    \n",
    "    \"\"\"Realiza simulaciones de FGLS para distintos tamaños de muestra y reporta estadísticas de interés.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for n_obs in n_observations_list:\n",
    "        sample_results = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Generación de datos\n",
    "            x = np.random.uniform(1, 50, n_obs)\n",
    "            u = np.random.normal(scale=x)\n",
    "            y = beta_0_true + beta_1_true * x + u\n",
    "            \n",
    "            # Estimación FGLS\n",
    "            beta_0_hat, beta_1_hat, se_beta1_hat = fgls_estimation_numpy4(x, y)\n",
    "            \n",
    "            # Test de hipótesis para beta_1 = 0.8\n",
    "            t_stat = (beta_1_hat - beta1_H0) / se_beta1_hat\n",
    "            p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "            reject_1pct = p_value < 0.01\n",
    "            reject_5pct = p_value < 0.05\n",
    "            \n",
    "            sample_results.append([beta_0_hat, beta_1_hat, reject_1pct, reject_5pct])\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        df_results = pd.DataFrame(sample_results, columns=['beta_0_hat', 'beta_1_hat', 'reject_1pct', 'reject_5pct'])\n",
    "        \n",
    "        # Calcular estadísticas\n",
    "        mean_beta0 = df_results['beta_0_hat'].mean()\n",
    "        mean_beta1 = df_results['beta_1_hat'].mean()\n",
    "        median_beta0 = df_results['beta_0_hat'].median()\n",
    "        median_beta1 = df_results['beta_1_hat'].median()\n",
    "        std_beta0 = df_results['beta_0_hat'].std()\n",
    "        std_beta1 = df_results['beta_1_hat'].std()\n",
    "        test_size_1pct = df_results['reject_1pct'].mean()\n",
    "        test_size_5pct = df_results['reject_5pct'].mean()\n",
    "        \n",
    "        results.append([n_obs, mean_beta0, median_beta0, std_beta0, mean_beta1, median_beta1, std_beta1, test_size_1pct, test_size_5pct])\n",
    "    \n",
    "    # Crear DataFrame final\n",
    "    results_df = pd.DataFrame(results, columns=['n_observations', 'mean_beta0', 'median_beta0', 'std_beta0', 'mean_beta1', 'median_beta1', 'std_beta1', 'test_size_1pct', 'test_size_5pct'])\n",
    "    \n",
    "    return results_df\n",
    "test_results = simulate_fgls(n_samples=5000, n_observations_list=[5, 10, 30, 100, 200], beta_0_true=-3, beta_1_true=0.4, beta1_H0=0.8)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio Beta1: 0.7996\n",
      "Promedio SE Beta1: 0.0155\n",
      "Proporción de rechazos H0 (p<0.05): 0.0106\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros\n",
    "n_obs = 500  # Número de observaciones por simulación\n",
    "n_sim = 5000  # Número de simulaciones\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "sigma = 1  # Desviación estándar del error\n",
    "\n",
    "# Almacenar resultados\n",
    "beta1_estimates = np.zeros(n_sim)\n",
    "se_beta1_estimates = np.zeros(n_sim)\n",
    "p_values = np.zeros(n_sim)\n",
    "\n",
    "for i in range(n_sim):\n",
    "    # Generar datos\n",
    "    x = np.random.uniform(0, 10, n_obs)\n",
    "    epsilon = np.random.normal(0, sigma, n_obs)\n",
    "    y = beta_0_true + beta_1_true * x + epsilon\n",
    "    \n",
    "    # Agregar constante para el intercepto\n",
    "    X = sm.add_constant(x)\n",
    "    \n",
    "    # Ajustar modelo OLS\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # Extraer beta1, su error estándar y calcular p-valor\n",
    "    beta1_estimates[i] = model.params[1]\n",
    "    se_beta1_estimates[i] = model.bse[1]\n",
    "    t_stat = (model.params[1] - 0.8) / model.bse[1]\n",
    "    p_values[i] = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "\n",
    "# Resultados promedio\n",
    "mean_beta1 = np.mean(beta1_estimates)\n",
    "mean_se_beta1 = np.mean(se_beta1_estimates)\n",
    "mean_p_value = np.mean(p_values)\n",
    "\n",
    "print(f\"Promedio Beta1: {mean_beta1:.4f}\")\n",
    "print(f\"Promedio SE Beta1: {mean_se_beta1:.4f}\")\n",
    "print(f\"Proporción de rechazos H0 (p<0.05): {np.mean(p_values < 0.01):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'beta_1_hat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'beta_1_hat'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m sns\u001b[38;5;241m.\u001b[39mhistplot(\u001b[43mtest_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbeta_1_hat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, kde\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39maxvline(\u001b[38;5;241m0.8\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValor de H0: 0.8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'beta_1_hat'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.histplot(df_results[\"beta_1_hat\"], kde=True)\n",
    "plt.axvline(beta1_H0, color='red', linestyle=\"--\", label=\"Valor de H0: 0.8\")\n",
    "plt.legend()\n",
    "plt.title(\"Distribución de Beta 1 Estimado\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>mean_beta0</th>\n",
       "      <th>median_beta0</th>\n",
       "      <th>std_beta0</th>\n",
       "      <th>mean_beta1</th>\n",
       "      <th>median_beta1</th>\n",
       "      <th>std_beta1</th>\n",
       "      <th>test_size_1pct</th>\n",
       "      <th>test_size_5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>7.886475</td>\n",
       "      <td>-3.021852</td>\n",
       "      <td>811.247071</td>\n",
       "      <td>-0.161186</td>\n",
       "      <td>0.438020</td>\n",
       "      <td>44.853066</td>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>-4.736752</td>\n",
       "      <td>-2.840837</td>\n",
       "      <td>716.166070</td>\n",
       "      <td>-0.040182</td>\n",
       "      <td>0.379416</td>\n",
       "      <td>48.722276</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.3140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>-2.923546</td>\n",
       "      <td>-3.069659</td>\n",
       "      <td>162.486675</td>\n",
       "      <td>0.118178</td>\n",
       "      <td>0.391275</td>\n",
       "      <td>34.192519</td>\n",
       "      <td>0.3958</td>\n",
       "      <td>0.5120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>-3.011968</td>\n",
       "      <td>-2.999841</td>\n",
       "      <td>134.097435</td>\n",
       "      <td>0.240242</td>\n",
       "      <td>0.399580</td>\n",
       "      <td>26.483190</td>\n",
       "      <td>0.5702</td>\n",
       "      <td>0.6856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>-2.114750</td>\n",
       "      <td>-2.927595</td>\n",
       "      <td>86.312413</td>\n",
       "      <td>0.120036</td>\n",
       "      <td>0.393478</td>\n",
       "      <td>17.257694</td>\n",
       "      <td>0.6822</td>\n",
       "      <td>0.7818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>-2.666872</td>\n",
       "      <td>-2.989617</td>\n",
       "      <td>62.486918</td>\n",
       "      <td>0.329250</td>\n",
       "      <td>0.396693</td>\n",
       "      <td>12.473935</td>\n",
       "      <td>0.8006</td>\n",
       "      <td>0.8490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  mean_beta0  median_beta0   std_beta0  mean_beta1  \\\n",
       "0               5    7.886475     -3.021852  811.247071   -0.161186   \n",
       "1              10   -4.736752     -2.840837  716.166070   -0.040182   \n",
       "2              30   -2.923546     -3.069659  162.486675    0.118178   \n",
       "3             100   -3.011968     -2.999841  134.097435    0.240242   \n",
       "4             200   -2.114750     -2.927595   86.312413    0.120036   \n",
       "5             500   -2.666872     -2.989617   62.486918    0.329250   \n",
       "\n",
       "   median_beta1  std_beta1  test_size_1pct  test_size_5pct  \n",
       "0      0.438020  44.853066          0.0710          0.1600  \n",
       "1      0.379416  48.722276          0.2140          0.3140  \n",
       "2      0.391275  34.192519          0.3958          0.5120  \n",
       "3      0.399580  26.483190          0.5702          0.6856  \n",
       "4      0.393478  17.257694          0.6822          0.7818  \n",
       "5      0.396693  12.473935          0.8006          0.8490  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simulate_fgls(n_samples, n_observations_list, beta_0_true, beta_1_true, beta1_H0):\n",
    "    \n",
    "    \"\"\"Realiza simulaciones de FGLS para distintos tamaños de muestra y reporta estadísticas de interés.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for n_obs in n_observations_list:\n",
    "        sample_results = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Generación de datos\n",
    "            x = np.random.uniform(1, 50, n_obs)\n",
    "            u = np.random.normal(scale=x)\n",
    "            y = beta_0_true + beta_1_true * x + u\n",
    "            \n",
    "            # Estimación FGLS\n",
    "            beta_0_hat, beta_1_hat, se_beta1_hat = fgls_estimation(x, y)\n",
    "            \n",
    "            # Test de hipótesis para beta_1 = 0.8\n",
    "            t_stat = (beta_1_hat - beta1_H0) / se_beta1_hat\n",
    "            p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "            reject_1pct = p_value < 0.01\n",
    "            reject_5pct = p_value < 0.05\n",
    "            \n",
    "            sample_results.append([beta_0_hat, beta_1_hat, reject_1pct, reject_5pct])\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        df_results = pd.DataFrame(sample_results, columns=['beta_0_hat', 'beta_1_hat', 'reject_1pct', 'reject_5pct'])\n",
    "        \n",
    "        # Calcular estadísticas\n",
    "        mean_beta0 = df_results['beta_0_hat'].mean()\n",
    "        mean_beta1 = df_results['beta_1_hat'].mean()\n",
    "        median_beta0 = df_results['beta_0_hat'].median()\n",
    "        median_beta1 = df_results['beta_1_hat'].median()\n",
    "        std_beta0 = df_results['beta_0_hat'].std()\n",
    "        std_beta1 = df_results['beta_1_hat'].std()\n",
    "        test_size_1pct = df_results['reject_1pct'].mean()\n",
    "        test_size_5pct = df_results['reject_5pct'].mean()\n",
    "        \n",
    "        results.append([n_obs, mean_beta0, median_beta0, std_beta0, mean_beta1, median_beta1, std_beta1, test_size_1pct, test_size_5pct])\n",
    "    \n",
    "    # Crear DataFrame final\n",
    "    results_df = pd.DataFrame(results, columns=['n_observations', 'mean_beta0', 'median_beta0', 'std_beta0', 'mean_beta1', 'median_beta1', 'std_beta1', 'test_size_1pct', 'test_size_5pct'])\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Parámetros iniciales\n",
    "test_results = simulate_fgls(n_samples=5000, n_observations_list=[5, 10, 30, 100, 200, 500], beta_0_true=-3, beta_1_true=0.4, beta1_H0=0.8)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "n_observations = 5  # Observaciones por muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "# Matriz omega (varianzas)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "\n",
    "# Descomposición de Cholesky de omega\n",
    "P = np.linalg.cholesky(omega)\n",
    "\n",
    "# Inversa de P\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "# Contenedores para almacenar muestras y estimadores\n",
    "samples = []\n",
    "results = []\n",
    "\n",
    "# Simulación de M muestras\n",
    "for sample_id in range(n_samples):\n",
    "    # 1. Generar datos\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "    # Almacenar muestra\n",
    "    samples.append(pd.DataFrame({'sample': i, 'x': x, 'y': y, 'u': u}))\n",
    "\n",
    "    # 2. Transformar para MCG\n",
    "    y_tilde = P_inv @ y\n",
    "    X = np.column_stack((np.ones(n_observations), x))  # Matriz de diseño original\n",
    "    X_tilde = P_inv @ X\n",
    "\n",
    "    # 3. Ajustar modelo transformado con MCO\n",
    "    beta_hat = np.linalg.inv(X_tilde.T @ X_tilde) @ (X_tilde.T @ y_tilde)\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append([i, beta_0_hat, beta_1_hat, se_beta1_hat])\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "# Crear DataFrame con los resultados de FGLS\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    'sample', 'beta_0_hat', 'beta_1_hat', 'se_beta1_hat'\n",
    "])\n",
    "\n",
    "# Mostrar ejemplos de los DataFrames generados\n",
    "print(\"Ejemplo de all_samples:\")\n",
    "print(all_samples.head())\n",
    "print(\"\\nEjemplo de results_df:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analizar.\n",
    "def fgls_estimation(x, y):\n",
    "    \"\"\"Función de estimación FGLS (suponiendo que ya está definida en el código original).\"\"\"\n",
    "    X = np.vstack([np.ones_like(x), x]).T\n",
    "    beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    residuals = y - X @ beta_hat\n",
    "    weights = 1 / (x ** 2)\n",
    "    W = np.diag(weights)\n",
    "    beta_hat_wls = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ y)\n",
    "    residuals_wls = y - X @ beta_hat_wls\n",
    "    sigma_sq = np.sum(residuals_wls**2) / (len(y) - 2)\n",
    "    var_beta = sigma_sq * np.linalg.inv(X.T @ W @ X)\n",
    "    se_beta1_hat = np.sqrt(var_beta[1, 1])\n",
    "    return beta_hat_wls[0], beta_hat_wls[1], se_beta1_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6867746  8.01926913 4.94568308 7.5111866  9.80190561] [ 0.15715015 25.66899652 -0.77641616 -2.17923096 19.85642126]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "# Definir número de observaciones\n",
    "n = 5  # Ajusta según necesidad\n",
    "\n",
    "# Generar datos\n",
    "x = (10 - 1) * np.random.uniform(size=n) + 1\n",
    "u = np.random.normal(size=n) * x\n",
    "print(x,u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6867746  8.01926913 4.94568308 7.5111866  9.80190561] [ 0.15715015 25.66899652 -0.77641616 -2.17923096 19.85642126]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "x = np.random.uniform(1, 10, n)\n",
    "u = np.random.normal(scale=x)\n",
    "print(x,u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.57784606,  1.22254885,  0.13128065, -2.32968685, 10.14315422])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "u = np.random.multivariate_normal(mean=np.zeros(n), cov=omega)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mean and cov must have same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, n_observations)  \u001b[38;5;66;03m# x ~ U[1, 50]\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# u ~ N(0, omega)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m beta_0_true \u001b[38;5;241m+\u001b[39m beta_1_true \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m u\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:4210\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: mean and cov must have same length"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "u = np.random.multivariate_normal(mean=np.zeros(10), cov=omega)  # u ~ N(0, omega)\n",
    "y = beta_0_true + beta_1_true * x + u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mean and cov must have same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m omega \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag([\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m36\u001b[39m][:\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m u\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:4210\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: mean and cov must have same length"
     ]
    }
   ],
   "source": [
    "omega = np.diag([4, 9, 16, 25, 36][:10])\n",
    "u = np.random.multivariate_normal(mean=np.zeros(10), cov=omega)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mean and cov must have same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m u\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:4210\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: mean and cov must have same length"
     ]
    }
   ],
   "source": [
    "u = np.random.multivariate_normal(mean=np.zeros(10), cov=omega)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>u</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44.691948</td>\n",
       "      <td>0.394367</td>\n",
       "      <td>33.147926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.945501</td>\n",
       "      <td>-2.739349</td>\n",
       "      <td>21.417052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.364044</td>\n",
       "      <td>-5.117829</td>\n",
       "      <td>21.773406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>37.251050</td>\n",
       "      <td>1.502358</td>\n",
       "      <td>28.303198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>34.525174</td>\n",
       "      <td>6.038023</td>\n",
       "      <td>30.658162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.409759</td>\n",
       "      <td>0.030947</td>\n",
       "      <td>-0.241246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.352028</td>\n",
       "      <td>-0.504023</td>\n",
       "      <td>7.977600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30.112575</td>\n",
       "      <td>2.001504</td>\n",
       "      <td>23.091564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.797610</td>\n",
       "      <td>3.095406</td>\n",
       "      <td>4.733495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>20.579967</td>\n",
       "      <td>8.232626</td>\n",
       "      <td>21.696599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample  index          x         u          y\n",
       "0       0      0  44.691948  0.394367  33.147926\n",
       "1       0      1  33.945501 -2.739349  21.417052\n",
       "2       0      2  37.364044 -5.117829  21.773406\n",
       "3       0      3  37.251050  1.502358  28.303198\n",
       "4       0      4  34.525174  6.038023  30.658162\n",
       "5       1      0   3.409759  0.030947  -0.241246\n",
       "6       1      1  14.352028 -0.504023   7.977600\n",
       "7       1      2  30.112575  2.001504  23.091564\n",
       "8       1      3   5.797610  3.095406   4.733495\n",
       "9       1      4  20.579967  8.232626  21.696599"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parámetros del modelo\n",
    "beta_0 = -3\n",
    "beta_1 = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas y covarianzas de u\n",
    "n_observations = 5  # Tamaño de cada muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "\n",
    "# Generación de muestras\n",
    "samples = []\n",
    "np.random.seed(3649)  #Últimos 4 números de mi documento de identidad.\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1 * x + u  # Generar y\n",
    "    samples.append(pd.DataFrame({'x': x, 'u': u, 'y': y}))\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples, keys=range(n_samples), names=['sample', 'index']).reset_index()\n",
    "\n",
    "# Mostrar algunas filas\n",
    "all_samples.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma_hat: 1.0190342558308039\n",
      "beta1 (coeficiente de x): 1.049125141002301\n",
      "beta0 (intercepto): 1.1894337904253731\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "\n",
    "x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "x2 = x**2\n",
    "u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "# Creamos un DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'x2': x2,\n",
    "    'u': u,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "# 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "# 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "df['u_hat'] = ols_results.resid\n",
    "df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "# 3. Dada la forma funcional de la heterocedasticidad de White para este modelo, sigma2 ~ x x2, se estima por OLS usando u_hat2 como proxy de sigma2\n",
    "ols_model_aux = sm.OLS(df['u_hat2'], df[['x', 'x2']]) #Ver de agregar intercepto.\n",
    "ols_model_aux_results = ols_model_aux.fit()\n",
    "gamma_hat1 = ols_model_aux_results.params['x']\n",
    "gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "# 4. Usar las estimaciones de la regresión auxiliar y obtener las varianzas ajustadas (no consistentes)\n",
    "df['sigma2_hat'] = ols_model_aux_results.predict(df[['x', 'x2']])\n",
    "\n",
    "# 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por la raíz de sigma2_hat y estimar por OLS\n",
    "df['u_hat2_over_sigma2_hat'] = df['u_hat2'] / df['sigma2_hat']\n",
    "df['x_over_sigma2_hat'] = df['x'] / df['sigma2_hat']\n",
    "df['x2_over_sigma2_hat'] = df['x2'] / df['sigma2_hat']\n",
    "\n",
    "ols_model_aux2 = sm.OLS(df['u_hat2_over_sigma2_hat'], df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']]) #Ver de agregar intercepto.\n",
    "ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "# 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "df['sigma2_tilde'] = ols_model_aux_results2.predict(df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']])\n",
    "df['sigma2_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "# 7. Usar uno sobre sigma tilde como ponderador en la regresión de y sobre x\n",
    "df['y_estrella'] = df['y'] / df['sigma_tilde']\n",
    "df['x_estrella'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "X_estrella = sm.add_constant(df['x_over_sigma_tilde']) \n",
    "final_ols_model = sm.OLS(df['y_estrella'], X_estrella)\n",
    "final_ols_results = final_ols_model.fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated beta_0: 19.1022, beta_1: 0.1914\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "#n_samples = 5000\n",
    "n_observations = 5\n",
    "\n",
    "x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "x2 = x**2\n",
    "u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "# Creamos un DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'x2': x2,\n",
    "    'u': u,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "# 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "# 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "df['u_hat'] = ols_results.resid\n",
    "df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "# 3. Regresión auxiliar: u_hat2 ~ x + x2 (incluyendo intercepto)\n",
    "aux_X = sm.add_constant(df[['x', 'x2']])\n",
    "ols_model_aux = sm.OLS(df['u_hat2'], aux_X)\n",
    "ols_model_aux_results = ols_model_aux.fit()\n",
    "gamma_hat1 = ols_model_aux_results.params['x']\n",
    "gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "# 4. Usar las estimaciones para obtener las varianzas ajustadas\n",
    "df['sigma2_hat'] = ols_model_aux_results.predict(aux_X)\n",
    "\n",
    "# 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por la raíz de sigma2_hat y estimar por OLS\n",
    "df['u_hat2_over_sigma2_hat'] = df['u_hat2'] / df['sigma2_hat']\n",
    "df['x_over_sigma2_hat'] = df['x'] / df['sigma2_hat']\n",
    "df['x2_over_sigma2_hat'] = df['x2'] / df['sigma2_hat']\n",
    "\n",
    "aux_X_2 = sm.add_constant(df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']])\n",
    "ols_model_aux2 = sm.OLS(df['u_hat2_over_sigma2_hat'], aux_X_2)\n",
    "ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "# 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "df['sigma2_tilde'] = ols_model_aux_results2.predict(aux_X_2)\n",
    "df['sigma_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "# 7. Usar uno sobre sigma tilde como ponderador en la regresión de y sobre x\n",
    "df['y_star'] = df['y'] / df['sigma_tilde']\n",
    "df['x_star'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "aux_X_star = sm.add_constant(df['x_star']) \n",
    "final_ols_model = sm.OLS(df['y_star'], aux_X_star)\n",
    "final_ols_results = final_ols_model.fit() \n",
    "\n",
    "# Estimaciones finales\n",
    "beta_0_hat = final_ols_results.params['const']\n",
    "beta_1_hat = final_ols_results.params['x_star']\n",
    "print(f\"Estimated beta_0: {beta_0_hat:.4f}, beta_1: {beta_1_hat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de all_samples:\n",
      "   sample          x          y         u\n",
      "0       0  44.691948  33.147926  0.394367\n",
      "1       0  33.945501  21.417052 -2.739349\n",
      "2       0  37.364044  21.773406 -5.117829\n",
      "3       0  37.251050  28.303198  1.502358\n",
      "4       0  34.525174  30.658162  6.038023\n",
      "\n",
      "Ejemplo de results_df:\n",
      "   sample  beta_0_hat  beta_1_hat  se_beta1_hat\n",
      "0       0   19.102219    0.191446      1.192612\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>beta_0_hat</th>\n",
       "      <th>beta_1_hat</th>\n",
       "      <th>se_beta1_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19.102219</td>\n",
       "      <td>0.191446</td>\n",
       "      <td>1.192612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample  beta_0_hat  beta_1_hat  se_beta1_hat\n",
       "0       0   19.102219    0.191446      1.192612"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7429979282613803"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['beta_1_hat'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación por MCC (beta_0, beta_1):\n",
      "[-0.10411783 -0.04700603  0.00813422  0.03736534  0.06757768]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definimos parámetros\n",
    "beta_0 = -3\n",
    "beta_1 = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "N = 5  # Observaciones por muestra\n",
    "M = 5000  # Cantidad de muestras\n",
    "\n",
    "# Generación de datos\n",
    "np.random.seed(3649)\n",
    "x = np.random.uniform(1, 50, size=(M, N))  # x ~ U[1, 50]\n",
    "chol_omega = np.linalg.cholesky(omega)     # P = Cholesky de omega\n",
    "u = chol_omega @ np.random.randn(N, M)    # u ~ N(0, omega)\n",
    "u = u.T  # Transpuesta para mantener dimensiones MxN\n",
    "y = beta_0 + beta_1 * x + u               # y_i = beta_0 + beta_1 * x_i + u_i\n",
    "\n",
    "# Transformación para MCC\n",
    "P_inv = np.linalg.inv(chol_omega)         # Inversa de P\n",
    "y_star = y @ P_inv.T                      # y* = P^-1 * y\n",
    "x_star = x @ P_inv.T                      # X* = P^-1 * X\n",
    "x_star = np.hstack([np.ones((M, 1)), x_star])  # Agregar constante a X*\n",
    "\n",
    "# Estimación MCC\n",
    "beta_mcc = np.linalg.inv(x_star.T @ x_star) @ x_star.T @ y_star\n",
    "\n",
    "# Resultados\n",
    "print(\"Estimación por MCC (beta_0, beta_1):\")\n",
    "print(beta_mcc.mean(axis=0))  # Promedio de betas estimadas en todas las muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.41230844, -1.08667462, -0.74686156, -0.56526369, -0.38246838])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_mcc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44.69194777, 33.9455005 , 37.36404406, 37.25104984, 34.52517416])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([np.ones((M, 1)), x]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  0,  0,  0,  0],\n",
       "       [ 0,  9,  0,  0,  0],\n",
       "       [ 0,  0, 16,  0,  0],\n",
       "       [ 0,  0,  0, 25,  0],\n",
       "       [ 0,  0,  0,  0, 36]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0., 0., 0.],\n",
       "       [0., 3., 0., 0., 0.],\n",
       "       [0., 0., 4., 0., 0.],\n",
       "       [0., 0., 0., 5., 0.],\n",
       "       [0., 0., 0., 0., 6.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(omega, chol_omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Matriz omega (varianzas)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "\n",
    "# Descomposición de Cholesky de omega\n",
    "P = np.linalg.cholesky(omega)\n",
    "\n",
    "# Inversa de P\n",
    "P_inv = np.linalg.inv(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.33333333, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.25      , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.2       , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.16666667]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'beta_0_hat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     beta_hat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(X_tilde\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m X_tilde) \u001b[38;5;241m@\u001b[39m (X_tilde\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m y_tilde)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Almacenar resultados\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend([i, \u001b[43mbeta_0_hat\u001b[49m, beta_1_hat, se_beta1_hat])\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Combinar todas las muestras en un único DataFrame\u001b[39;00m\n\u001b[0;32m     48\u001b[0m all_samples \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(samples)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'beta_0_hat' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "n_observations = 5  # Observaciones por muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "# Matriz omega (varianzas)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "\n",
    "# Descomposición de Cholesky de omega\n",
    "P = np.linalg.cholesky(omega)\n",
    "\n",
    "# Inversa de P\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "# Contenedores para almacenar muestras y estimadores\n",
    "samples = []\n",
    "results = []\n",
    "\n",
    "# Simulación de M muestras\n",
    "for sample_id in range(n_samples):\n",
    "    # 1. Generar datos\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "    # Almacenar muestra\n",
    "    samples.append(pd.DataFrame({'sample': i, 'x': x, 'y': y, 'u': u}))\n",
    "\n",
    "    # 2. Transformar para MCG\n",
    "    y_tilde = P_inv @ y\n",
    "    X = np.column_stack((np.ones(n_observations), x))  # Matriz de diseño original\n",
    "    X_tilde = P_inv @ X\n",
    "\n",
    "    # 3. Ajustar modelo transformado con MCO\n",
    "    beta_hat = np.linalg.inv(X_tilde.T @ X_tilde) @ (X_tilde.T @ y_tilde)\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append([i, beta_0_hat, beta_1_hat, se_beta1_hat])\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "# Crear DataFrame con los resultados de FGLS\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    'sample', 'beta_0_hat', 'beta_1_hat', 'se_beta1_hat'\n",
    "])\n",
    "\n",
    "# Mostrar ejemplos de los DataFrames generados\n",
    "print(\"Ejemplo de all_samples:\")\n",
    "print(all_samples.head())\n",
    "print(\"\\nEjemplo de results_df:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de all_samples:\n",
      "   sample          x          y         u\n",
      "0       0  44.691948  33.147926  0.394367\n",
      "1       0  33.945501  21.417052 -2.739349\n",
      "2       0  37.364044  21.773406 -5.117829\n",
      "3       0  37.251050  28.303198  1.502358\n",
      "4       0  34.525174  30.658162  6.038023\n",
      "\n",
      "Ejemplo de results_df:\n",
      "   sample  beta_0_hat  beta_1_hat  se_beta1_hat\n",
      "0       0  -11.213024    0.989465      0.301298\n",
      "1       1   -3.104349    0.893636      0.119634\n",
      "2       2    1.983310    0.630313      0.192682\n",
      "3       3   -1.290987    0.670113      0.142292\n",
      "4       4   -0.085546    0.737440      0.122292\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "n_observations = 5  # Observaciones por muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "# Matriz omega (varianzas)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "\n",
    "# Descomposición de Cholesky de omega\n",
    "P = np.linalg.cholesky(omega)\n",
    "\n",
    "# Inversa de P\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "# Contenedores para almacenar muestras y estimadores\n",
    "samples = []\n",
    "results = []\n",
    "\n",
    "# Simulación de M muestras\n",
    "for i in range(n_samples):\n",
    "    # 1. Generar datos\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "    # Almacenar muestra\n",
    "    samples.append(pd.DataFrame({'sample': i, 'x': x, 'y': y, 'u': u}))\n",
    "\n",
    "    # 2. Transformar para MCG\n",
    "    y_tilde = P_inv @ y\n",
    "    X = sm.add_constant(x)  # Agregar columna de unos automáticamente\n",
    "    X_tilde = P_inv @ X\n",
    "\n",
    "    # 3. Ajustar modelo transformado con statsmodels\n",
    "    model = sm.OLS(y_tilde, X_tilde)\n",
    "    results_model = model.fit()\n",
    "\n",
    "    # Extraer coeficientes y error estándar\n",
    "    beta_0_hat, beta_1_hat = results_model.params\n",
    "    se_beta1_hat = results_model.bse[1]  # Error estándar del coeficiente beta_1\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append([i, beta_0_hat, beta_1_hat, se_beta1_hat])\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "# Crear DataFrame con los resultados de FGLS\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    'sample', 'beta_0_hat', 'beta_1_hat', 'se_beta1_hat'\n",
    "])\n",
    "\n",
    "# Mostrar ejemplos de los DataFrames generados\n",
    "print(\"Ejemplo de all_samples:\")\n",
    "print(all_samples.head())\n",
    "print(\"\\nEjemplo de results_df:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7964059833013949"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['beta_1_hat'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(3639)\n",
    "n_observations = 20\n",
    "\n",
    "# Generar x1 y x2\n",
    "x1_core = np.linspace(-1, 1, 18)  # 18 puntos entre -1 y 1\n",
    "x1 = np.concatenate([[-1.1], x1_core, [1.1]])  # Agregar extremos -1.1 y 1.1\n",
    "x2 = np.random.normal(0, 1, n_observations)\n",
    "X = sm.add_constant(np.column_stack((x1, x2)))\n",
    "\n",
    "# Coeficientes\n",
    "beta0, beta1, beta2 = 1, 1, 1\n",
    "\n",
    "#Errores\n",
    "un = np.random.normal(0, 1, n_observations) #un: u normal.\n",
    "v1 = np.ones(n_observations) #v1: v = 1\n",
    "ut = np.random.standard_t(5, n_observations) #ut: u t-student.\n",
    "v2 = np.exp(0.25 * x1 + 0.25 * x2) #v2: v = exp(0.25*x1 + 0.25*x2)\n",
    "\n",
    "#Diseños\n",
    "# Diseño 0: normalidad y homocedasticidad\n",
    "y0 = beta0 + beta1 * x1 + beta2 * x2 + np.sqrt(v1) * un\n",
    "# Diseño 1: normalidad y heterocedasticidad\n",
    "y1 = beta0 + beta1 * x1 + beta2 * x2 + np.sqrt(v2) * un\n",
    "# Diseño 2: t-student y heterocedasticidad\n",
    "y3 = beta0 + beta1 * x1 + beta2 * x2 + np.sqrt(v2) * ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test al 1%: 0.0024\n",
      "Tamaño del test al 5%: 0.0358\n",
      "Tamaño del test al 10%: 0.0848\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "np.random.seed(3639)\n",
    "n_observations = 20\n",
    "n_simulations = 5000\n",
    "\n",
    "# Generar x1 y x2\n",
    "x1_core = np.linspace(-1, 1, 18)  # 18 puntos entre -1 y 1\n",
    "x1 = np.concatenate([[-1.1], x1_core, [1.1]])  # Agregar extremos -1.1 y 1.1\n",
    "x2 = np.random.normal(0, 1, n_observations)\n",
    "X = sm.add_constant(np.column_stack((x1, x2)))\n",
    "\n",
    "# Coeficientes\n",
    "beta0, beta1, beta2 = 1, 1, 1\n",
    "\n",
    "# Test de White\n",
    "significance_levels = [0.01, 0.05, 0.10]\n",
    "rejection_counts = {alpha: 0 for alpha in significance_levels}\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    # Errores\n",
    "    un = np.random.normal(0, 1, n_observations)\n",
    "    v1 = np.ones(n_observations)\n",
    "    \n",
    "    # Diseño 0: normalidad y homocedasticidad\n",
    "    y0 = beta0 + beta1 * x1 + beta2 * x2 + np.sqrt(v1) * un\n",
    "    \n",
    "    # Estimación por MCO\n",
    "    modelo = sm.OLS(y0, X).fit()\n",
    "    \n",
    "    # Test de White\n",
    "    white_test = het_white(modelo.resid, X)\n",
    "    p_value = white_test[1]  # p-valor de la estadística LM\n",
    "    \n",
    "    # Contar rechazos de H0 para cada nivel de significancia\n",
    "    for alpha in significance_levels:\n",
    "        if p_value < alpha:\n",
    "            rejection_counts[alpha] += 1\n",
    "\n",
    "# Reportar tamaños del test\n",
    "for alpha, count in rejection_counts.items():\n",
    "    print(f\"Tamaño del test al {int(alpha * 100)}%: {count / n_simulations:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poder del test de White usando errores poblacionales:\n",
      "Diseño 1 (Normalidad y heterocedasticidad): {0.01: 0.0142, 0.05: 0.0894, 0.1: 0.165}\n",
      "Diseño 2 (No-normalidad y heterocedasticidad): {0.01: 0.0194, 0.05: 0.0888, 0.1: 0.148}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "np.random.seed(3639)\n",
    "n_simulaciones = 5000\n",
    "n = 20\n",
    "\n",
    "# Niveles de significancia\n",
    "alpha_levels = [0.01, 0.05, 0.10]\n",
    "\n",
    "# Contadores de rechazos\n",
    "rechazos_diseño1 = {alpha: 0 for alpha in alpha_levels}\n",
    "rechazos_diseño2 = {alpha: 0 for alpha in alpha_levels}\n",
    "\n",
    "# Simulaciones\n",
    "for _ in range(n_simulaciones):\n",
    "    # Generar x1 y x2\n",
    "    x1_core = np.linspace(-1, 1, 18)  # 18 puntos entre -1 y 1\n",
    "    x1 = np.concatenate([[-1.1], x1_core, [1.1]])  # Agregar extremos -1.1 y 1.1\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "\n",
    "    # Generar errores poblacionales\n",
    "    un = np.random.normal(0, 1, n)  # Normal(0,1)\n",
    "    ut = np.random.standard_t(5, n)  # t-student con 5 grados de libertad\n",
    "    v2 = np.exp(0.25 * x1 + 0.25 * x2)  # Heterocedasticidad\n",
    "\n",
    "    # Diseños:\n",
    "    u_diseño1 = np.sqrt(v2) * un  # Normalidad y heterocedasticidad\n",
    "    u_diseño2 = np.sqrt(v2) * ut  # No-normalidad y heterocedasticidad\n",
    "\n",
    "    # Aplicar test de White sobre los errores poblacionales\n",
    "    X_aux = sm.add_constant(np.column_stack((x1, x2)))\n",
    "    \n",
    "    # Diseño 1\n",
    "    white_test_1 = het_white(u_diseño1, X_aux)\n",
    "    p_value_1 = white_test_1[1]  # p-valor del test de White\n",
    "\n",
    "    # Diseño 2\n",
    "    white_test_2 = het_white(u_diseño2, X_aux)\n",
    "    p_value_2 = white_test_2[1]  # p-valor del test de White\n",
    "\n",
    "    # Contar rechazos para cada nivel de significancia\n",
    "    for alpha in alpha_levels:\n",
    "        if p_value_1 < alpha:\n",
    "            rechazos_diseño1[alpha] += 1\n",
    "        if p_value_2 < alpha:\n",
    "            rechazos_diseño2[alpha] += 1\n",
    "\n",
    "# Calcular tasas de rechazo (poder del test)\n",
    "poder_diseño1 = {alpha: rechazos_diseño1[alpha] / n_simulaciones for alpha in alpha_levels}\n",
    "poder_diseño2 = {alpha: rechazos_diseño2[alpha] / n_simulaciones for alpha in alpha_levels}\n",
    "\n",
    "# Resultados\n",
    "print(\"Poder del test de White usando errores poblacionales:\")\n",
    "print(f\"Diseño 1 (Normalidad y heterocedasticidad): {poder_diseño1}\")\n",
    "print(f\"Diseño 2 (No-normalidad y heterocedasticidad): {poder_diseño2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poder del test de White usando errores poblacionales:\n",
      "Diseño 1 (Normalidad y heterocedasticidad): {0.01: 0.0142, 0.05: 0.0894, 0.1: 0.165}\n",
      "Diseño 2 (No-normalidad y heterocedasticidad): {0.01: 0.0194, 0.05: 0.0888, 0.1: 0.148}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "np.random.seed(3639)\n",
    "n_simulaciones = 5000\n",
    "n = 20\n",
    "\n",
    "# Niveles de significancia\n",
    "alpha_levels = [0.01, 0.05, 0.10]\n",
    "\n",
    "# Contadores de rechazos\n",
    "rechazos_diseño1 = {alpha: 0 for alpha in alpha_levels}\n",
    "rechazos_diseño2 = {alpha: 0 for alpha in alpha_levels}\n",
    "\n",
    "# Simulaciones\n",
    "for _ in range(n_simulaciones):\n",
    "    # Generar x1 y x2\n",
    "    x1_core = np.linspace(-1, 1, n-2)  # 18 puntos entre -1 y 1\n",
    "    x1 = np.concatenate([[-1.1], x1_core, [1.1]])  # Agregar extremos -1.1 y 1.1\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "\n",
    "    # Generar errores poblacionales\n",
    "    un = np.random.normal(0, 1, n)  # Normal(0,1)\n",
    "    ut = np.random.standard_t(5, n)  # t-student con 5 grados de libertad\n",
    "    v2 = np.exp(0.25 * x1 + 0.25 * x2)  # Heterocedasticidad\n",
    "\n",
    "    # Diseños:\n",
    "    u_diseño1 = np.sqrt(v2) * un  # Normalidad y heterocedasticidad\n",
    "    u_diseño2 = np.sqrt(v2) * ut  # No-normalidad y heterocedasticidad\n",
    "\n",
    "    # Aplicar test de White sobre los errores poblacionales\n",
    "    X_aux = sm.add_constant(np.column_stack((x1, x2)))\n",
    "    \n",
    "    # Diseño 1\n",
    "    white_test_1 = het_white(u_diseño1, X_aux)\n",
    "    p_value_1 = white_test_1[1]  # p-valor del test de White\n",
    "\n",
    "    # Diseño 2\n",
    "    white_test_2 = het_white(u_diseño2, X_aux)\n",
    "    p_value_2 = white_test_2[1]  # p-valor del test de White\n",
    "\n",
    "    # Contar rechazos para cada nivel de significancia\n",
    "    for alpha in alpha_levels:\n",
    "        if p_value_1 < alpha:\n",
    "            rechazos_diseño1[alpha] += 1\n",
    "        if p_value_2 < alpha:\n",
    "            rechazos_diseño2[alpha] += 1\n",
    "\n",
    "# Calcular tasas de rechazo (poder del test)\n",
    "poder_diseño1 = {alpha: rechazos_diseño1[alpha] / n_simulaciones for alpha in alpha_levels}\n",
    "poder_diseño2 = {alpha: rechazos_diseño2[alpha] / n_simulaciones for alpha in alpha_levels}\n",
    "\n",
    "# Resultados\n",
    "print(\"Poder del test de White usando errores poblacionales:\")\n",
    "print(f\"Diseño 1 (Normalidad y heterocedasticidad): {poder_diseño1}\")\n",
    "print(f\"Diseño 2 (No-normalidad y heterocedasticidad): {poder_diseño2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sesgos relativos estimados (White):\n",
      "B0: -0.10264479591532095, B1: 0.07860651184160447, B2: -0.3352494413374326\n",
      "\n",
      "Sesgos relativos estimados (White con residuos originales):\n",
      "B0: -0.19168155137200596, B1: 0.02866193652607788, B2: -0.5327384396207882\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "# Fijar semilla para reproducibilidad\n",
    "np.random.seed(144)\n",
    "\n",
    "# Cantidad de observaciones\n",
    "n_obs = 20\n",
    "\n",
    "# Generar las variables\n",
    "x1 = np.linspace(-1.1, 0.9, n_obs)\n",
    "x2 = np.random.normal(size=n_obs)\n",
    "x0 = np.ones(n_obs)\n",
    "\n",
    "# Crear DataFrame\n",
    "data = pd.DataFrame({\"x0\": x0, \"x1\": x1, \"x2\": x2})\n",
    "\n",
    "# Generar la matriz X y X_2\n",
    "X = data[[\"x0\", \"x1\", \"x2\"]].values\n",
    "X_2 = data[[\"x1\", \"x2\", \"x0\"]].values\n",
    "\n",
    "# Generar las variables v, u y y\n",
    "v = np.exp(0.25 * x1 + 0.25 * x2)\n",
    "u = np.random.normal(0, 1, size=n_obs)\n",
    "y = 1 + x1 + x2 + np.sqrt(v) * u\n",
    "\n",
    "data[\"v\"] = v\n",
    "data[\"u\"] = u\n",
    "data[\"y\"] = y\n",
    "\n",
    "# Regresión: Incluimos explícitamente la constante\n",
    "model = OLS(y, X)  # Usamos X completo, incluyendo x0\n",
    "results = model.fit()\n",
    "\n",
    "# Obtener los parámetros estimados (B)\n",
    "B = results.params  # Esto ahora tendrá 3 elementos (x0, x1, x2)\n",
    "\n",
    "# Generar la matriz de omega de White\n",
    "Y = y.reshape(-1, 1)\n",
    "uhat = Y - X_2 @ B.reshape(-1, 1)  # Ahora X_2 y B tienen dimensiones compatibles\n",
    "\n",
    "# Continuar el cálculo como estaba\n",
    "data[\"uhat\"] = uhat.flatten()\n",
    "data[\"uhat_2\"] = data[\"uhat\"] ** 2\n",
    "uhat_2 = np.diag(data[\"uhat_2\"].values)\n",
    "\n",
    "omega_hat = uhat_2\n",
    "\n",
    "# Matriz de omega de White con residuos originales\n",
    "u_ori = (np.sqrt(v) * u) ** 2\n",
    "data[\"u_ori\"] = u_ori\n",
    "omega_wori = np.diag(u_ori)\n",
    "\n",
    "# Matrices de varianzas y covarianzas de los betas\n",
    "X_transpose_X_inv = np.linalg.inv(X.T @ X)\n",
    "\n",
    "Sigma = X_transpose_X_inv @ (X.T @ omega @ X) @ X_transpose_X_inv\n",
    "Sigma_hat = X_transpose_X_inv @ (X.T @ omega_hat @ X) @ X_transpose_X_inv\n",
    "Sigma_wori = X_transpose_X_inv @ (X.T @ omega_wori @ X) @ X_transpose_X_inv\n",
    "\n",
    "# Sesgos relativos\n",
    "bias_B0 = (Sigma_hat[0, 0] - Sigma[0, 0]) / Sigma[0, 0]\n",
    "bias_B1 = (Sigma_hat[1, 1] - Sigma[1, 1]) / Sigma[1, 1]\n",
    "bias_B2 = (Sigma_hat[2, 2] - Sigma[2, 2]) / Sigma[2, 2]\n",
    "\n",
    "# Sesgos relativos con White con residuos originales\n",
    "bias_ori_B0 = (Sigma_wori[0, 0] - Sigma[0, 0]) / Sigma[0, 0]\n",
    "bias_ori_B1 = (Sigma_wori[1, 1] - Sigma[1, 1]) / Sigma[1, 1]\n",
    "bias_ori_B2 = (Sigma_wori[2, 2] - Sigma[2, 2]) / Sigma[2, 2]\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Sesgos relativos estimados (White):\")\n",
    "print(f\"B0: {bias_B0}, B1: {bias_B1}, B2: {bias_B2}\")\n",
    "\n",
    "print(\"\\nSesgos relativos estimados (White con residuos originales):\")\n",
    "print(f\"B0: {bias_ori_B0}, B1: {bias_ori_B1}, B2: {bias_ori_B2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test: {0.01: 0.0, 0.05: 0.0012}\n",
      "Poder del test: {0: 0.9014, 0.4: 0.452}\n",
      "Estadísticas descriptivas:\n",
      "           beta_0    beta_1\n",
      "mean   -2.958325  0.798089\n",
      "median -2.890889  0.796406\n",
      "std     4.649156  0.162026\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para estimar por FGLS\n",
    "def fgls_estimation(x, y, omega):\n",
    "    W = np.linalg.inv(omega)  # Matriz de pesos inversos\n",
    "    X = np.column_stack((np.ones(len(x)), x))  # Matriz de diseño\n",
    "    beta_hat = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ y)  # Estimación FGLS\n",
    "    var_beta = np.linalg.inv(X.T @ W @ X)  # Varianza de los estimadores\n",
    "    return beta_hat, np.sqrt(np.diag(var_beta))  # Estimadores y errores estándar\n",
    "\n",
    "# Simulaciones\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    beta_hat, se_beta = fgls_estimation(x, y, omega)  # Estimación FGLS\n",
    "    t_stat = (beta_hat[1] - beta_1_true) / se_beta[1]  # Estadístico t para beta_1\n",
    "    p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_observations - 2))  # Valor p\n",
    "    results.append([beta_hat[0], beta_hat[1], se_beta[0], se_beta[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        beta_hat, se_beta = fgls_estimation(x, y, omega)\n",
    "        t_stat = (beta_hat[1] - beta_1_true) / se_beta[1]\n",
    "        p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_observations - 2))\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test: {0.01: 0.6018, 0.05: 0.8908}\n",
      "Poder del test: {0: 0.0532, 0.4: 0.5276}\n",
      "Estadísticas descriptivas:\n",
      "           beta_0    beta_1\n",
      "mean   -2.958325  0.798089\n",
      "median -2.890889  0.796406\n",
      "std     4.649156  0.162026\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para estimar por FGLS usando statsmodels\n",
    "def fgls_estimation(x, y, omega):\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    model = sm.GLS(y, X, sigma=omega)  # Estimación FGLS con sigma = omega\n",
    "    results = model.fit()  # Ajustar el modelo\n",
    "    return results\n",
    "\n",
    "# Simulaciones\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    result = fgls_estimation(x, y, omega)  # Estimación FGLS\n",
    "    t_stat = result.tvalues[1]  # Estadístico t para beta_1\n",
    "    p_value = result.pvalues[1]  # Valor p para beta_1\n",
    "    results.append([result.params[0], result.params[1], result.bse[0], result.bse[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        result = fgls_estimation(x, y, omega)\n",
    "        t_stat = result.tvalues[1]\n",
    "        p_value = result.pvalues[1]\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta_0_sm</th>\n",
       "      <th>beta_1_sm</th>\n",
       "      <th>se_beta_0_sm</th>\n",
       "      <th>se_beta_1_sm</th>\n",
       "      <th>beta_0_manual</th>\n",
       "      <th>beta_1_manual</th>\n",
       "      <th>se_beta_0_manual</th>\n",
       "      <th>se_beta_1_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.124395</td>\n",
       "      <td>0.803626</td>\n",
       "      <td>3.777375</td>\n",
       "      <td>0.136698</td>\n",
       "      <td>-3.124395</td>\n",
       "      <td>0.803626</td>\n",
       "      <td>3.777375</td>\n",
       "      <td>0.136698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>-3.050941</td>\n",
       "      <td>0.804804</td>\n",
       "      <td>3.135215</td>\n",
       "      <td>0.118106</td>\n",
       "      <td>-3.050941</td>\n",
       "      <td>0.804804</td>\n",
       "      <td>3.135215</td>\n",
       "      <td>0.118106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.543793</td>\n",
       "      <td>0.162838</td>\n",
       "      <td>2.673090</td>\n",
       "      <td>0.088475</td>\n",
       "      <td>4.543793</td>\n",
       "      <td>0.162838</td>\n",
       "      <td>2.673090</td>\n",
       "      <td>0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        beta_0_sm  beta_1_sm  se_beta_0_sm  se_beta_1_sm  beta_0_manual  \\\n",
       "mean    -3.124395   0.803626      3.777375      0.136698      -3.124395   \n",
       "median  -3.050941   0.804804      3.135215      0.118106      -3.050941   \n",
       "std      4.543793   0.162838      2.673090      0.088475       4.543793   \n",
       "\n",
       "        beta_1_manual  se_beta_0_manual  se_beta_1_manual  \n",
       "mean         0.803626          3.777375          0.136698  \n",
       "median       0.804804          3.135215          0.118106  \n",
       "std          0.162838          2.673090          0.088475  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Parámetros\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_observations = 5  # Pequeña muestra\n",
    "n_samples = 5000  # Número de simulaciones\n",
    "\n",
    "# Generar datos\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_data():\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    return x, y\n",
    "\n",
    "# Método 1: Estimación con FGLS usando statsmodels\n",
    "def fgls_statsmodels(x, y):\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    model = sm.GLS(y, X, sigma=omega)  # Estimación FGLS con sigma = omega\n",
    "    results = model.fit()  # Ajustar el modelo\n",
    "    return results\n",
    "\n",
    "# Método 2: Estimación manual FGLS\n",
    "def fgls_manual(x, y, omega):\n",
    "    X = np.column_stack([np.ones(n_observations), x])  # Matriz de diseño con intercepto\n",
    "    # Calcular la inversa de Omega\n",
    "    omega_inv = np.linalg.inv(omega)\n",
    "    # Estimar Beta usando la fórmula FGLS\n",
    "    beta_hat = np.linalg.inv(X.T @ omega_inv @ X) @ (X.T @ omega_inv @ y)\n",
    "    # Estimación de errores estándar (desviación estándar)\n",
    "    residuals = y - X @ beta_hat\n",
    "    sigma_hat = (residuals.T @ omega_inv @ residuals) / (n_observations - 2)  # Varianza de los errores\n",
    "    cov_beta_hat = np.linalg.inv(X.T @ omega_inv @ X) * sigma_hat  # Varianza de los coeficientes\n",
    "    se_beta_hat = np.sqrt(np.diag(cov_beta_hat))  # Desviación estándar de los coeficientes\n",
    "    return beta_hat, se_beta_hat\n",
    "\n",
    "# Comparación de ambos métodos\n",
    "results_fgls_sm = []\n",
    "results_fgls_manual = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x, y = generate_data()\n",
    "    \n",
    "    # Método 1: FGLS con statsmodels\n",
    "    result_sm = fgls_statsmodels(x, y)\n",
    "    results_fgls_sm.append([result_sm.params[0], result_sm.params[1], result_sm.bse[0], result_sm.bse[1]])\n",
    "    \n",
    "    # Método 2: Estimación manual\n",
    "    beta_hat_manual, se_beta_hat_manual = fgls_manual(x, y, omega)\n",
    "    results_fgls_manual.append([beta_hat_manual[0], beta_hat_manual[1], se_beta_hat_manual[0], se_beta_hat_manual[1]])\n",
    "\n",
    "# Convertir resultados a DataFrame para comparar\n",
    "results_fgls_sm_df = pd.DataFrame(results_fgls_sm, columns=['beta_0_sm', 'beta_1_sm', 'se_beta_0_sm', 'se_beta_1_sm'])\n",
    "results_fgls_manual_df = pd.DataFrame(results_fgls_manual, columns=['beta_0_manual', 'beta_1_manual', 'se_beta_0_manual', 'se_beta_1_manual'])\n",
    "\n",
    "# Comparar medias, medianas y desviaciones estándar\n",
    "comparison = pd.concat([\n",
    "    results_fgls_sm_df[['beta_0_sm', 'beta_1_sm', 'se_beta_0_sm', 'se_beta_1_sm']].agg(['mean', 'median', 'std']),\n",
    "    results_fgls_manual_df[['beta_0_manual', 'beta_1_manual', 'se_beta_0_manual', 'se_beta_1_manual']].agg(['mean', 'median', 'std'])\n",
    "], axis=1)\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test: {0.01: 0.298, 0.05: 0.6176}\n",
      "Poder del test: {0: 0.0282, 0.4: 0.2734}\n",
      "Estadísticas descriptivas:\n",
      "           beta_0    beta_1\n",
      "mean   -2.958325  0.798089\n",
      "median -2.890889  0.796406\n",
      "std     4.649156  0.162026\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para estimar por FGLS usando statsmodels con errores robustos\n",
    "def fgls_estimation(x, y, omega):\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    model = sm.GLS(y, X, sigma=omega)  # Estimación FGLS con sigma = omega\n",
    "    results = model.fit()  # Ajustar el modelo\n",
    "    robust_results = results.get_robustcov_results(cov_type='HC3')  # Errores estándar robustos (HC3)\n",
    "    return robust_results\n",
    "\n",
    "# Simulaciones\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    robust_results = fgls_estimation(x, y, omega)  # Estimación FGLS con errores robustos\n",
    "    t_stat = robust_results.tvalues[1]  # Estadístico t para beta_1\n",
    "    p_value = robust_results.pvalues[1]  # Valor p para beta_1\n",
    "    results.append([robust_results.params[0], robust_results.params[1], robust_results.bse[0], robust_results.bse[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        robust_results = fgls_estimation(x, y, omega)\n",
    "        t_stat = robust_results.tvalues[1]\n",
    "        p_value = robust_results.pvalues[1]\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sigma must be a scalar, 1d of length 5 or a 2d array of shape 5 x 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m u \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmultivariate_normal(mean\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(n_observations), cov\u001b[38;5;241m=\u001b[39momega)  \u001b[38;5;66;03m# u ~ N(0, omega)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m y \u001b[38;5;241m=\u001b[39m beta_0 \u001b[38;5;241m+\u001b[39m beta_1_true \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m u  \u001b[38;5;66;03m# Generar y\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m fgls_result \u001b[38;5;241m=\u001b[39m \u001b[43mfgls_white_estimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Estimación FGLS White\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Obtener estadísticos de interés\u001b[39;00m\n\u001b[0;32m     43\u001b[0m t_stat \u001b[38;5;241m=\u001b[39m fgls_result\u001b[38;5;241m.\u001b[39mtvalues[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Estadístico t para beta_1\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m, in \u001b[0;36mfgls_white_estimation\u001b[1;34m(x, y, omega)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Paso 3: Estimación FGLS con la matriz robusta\u001b[39;00m\n\u001b[0;32m     26\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(robust_cov)  \u001b[38;5;66;03m# Matriz de pesos (inversa de la matriz de varianzas-covarianzas)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m fgls_model \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Estimación FGLS usando la matriz robusta\u001b[39;00m\n\u001b[0;32m     28\u001b[0m fgls_results \u001b[38;5;241m=\u001b[39m fgls_model\u001b[38;5;241m.\u001b[39mfit()  \u001b[38;5;66;03m# Ajuste FGLS\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fgls_results\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:534\u001b[0m, in \u001b[0;36mGLS.__init__\u001b[1;34m(self, endog, exog, sigma, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_kwargs(kwargs)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# TODO: add options igls, for iterative fgls if sigma is None\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# TODO: default if sigma is none should be two-step GLS\u001b[39;00m\n\u001b[1;32m--> 534\u001b[0m sigma, cholsigmainv \u001b[38;5;241m=\u001b[39m \u001b[43m_get_sigma\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    537\u001b[0m                           hasconst\u001b[38;5;241m=\u001b[39mhasconst, sigma\u001b[38;5;241m=\u001b[39msigma,\n\u001b[0;32m    538\u001b[0m                           cholsigmainv\u001b[38;5;241m=\u001b[39mcholsigmainv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# store attribute names for data arrays\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:181\u001b[0m, in \u001b[0;36m_get_sigma\u001b[1;34m(sigma, nobs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sigma\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (nobs, nobs):\n\u001b[1;32m--> 181\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSigma must be a scalar, 1d of length \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m or a 2d \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray of shape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m x \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (nobs, nobs, nobs))\n\u001b[0;32m    183\u001b[0m     cholsigmainv, info \u001b[38;5;241m=\u001b[39m dtrtri(cholesky(sigma, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    184\u001b[0m                                 lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, overwrite_c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Sigma must be a scalar, 1d of length 5 or a 2d array of shape 5 x 5"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Parámetros del modelo\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas (heterocedasticidad)\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para realizar la estimación FGLS con errores robustos de White\n",
    "def fgls_white_estimation(x, y, omega):\n",
    "    # Paso 1: Estimación OLS inicial\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "    ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "    \n",
    "    # Paso 2: Estimación de la matriz de varianzas-covarianzas de los errores\n",
    "    residuals = ols_results.resid  # Residuos de la estimación OLS\n",
    "    # Usamos la matriz de varianzas-covarianzas robusta (White)\n",
    "    robust_cov = ols_results.get_robustcov_results(cov_type='HC3').cov_params()  # Matriz robusta de varianzas-covarianzas\n",
    "    \n",
    "    # Paso 3: Estimación FGLS con la matriz robusta\n",
    "    W = np.linalg.inv(robust_cov)  # Matriz de pesos (inversa de la matriz de varianzas-covarianzas)\n",
    "    fgls_model = sm.GLS(y, X, sigma=W)  # Estimación FGLS usando la matriz robusta\n",
    "    fgls_results = fgls_model.fit()  # Ajuste FGLS\n",
    "    \n",
    "    return fgls_results\n",
    "\n",
    "# Simulación de los datos\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    fgls_result = fgls_white_estimation(x, y, omega)  # Estimación FGLS White\n",
    "    \n",
    "    # Obtener estadísticos de interés\n",
    "    t_stat = fgls_result.tvalues[1]  # Estadístico t para beta_1\n",
    "    p_value = fgls_result.pvalues[1]  # Valor p para beta_1\n",
    "    results.append([fgls_result.params[0], fgls_result.params[1], fgls_result.bse[0], fgls_result.bse[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        fgls_result = fgls_white_estimation(x, y, omega)\n",
    "        p_value = fgls_result.pvalues[1]\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
