{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNIVERSIDAD TORCUATO DI TELLA**\n",
    "## **MAESTRÍA EN ECONOMETRÍA**\n",
    "\n",
    "---\n",
    "\n",
    "### **TRABAJO PRÁCTICO DE ECONOMETRÍA**\n",
    "\n",
    "- **Profesor:** González-Rozada, Martín  \n",
    "- **Ayudante:** Lening, Iara  \n",
    "- **Alumno:** Guzzi, David Alexander  (Legajo n°: 24H1970)  \n",
    "\n",
    "**Ciclo Lectivo:** Tercer Trimestre, 2024  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. PROPIEDADES DE MUESTRA FINITA DE FGLS (MCGE).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de all_samples:\n",
      "   sample          x          y         u\n",
      "0       0  44.691948  33.147926  0.394367\n",
      "1       0  33.945501  21.417052 -2.739349\n",
      "2       0  37.364044  21.773406 -5.117829\n",
      "3       0  37.251050  28.303198  1.502358\n",
      "4       0  34.525174  30.658162  6.038023\n",
      "\n",
      "Ejemplo de results_df:\n",
      "   sample  beta_0_hat  beta_1_hat  se_beta1_hat\n",
      "0       0   19.102219    0.191446      1.192612\n",
      "1       1   18.489797   -0.070806      0.000040\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 2\n",
    "n_observations = 5\n",
    "\n",
    "# Función para realizar FGLS\n",
    "def fgls_estimation(x, y):\n",
    "    # DataFrame para almacenar las variables intermedias\n",
    "    df = pd.DataFrame({'x': x, 'y': y})\n",
    "    df['x2'] = df['x'] ** 2\n",
    "\n",
    "    # 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "    ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "    # 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "    df['u_hat'] = ols_results.resid\n",
    "    df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "    # 3. Dada la forma funcional de la heterocedasticidad de White para este modelo, sigma2 ~ x + x2, se estima por OLS usando u_hat2 como proxy de sigma2\n",
    "    aux_X = sm.add_constant(df[['x', 'x2']])\n",
    "    ols_model_aux = sm.OLS(df['u_hat2'], aux_X)\n",
    "    ols_model_aux_results = ols_model_aux.fit()\n",
    "    gamma_hat1 = ols_model_aux_results.params['x']\n",
    "    gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "    # 4. Usar las estimaciones de la regresión auxiliar y obtener las varianzas ajustadas (no consistentes)\n",
    "    df['sigma2_hat'] = ols_model_aux_results.predict(aux_X)\n",
    "\n",
    "    # 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por la raíz de sigma2_hat y estimar por OLS\n",
    "    df['u_hat2_over_sigma2_hat'] = df['u_hat2'] / df['sigma2_hat']\n",
    "    df['x_over_sigma2_hat'] = df['x'] / df['sigma2_hat']\n",
    "    df['x2_over_sigma2_hat'] = df['x2'] / df['sigma2_hat']\n",
    "\n",
    "    aux_X_2 = sm.add_constant(df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']])\n",
    "    ols_model_aux2 = sm.OLS(df['u_hat2_over_sigma2_hat'], aux_X_2)\n",
    "    ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "    gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "    gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "    # 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "    df['sigma2_tilde'] = ols_model_aux_results2.predict(aux_X_2)\n",
    "    \n",
    "    # Corregir valores negativos o muy pequeños\n",
    "    df['sigma2_tilde'] = df['sigma2_tilde'].clip(lower=1e-10)\n",
    "    df['sigma_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "    # Verificar si hay valores NaN\n",
    "    if df['sigma_tilde'].isna().any():\n",
    "        raise ValueError(\"Se encontraron valores NaN en sigma_tilde\")\n",
    "\n",
    "    # 7. Usar uno sobre sigma tilde como ponderador en la regresión de y ~ x\n",
    "    df['y_star'] = df['y'] / df['sigma_tilde']\n",
    "    df['x_star'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "    aux_X_star = sm.add_constant(df['x_star']) \n",
    "    final_ols_model = sm.OLS(df['y_star'], aux_X_star)\n",
    "    final_ols_results = final_ols_model.fit() \n",
    "\n",
    "    # Estimaciones finales\n",
    "    beta_0_hat = final_ols_results.params['const']\n",
    "    beta_1_hat = final_ols_results.params['x_star']\n",
    "    se_beta1_hat = final_ols_results.bse.iloc[1]\n",
    "\n",
    "    return beta_0_hat, beta_1_hat, se_beta1_hat\n",
    "\n",
    "# Inicializar listas para almacenar resultados\n",
    "samples = []\n",
    "results = []\n",
    "\n",
    "# Generar muestras y aplicar FGLS\n",
    "for i in range(n_samples):\n",
    "    # Generar datos\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "    # Almacenar muestra\n",
    "    samples.append(pd.DataFrame({'sample': i, 'x': x, 'y': y, 'u': u}))\n",
    "\n",
    "    # Aplicar FGLS\n",
    "    beta_0_hat, beta_1_hat, se_beta1_hat = fgls_estimation(x, y)\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append([i, beta_0_hat, beta_1_hat, se_beta1_hat])\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "# Crear DataFrame con los resultados de FGLS\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    'sample', 'beta_0_hat', 'beta_1_hat', 'se_beta1_hat'\n",
    "])\n",
    "\n",
    "# Mostrar ejemplos de los DataFrames generados\n",
    "print(\"Ejemplo de all_samples:\")\n",
    "print(all_samples.head())\n",
    "print(\"\\nEjemplo de results_df:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   n_observations   mean_beta0  mean_beta1  median_beta0  median_beta1  \\\n",
      "0              10 -3501.769353    0.512049     -0.593216      0.573930   \n",
      "1              30 -6288.425056    0.382018     -4.460198      0.500146   \n",
      "2             100 -4354.976504    0.262699     -8.724462      0.399269   \n",
      "3             200 -2521.082180    0.181429     -7.845445      0.290059   \n",
      "4             500 -1266.223860    0.092208     -6.440305      0.189166   \n",
      "\n",
      "      std_beta0  std_beta1  test_size_1pct  test_size_5pct  \n",
      "0  55118.631542   0.963198          0.3918          0.4516  \n",
      "1  65583.285787   0.793361          0.4722          0.5372  \n",
      "2  30027.258599   0.691316          0.6004          0.6496  \n",
      "3  16936.935128   0.711338          0.6538          0.6920  \n",
      "4   4207.140417   0.769100          0.6652          0.6930  \n"
     ]
    }
   ],
   "source": [
    "# Función para realizar FGLS\n",
    "def fgls_estimation(x, y):\n",
    "    # DataFrame para almacenar las variables intermedias\n",
    "    df = pd.DataFrame({'x': x, 'y': y})\n",
    "    df['x2'] = df['x'] ** 2\n",
    "\n",
    "    # 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "    ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "    # 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "    df['u_hat'] = ols_results.resid\n",
    "    df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "    # 3. Dada la forma funcional de la heterocedasticidad de White para este modelo, sigma2 ~ x + x2, se estima por OLS usando u_hat2 como proxy de sigma2\n",
    "    aux_X = sm.add_constant(df[['x', 'x2']])\n",
    "    ols_model_aux = sm.OLS(df['u_hat2'], aux_X)\n",
    "    ols_model_aux_results = ols_model_aux.fit()\n",
    "    gamma_hat1 = ols_model_aux_results.params['x']\n",
    "    gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "    # 4. Usar las estimaciones de la regresión auxiliar y obtener las varianzas ajustadas (no consistentes)\n",
    "    df['sigma2_hat'] = ols_model_aux_results.predict(aux_X)\n",
    "\n",
    "    # 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por la raíz de sigma2_hat y estimar por OLS\n",
    "    df['u_hat2_over_sigma2_hat'] = df['u_hat2'] / df['sigma2_hat']\n",
    "    df['x_over_sigma2_hat'] = df['x'] / df['sigma2_hat']\n",
    "    df['x2_over_sigma2_hat'] = df['x2'] / df['sigma2_hat']\n",
    "\n",
    "    aux_X_2 = sm.add_constant(df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']])\n",
    "    ols_model_aux2 = sm.OLS(df['u_hat2_over_sigma2_hat'], aux_X_2)\n",
    "    ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "    gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "    gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "    # 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "    df['sigma2_tilde'] = ols_model_aux_results2.predict(aux_X_2)\n",
    "    \n",
    "    # Corregir valores negativos o muy pequeños\n",
    "    df['sigma2_tilde'] = df['sigma2_tilde'].clip(lower=1e-10)\n",
    "    df['sigma_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "    # Verificar si hay valores NaN\n",
    "    if df['sigma_tilde'].isna().any():\n",
    "        raise ValueError(\"Se encontraron valores NaN en sigma_tilde\")\n",
    "\n",
    "    # 7. Usar uno sobre sigma tilde como ponderador en la regresión de y ~ x\n",
    "    df['y_star'] = df['y'] / df['sigma_tilde']\n",
    "    df['x_star'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "    aux_X_star = sm.add_constant(df['x_star']) \n",
    "    final_ols_model = sm.OLS(df['y_star'], aux_X_star)\n",
    "    final_ols_results = final_ols_model.fit() \n",
    "\n",
    "    # Estimaciones finales\n",
    "    beta_0_hat = final_ols_results.params['const']\n",
    "    beta_1_hat = final_ols_results.params['x_star']\n",
    "    se_beta1_hat = final_ols_results.bse.iloc[1]\n",
    "\n",
    "    return beta_0_hat, beta_1_hat, se_beta1_hat\n",
    "\n",
    "def simulate_fgls(n_samples, n_observations_list, beta_0_true, beta_1_true):\n",
    "    \"\"\"Realiza simulaciones de FGLS para distintos tamaños de muestra y reporta estadísticas de interés.\"\"\"\n",
    "    results = []\n",
    "    for n_obs in n_observations_list:\n",
    "        sample_results = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Generación de datos\n",
    "            x = np.random.uniform(1, 50, n_obs)\n",
    "            u = np.random.normal(scale=x)\n",
    "            y = beta_0_true + beta_1_true * x + u\n",
    "            \n",
    "            # Estimación FGLS\n",
    "            beta_0_hat, beta_1_hat, se_beta1_hat = fgls_estimation(x, y)\n",
    "            \n",
    "            # Test de hipótesis para beta_1 = 0.8\n",
    "            t_stat = (beta_1_hat - 0.8) / se_beta1_hat\n",
    "            p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "            reject_1pct = p_value < 0.01\n",
    "            reject_5pct = p_value < 0.05\n",
    "            \n",
    "            sample_results.append([beta_0_hat, beta_1_hat, reject_1pct, reject_5pct])\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        df_results = pd.DataFrame(sample_results, columns=['beta_0_hat', 'beta_1_hat', 'reject_1pct', 'reject_5pct'])\n",
    "        \n",
    "        # Calcular estadísticas\n",
    "        mean_beta0 = df_results['beta_0_hat'].mean()\n",
    "        mean_beta1 = df_results['beta_1_hat'].mean()\n",
    "        median_beta0 = df_results['beta_0_hat'].median()\n",
    "        median_beta1 = df_results['beta_1_hat'].median()\n",
    "        std_beta0 = df_results['beta_0_hat'].std()\n",
    "        std_beta1 = df_results['beta_1_hat'].std()\n",
    "        test_size_1pct = df_results['reject_1pct'].mean()\n",
    "        test_size_5pct = df_results['reject_5pct'].mean()\n",
    "        \n",
    "        results.append([n_obs, mean_beta0, mean_beta1, median_beta0, median_beta1, std_beta0, std_beta1, test_size_1pct, test_size_5pct])\n",
    "    \n",
    "    # Crear DataFrame final\n",
    "    results_df = pd.DataFrame(results, columns=['n_observations', 'mean_beta0', 'mean_beta1', 'median_beta0', 'median_beta1', 'std_beta0', 'std_beta1', 'test_size_1pct', 'test_size_5pct'])\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Parámetros iniciales\n",
    "test_results = simulate_fgls(n_samples=5000, n_observations_list=[5, 10, 30, 100, 200, 500], beta_0_true=-3, beta_1_true=0.8)\n",
    "print(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>mean_beta0</th>\n",
       "      <th>mean_beta1</th>\n",
       "      <th>median_beta0</th>\n",
       "      <th>median_beta1</th>\n",
       "      <th>std_beta0</th>\n",
       "      <th>std_beta1</th>\n",
       "      <th>test_size_1pct</th>\n",
       "      <th>test_size_5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>-3501.769353</td>\n",
       "      <td>0.512049</td>\n",
       "      <td>-0.593216</td>\n",
       "      <td>0.573930</td>\n",
       "      <td>55118.631542</td>\n",
       "      <td>0.963198</td>\n",
       "      <td>0.3918</td>\n",
       "      <td>0.4516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>-6288.425056</td>\n",
       "      <td>0.382018</td>\n",
       "      <td>-4.460198</td>\n",
       "      <td>0.500146</td>\n",
       "      <td>65583.285787</td>\n",
       "      <td>0.793361</td>\n",
       "      <td>0.4722</td>\n",
       "      <td>0.5372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>-4354.976504</td>\n",
       "      <td>0.262699</td>\n",
       "      <td>-8.724462</td>\n",
       "      <td>0.399269</td>\n",
       "      <td>30027.258599</td>\n",
       "      <td>0.691316</td>\n",
       "      <td>0.6004</td>\n",
       "      <td>0.6496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>-2521.082180</td>\n",
       "      <td>0.181429</td>\n",
       "      <td>-7.845445</td>\n",
       "      <td>0.290059</td>\n",
       "      <td>16936.935128</td>\n",
       "      <td>0.711338</td>\n",
       "      <td>0.6538</td>\n",
       "      <td>0.6920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>-1266.223860</td>\n",
       "      <td>0.092208</td>\n",
       "      <td>-6.440305</td>\n",
       "      <td>0.189166</td>\n",
       "      <td>4207.140417</td>\n",
       "      <td>0.769100</td>\n",
       "      <td>0.6652</td>\n",
       "      <td>0.6930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations   mean_beta0  mean_beta1  median_beta0  median_beta1  \\\n",
       "0              10 -3501.769353    0.512049     -0.593216      0.573930   \n",
       "1              30 -6288.425056    0.382018     -4.460198      0.500146   \n",
       "2             100 -4354.976504    0.262699     -8.724462      0.399269   \n",
       "3             200 -2521.082180    0.181429     -7.845445      0.290059   \n",
       "4             500 -1266.223860    0.092208     -6.440305      0.189166   \n",
       "\n",
       "      std_beta0  std_beta1  test_size_1pct  test_size_5pct  \n",
       "0  55118.631542   0.963198          0.3918          0.4516  \n",
       "1  65583.285787   0.793361          0.4722          0.5372  \n",
       "2  30027.258599   0.691316          0.6004          0.6496  \n",
       "3  16936.935128   0.711338          0.6538          0.6920  \n",
       "4   4207.140417   0.769100          0.6652          0.6930  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6867746  8.01926913 4.94568308 7.5111866  9.80190561] [ 0.15715015 25.66899652 -0.77641616 -2.17923096 19.85642126]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "# Definir número de observaciones\n",
    "n = 5  # Ajusta según necesidad\n",
    "\n",
    "# Generar datos\n",
    "x = (10 - 1) * np.random.uniform(size=n) + 1\n",
    "u = np.random.normal(size=n) * x\n",
    "print(x,u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6867746  8.01926913 4.94568308 7.5111866  9.80190561] [ 0.15715015 25.66899652 -0.77641616 -2.17923096 19.85642126]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "x = np.random.uniform(1, 10, n)\n",
    "u = np.random.normal(scale=x)\n",
    "print(x,u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.57784606,  1.22254885,  0.13128065, -2.32968685, 10.14315422])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "u = np.random.multivariate_normal(mean=np.zeros(n), cov=omega)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mean and cov must have same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, n_observations)  \u001b[38;5;66;03m# x ~ U[1, 50]\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# u ~ N(0, omega)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m beta_0_true \u001b[38;5;241m+\u001b[39m beta_1_true \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m u\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:4210\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: mean and cov must have same length"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "u = np.random.multivariate_normal(mean=np.zeros(10), cov=omega)  # u ~ N(0, omega)\n",
    "y = beta_0_true + beta_1_true * x + u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mean and cov must have same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m omega \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag([\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m36\u001b[39m][:\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m u\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:4210\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: mean and cov must have same length"
     ]
    }
   ],
   "source": [
    "omega = np.diag([4, 9, 16, 25, 36][:10])\n",
    "u = np.random.multivariate_normal(mean=np.zeros(10), cov=omega)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mean and cov must have same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m u\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:4210\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: mean and cov must have same length"
     ]
    }
   ],
   "source": [
    "u = np.random.multivariate_normal(mean=np.zeros(10), cov=omega)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>u</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44.691948</td>\n",
       "      <td>0.394367</td>\n",
       "      <td>33.147926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.945501</td>\n",
       "      <td>-2.739349</td>\n",
       "      <td>21.417052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.364044</td>\n",
       "      <td>-5.117829</td>\n",
       "      <td>21.773406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>37.251050</td>\n",
       "      <td>1.502358</td>\n",
       "      <td>28.303198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>34.525174</td>\n",
       "      <td>6.038023</td>\n",
       "      <td>30.658162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.409759</td>\n",
       "      <td>0.030947</td>\n",
       "      <td>-0.241246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.352028</td>\n",
       "      <td>-0.504023</td>\n",
       "      <td>7.977600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30.112575</td>\n",
       "      <td>2.001504</td>\n",
       "      <td>23.091564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.797610</td>\n",
       "      <td>3.095406</td>\n",
       "      <td>4.733495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>20.579967</td>\n",
       "      <td>8.232626</td>\n",
       "      <td>21.696599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample  index          x         u          y\n",
       "0       0      0  44.691948  0.394367  33.147926\n",
       "1       0      1  33.945501 -2.739349  21.417052\n",
       "2       0      2  37.364044 -5.117829  21.773406\n",
       "3       0      3  37.251050  1.502358  28.303198\n",
       "4       0      4  34.525174  6.038023  30.658162\n",
       "5       1      0   3.409759  0.030947  -0.241246\n",
       "6       1      1  14.352028 -0.504023   7.977600\n",
       "7       1      2  30.112575  2.001504  23.091564\n",
       "8       1      3   5.797610  3.095406   4.733495\n",
       "9       1      4  20.579967  8.232626  21.696599"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parámetros del modelo\n",
    "beta_0 = -3\n",
    "beta_1 = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas y covarianzas de u\n",
    "n_observations = 5  # Tamaño de cada muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "\n",
    "# Generación de muestras\n",
    "samples = []\n",
    "np.random.seed(3649)  #Últimos 4 números de mi documento de identidad.\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1 * x + u  # Generar y\n",
    "    samples.append(pd.DataFrame({'x': x, 'u': u, 'y': y}))\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples, keys=range(n_samples), names=['sample', 'index']).reset_index()\n",
    "\n",
    "# Mostrar algunas filas\n",
    "all_samples.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma_hat: 1.0190342558308039\n",
      "beta1 (coeficiente de x): 1.049125141002301\n",
      "beta0 (intercepto): 1.1894337904253731\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "\n",
    "x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "x2 = x**2\n",
    "u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "# Creamos un DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'x2': x2,\n",
    "    'u': u,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "# 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "# 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "df['u_hat'] = ols_results.resid\n",
    "df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "# 3. Dada la forma funcional de la heterocedasticidad de White para este modelo, sigma2 ~ x x2, se estima por OLS usando u_hat2 como proxy de sigma2\n",
    "ols_model_aux = sm.OLS(df['u_hat2'], df[['x', 'x2']]) #Ver de agregar intercepto.\n",
    "ols_model_aux_results = ols_model_aux.fit()\n",
    "gamma_hat1 = ols_model_aux_results.params['x']\n",
    "gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "# 4. Usar las estimaciones de la regresión auxiliar y obtener las varianzas ajustadas (no consistentes)\n",
    "df['sigma2_hat'] = ols_model_aux_results.predict(df[['x', 'x2']])\n",
    "\n",
    "# 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por la raíz de sigma2_hat y estimar por OLS\n",
    "df['u_hat2_over_sigma2_hat'] = df['u_hat2'] / df['sigma2_hat']\n",
    "df['x_over_sigma2_hat'] = df['x'] / df['sigma2_hat']\n",
    "df['x2_over_sigma2_hat'] = df['x2'] / df['sigma2_hat']\n",
    "\n",
    "ols_model_aux2 = sm.OLS(df['u_hat2_over_sigma2_hat'], df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']]) #Ver de agregar intercepto.\n",
    "ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "# 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "df['sigma2_tilde'] = ols_model_aux_results2.predict(df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']])\n",
    "df['sigma2_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "# 7. Usar uno sobre sigma tilde como ponderador en la regresión de y sobre x\n",
    "df['y_estrella'] = df['y'] / df['sigma_tilde']\n",
    "df['x_estrella'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "X_estrella = sm.add_constant(df['x_over_sigma_tilde']) \n",
    "final_ols_model = sm.OLS(df['y_estrella'], X_estrella)\n",
    "final_ols_results = final_ols_model.fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated beta_0: 19.1022, beta_1: 0.1914\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "#n_samples = 5000\n",
    "n_observations = 5\n",
    "\n",
    "x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "x2 = x**2\n",
    "u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "# Creamos un DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'x2': x2,\n",
    "    'u': u,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "# 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "# 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "df['u_hat'] = ols_results.resid\n",
    "df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "# 3. Regresión auxiliar: u_hat2 ~ x + x2 (incluyendo intercepto)\n",
    "aux_X = sm.add_constant(df[['x', 'x2']])\n",
    "ols_model_aux = sm.OLS(df['u_hat2'], aux_X)\n",
    "ols_model_aux_results = ols_model_aux.fit()\n",
    "gamma_hat1 = ols_model_aux_results.params['x']\n",
    "gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "# 4. Usar las estimaciones para obtener las varianzas ajustadas\n",
    "df['sigma2_hat'] = ols_model_aux_results.predict(aux_X)\n",
    "\n",
    "# 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por la raíz de sigma2_hat y estimar por OLS\n",
    "df['u_hat2_over_sigma2_hat'] = df['u_hat2'] / df['sigma2_hat']\n",
    "df['x_over_sigma2_hat'] = df['x'] / df['sigma2_hat']\n",
    "df['x2_over_sigma2_hat'] = df['x2'] / df['sigma2_hat']\n",
    "\n",
    "aux_X_2 = sm.add_constant(df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']])\n",
    "ols_model_aux2 = sm.OLS(df['u_hat2_over_sigma2_hat'], aux_X_2)\n",
    "ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "# 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "df['sigma2_tilde'] = ols_model_aux_results2.predict(aux_X_2)\n",
    "df['sigma_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "# 7. Usar uno sobre sigma tilde como ponderador en la regresión de y sobre x\n",
    "df['y_star'] = df['y'] / df['sigma_tilde']\n",
    "df['x_star'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "aux_X_star = sm.add_constant(df['x_star']) \n",
    "final_ols_model = sm.OLS(df['y_star'], aux_X_star)\n",
    "final_ols_results = final_ols_model.fit() \n",
    "\n",
    "# Estimaciones finales\n",
    "beta_0_hat = final_ols_results.params['const']\n",
    "beta_1_hat = final_ols_results.params['x_star']\n",
    "print(f\"Estimated beta_0: {beta_0_hat:.4f}, beta_1: {beta_1_hat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de all_samples:\n",
      "   sample          x          y         u\n",
      "0       0  44.691948  33.147926  0.394367\n",
      "1       0  33.945501  21.417052 -2.739349\n",
      "2       0  37.364044  21.773406 -5.117829\n",
      "3       0  37.251050  28.303198  1.502358\n",
      "4       0  34.525174  30.658162  6.038023\n",
      "\n",
      "Ejemplo de results_df:\n",
      "   sample  beta_0_hat  beta_1_hat  se_beta1_hat\n",
      "0       0   19.102219    0.191446      1.192612\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>beta_0_hat</th>\n",
       "      <th>beta_1_hat</th>\n",
       "      <th>se_beta1_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19.102219</td>\n",
       "      <td>0.191446</td>\n",
       "      <td>1.192612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample  beta_0_hat  beta_1_hat  se_beta1_hat\n",
       "0       0   19.102219    0.191446      1.192612"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7429979282613803"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['beta_1_hat'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación por MCC (beta_0, beta_1):\n",
      "[-0.10411783 -0.04700603  0.00813422  0.03736534  0.06757768]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definimos parámetros\n",
    "beta_0 = -3\n",
    "beta_1 = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "N = 5  # Observaciones por muestra\n",
    "M = 5000  # Cantidad de muestras\n",
    "\n",
    "# Generación de datos\n",
    "np.random.seed(3649)\n",
    "x = np.random.uniform(1, 50, size=(M, N))  # x ~ U[1, 50]\n",
    "chol_omega = np.linalg.cholesky(omega)     # P = Cholesky de omega\n",
    "u = chol_omega @ np.random.randn(N, M)    # u ~ N(0, omega)\n",
    "u = u.T  # Transpuesta para mantener dimensiones MxN\n",
    "y = beta_0 + beta_1 * x + u               # y_i = beta_0 + beta_1 * x_i + u_i\n",
    "\n",
    "# Transformación para MCC\n",
    "P_inv = np.linalg.inv(chol_omega)         # Inversa de P\n",
    "y_star = y @ P_inv.T                      # y* = P^-1 * y\n",
    "x_star = x @ P_inv.T                      # X* = P^-1 * X\n",
    "x_star = np.hstack([np.ones((M, 1)), x_star])  # Agregar constante a X*\n",
    "\n",
    "# Estimación MCC\n",
    "beta_mcc = np.linalg.inv(x_star.T @ x_star) @ x_star.T @ y_star\n",
    "\n",
    "# Resultados\n",
    "print(\"Estimación por MCC (beta_0, beta_1):\")\n",
    "print(beta_mcc.mean(axis=0))  # Promedio de betas estimadas en todas las muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.41230844, -1.08667462, -0.74686156, -0.56526369, -0.38246838])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_mcc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44.69194777, 33.9455005 , 37.36404406, 37.25104984, 34.52517416])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([np.ones((M, 1)), x]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  0,  0,  0,  0],\n",
       "       [ 0,  9,  0,  0,  0],\n",
       "       [ 0,  0, 16,  0,  0],\n",
       "       [ 0,  0,  0, 25,  0],\n",
       "       [ 0,  0,  0,  0, 36]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0., 0., 0.],\n",
       "       [0., 3., 0., 0., 0.],\n",
       "       [0., 0., 4., 0., 0.],\n",
       "       [0., 0., 0., 5., 0.],\n",
       "       [0., 0., 0., 0., 6.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(omega, chol_omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'beta_0_hat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     beta_hat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(X_tilde\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m X_tilde) \u001b[38;5;241m@\u001b[39m (X_tilde\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m y_tilde)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Almacenar resultados\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend([i, \u001b[43mbeta_0_hat\u001b[49m, beta_1_hat, se_beta1_hat])\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Combinar todas las muestras en un único DataFrame\u001b[39;00m\n\u001b[0;32m     48\u001b[0m all_samples \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(samples)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'beta_0_hat' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "n_observations = 5  # Observaciones por muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "# Matriz omega (varianzas)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "\n",
    "# Descomposición de Cholesky de omega\n",
    "P = np.linalg.cholesky(omega)\n",
    "\n",
    "# Inversa de P\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "# Contenedores para almacenar muestras y estimadores\n",
    "samples = []\n",
    "results = []\n",
    "\n",
    "# Simulación de M muestras\n",
    "for sample_id in range(n_samples):\n",
    "    # 1. Generar datos\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "    # Almacenar muestra\n",
    "    samples.append(pd.DataFrame({'sample': i, 'x': x, 'y': y, 'u': u}))\n",
    "\n",
    "    # 2. Transformar para MCG\n",
    "    y_tilde = P_inv @ y\n",
    "    X = np.column_stack((np.ones(n_observations), x))  # Matriz de diseño original\n",
    "    X_tilde = P_inv @ X\n",
    "\n",
    "    # 3. Ajustar modelo transformado con MCO\n",
    "    beta_hat = np.linalg.inv(X_tilde.T @ X_tilde) @ (X_tilde.T @ y_tilde)\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append([i, beta_0_hat, beta_1_hat, se_beta1_hat])\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "# Crear DataFrame con los resultados de FGLS\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    'sample', 'beta_0_hat', 'beta_1_hat', 'se_beta1_hat'\n",
    "])\n",
    "\n",
    "# Mostrar ejemplos de los DataFrames generados\n",
    "print(\"Ejemplo de all_samples:\")\n",
    "print(all_samples.head())\n",
    "print(\"\\nEjemplo de results_df:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de all_samples:\n",
      "   sample          x          y         u\n",
      "0       0  44.691948  33.147926  0.394367\n",
      "1       0  33.945501  21.417052 -2.739349\n",
      "2       0  37.364044  21.773406 -5.117829\n",
      "3       0  37.251050  28.303198  1.502358\n",
      "4       0  34.525174  30.658162  6.038023\n",
      "\n",
      "Ejemplo de results_df:\n",
      "   sample  beta_0_hat  beta_1_hat  se_beta1_hat\n",
      "0       0  -11.213024    0.989465      0.301298\n",
      "1       1   -3.104349    0.893636      0.119634\n",
      "2       2    1.983310    0.630313      0.192682\n",
      "3       3   -1.290987    0.670113      0.142292\n",
      "4       4   -0.085546    0.737440      0.122292\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "n_observations = 5  # Observaciones por muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "# Matriz omega (varianzas)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "\n",
    "# Descomposición de Cholesky de omega\n",
    "P = np.linalg.cholesky(omega)\n",
    "\n",
    "# Inversa de P\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "# Contenedores para almacenar muestras y estimadores\n",
    "samples = []\n",
    "results = []\n",
    "\n",
    "# Simulación de M muestras\n",
    "for i in range(n_samples):\n",
    "    # 1. Generar datos\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "    # Almacenar muestra\n",
    "    samples.append(pd.DataFrame({'sample': i, 'x': x, 'y': y, 'u': u}))\n",
    "\n",
    "    # 2. Transformar para MCG\n",
    "    y_tilde = P_inv @ y\n",
    "    X = sm.add_constant(x)  # Agregar columna de unos automáticamente\n",
    "    X_tilde = P_inv @ X\n",
    "\n",
    "    # 3. Ajustar modelo transformado con statsmodels\n",
    "    model = sm.OLS(y_tilde, X_tilde)\n",
    "    results_model = model.fit()\n",
    "\n",
    "    # Extraer coeficientes y error estándar\n",
    "    beta_0_hat, beta_1_hat = results_model.params\n",
    "    se_beta1_hat = results_model.bse[1]  # Error estándar del coeficiente beta_1\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append([i, beta_0_hat, beta_1_hat, se_beta1_hat])\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "# Crear DataFrame con los resultados de FGLS\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    'sample', 'beta_0_hat', 'beta_1_hat', 'se_beta1_hat'\n",
    "])\n",
    "\n",
    "# Mostrar ejemplos de los DataFrames generados\n",
    "print(\"Ejemplo de all_samples:\")\n",
    "print(all_samples.head())\n",
    "print(\"\\nEjemplo de results_df:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7964059833013949"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['beta_1_hat'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(3639)\n",
    "n_observations = 20\n",
    "\n",
    "# Generar x1 y x2\n",
    "x1_core = np.linspace(-1, 1, 18)  # 18 puntos entre -1 y 1\n",
    "x1 = np.concatenate([[-1.1], x1_core, [1.1]])  # Agregar extremos -1.1 y 1.1\n",
    "x2 = np.random.normal(0, 1, n_observations)\n",
    "X = sm.add_constant(np.column_stack((x1, x2)))\n",
    "\n",
    "# Coeficientes\n",
    "beta0, beta1, beta2 = 1, 1, 1\n",
    "\n",
    "#Errores\n",
    "un = np.random.normal(0, 1, n_observations) #un: u normal.\n",
    "v1 = np.ones(n_observations) #v1: v = 1\n",
    "ut = np.random.standard_t(5, n_observations) #ut: u t-student.\n",
    "v2 = np.exp(0.25 * x1 + 0.25 * x2) #v2: v = exp(0.25*x1 + 0.25*x2)\n",
    "\n",
    "#Diseños\n",
    "# Diseño 0: normalidad y homocedasticidad\n",
    "y0 = beta0 + beta1 * x1 + beta2 * x2 + np.sqrt(v1) * un\n",
    "# Diseño 1: normalidad y heterocedasticidad\n",
    "y1 = beta0 + beta1 * x1 + beta2 * x2 + np.sqrt(v2) * un\n",
    "# Diseño 2: t-student y heterocedasticidad\n",
    "y3 = beta0 + beta1 * x1 + beta2 * x2 + np.sqrt(v2) * ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test al 1%: 0.0024\n",
      "Tamaño del test al 5%: 0.0358\n",
      "Tamaño del test al 10%: 0.0848\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "np.random.seed(3639)\n",
    "n_observations = 20\n",
    "n_simulations = 5000\n",
    "\n",
    "# Generar x1 y x2\n",
    "x1_core = np.linspace(-1, 1, 18)  # 18 puntos entre -1 y 1\n",
    "x1 = np.concatenate([[-1.1], x1_core, [1.1]])  # Agregar extremos -1.1 y 1.1\n",
    "x2 = np.random.normal(0, 1, n_observations)\n",
    "X = sm.add_constant(np.column_stack((x1, x2)))\n",
    "\n",
    "# Coeficientes\n",
    "beta0, beta1, beta2 = 1, 1, 1\n",
    "\n",
    "# Test de White\n",
    "significance_levels = [0.01, 0.05, 0.10]\n",
    "rejection_counts = {alpha: 0 for alpha in significance_levels}\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    # Errores\n",
    "    un = np.random.normal(0, 1, n_observations)\n",
    "    v1 = np.ones(n_observations)\n",
    "    \n",
    "    # Diseño 0: normalidad y homocedasticidad\n",
    "    y0 = beta0 + beta1 * x1 + beta2 * x2 + np.sqrt(v1) * un\n",
    "    \n",
    "    # Estimación por MCO\n",
    "    modelo = sm.OLS(y0, X).fit()\n",
    "    \n",
    "    # Test de White\n",
    "    white_test = het_white(modelo.resid, X)\n",
    "    p_value = white_test[1]  # p-valor de la estadística LM\n",
    "    \n",
    "    # Contar rechazos de H0 para cada nivel de significancia\n",
    "    for alpha in significance_levels:\n",
    "        if p_value < alpha:\n",
    "            rejection_counts[alpha] += 1\n",
    "\n",
    "# Reportar tamaños del test\n",
    "for alpha, count in rejection_counts.items():\n",
    "    print(f\"Tamaño del test al {int(alpha * 100)}%: {count / n_simulations:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poder del test de White usando errores poblacionales:\n",
      "Diseño 1 (Normalidad y heterocedasticidad): {0.01: 0.0142, 0.05: 0.0894, 0.1: 0.165}\n",
      "Diseño 2 (No-normalidad y heterocedasticidad): {0.01: 0.0194, 0.05: 0.0888, 0.1: 0.148}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "np.random.seed(3639)\n",
    "n_simulaciones = 5000\n",
    "n = 20\n",
    "\n",
    "# Niveles de significancia\n",
    "alpha_levels = [0.01, 0.05, 0.10]\n",
    "\n",
    "# Contadores de rechazos\n",
    "rechazos_diseño1 = {alpha: 0 for alpha in alpha_levels}\n",
    "rechazos_diseño2 = {alpha: 0 for alpha in alpha_levels}\n",
    "\n",
    "# Simulaciones\n",
    "for _ in range(n_simulaciones):\n",
    "    # Generar x1 y x2\n",
    "    x1_core = np.linspace(-1, 1, 18)  # 18 puntos entre -1 y 1\n",
    "    x1 = np.concatenate([[-1.1], x1_core, [1.1]])  # Agregar extremos -1.1 y 1.1\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "\n",
    "    # Generar errores poblacionales\n",
    "    un = np.random.normal(0, 1, n)  # Normal(0,1)\n",
    "    ut = np.random.standard_t(5, n)  # t-student con 5 grados de libertad\n",
    "    v2 = np.exp(0.25 * x1 + 0.25 * x2)  # Heterocedasticidad\n",
    "\n",
    "    # Diseños:\n",
    "    u_diseño1 = np.sqrt(v2) * un  # Normalidad y heterocedasticidad\n",
    "    u_diseño2 = np.sqrt(v2) * ut  # No-normalidad y heterocedasticidad\n",
    "\n",
    "    # Aplicar test de White sobre los errores poblacionales\n",
    "    X_aux = sm.add_constant(np.column_stack((x1, x2)))\n",
    "    \n",
    "    # Diseño 1\n",
    "    white_test_1 = het_white(u_diseño1, X_aux)\n",
    "    p_value_1 = white_test_1[1]  # p-valor del test de White\n",
    "\n",
    "    # Diseño 2\n",
    "    white_test_2 = het_white(u_diseño2, X_aux)\n",
    "    p_value_2 = white_test_2[1]  # p-valor del test de White\n",
    "\n",
    "    # Contar rechazos para cada nivel de significancia\n",
    "    for alpha in alpha_levels:\n",
    "        if p_value_1 < alpha:\n",
    "            rechazos_diseño1[alpha] += 1\n",
    "        if p_value_2 < alpha:\n",
    "            rechazos_diseño2[alpha] += 1\n",
    "\n",
    "# Calcular tasas de rechazo (poder del test)\n",
    "poder_diseño1 = {alpha: rechazos_diseño1[alpha] / n_simulaciones for alpha in alpha_levels}\n",
    "poder_diseño2 = {alpha: rechazos_diseño2[alpha] / n_simulaciones for alpha in alpha_levels}\n",
    "\n",
    "# Resultados\n",
    "print(\"Poder del test de White usando errores poblacionales:\")\n",
    "print(f\"Diseño 1 (Normalidad y heterocedasticidad): {poder_diseño1}\")\n",
    "print(f\"Diseño 2 (No-normalidad y heterocedasticidad): {poder_diseño2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poder del test de White usando errores poblacionales:\n",
      "Diseño 1 (Normalidad y heterocedasticidad): {0.01: 0.0142, 0.05: 0.0894, 0.1: 0.165}\n",
      "Diseño 2 (No-normalidad y heterocedasticidad): {0.01: 0.0194, 0.05: 0.0888, 0.1: 0.148}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "np.random.seed(3639)\n",
    "n_simulaciones = 5000\n",
    "n = 20\n",
    "\n",
    "# Niveles de significancia\n",
    "alpha_levels = [0.01, 0.05, 0.10]\n",
    "\n",
    "# Contadores de rechazos\n",
    "rechazos_diseño1 = {alpha: 0 for alpha in alpha_levels}\n",
    "rechazos_diseño2 = {alpha: 0 for alpha in alpha_levels}\n",
    "\n",
    "# Simulaciones\n",
    "for _ in range(n_simulaciones):\n",
    "    # Generar x1 y x2\n",
    "    x1_core = np.linspace(-1, 1, n-2)  # 18 puntos entre -1 y 1\n",
    "    x1 = np.concatenate([[-1.1], x1_core, [1.1]])  # Agregar extremos -1.1 y 1.1\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "\n",
    "    # Generar errores poblacionales\n",
    "    un = np.random.normal(0, 1, n)  # Normal(0,1)\n",
    "    ut = np.random.standard_t(5, n)  # t-student con 5 grados de libertad\n",
    "    v2 = np.exp(0.25 * x1 + 0.25 * x2)  # Heterocedasticidad\n",
    "\n",
    "    # Diseños:\n",
    "    u_diseño1 = np.sqrt(v2) * un  # Normalidad y heterocedasticidad\n",
    "    u_diseño2 = np.sqrt(v2) * ut  # No-normalidad y heterocedasticidad\n",
    "\n",
    "    # Aplicar test de White sobre los errores poblacionales\n",
    "    X_aux = sm.add_constant(np.column_stack((x1, x2)))\n",
    "    \n",
    "    # Diseño 1\n",
    "    white_test_1 = het_white(u_diseño1, X_aux)\n",
    "    p_value_1 = white_test_1[1]  # p-valor del test de White\n",
    "\n",
    "    # Diseño 2\n",
    "    white_test_2 = het_white(u_diseño2, X_aux)\n",
    "    p_value_2 = white_test_2[1]  # p-valor del test de White\n",
    "\n",
    "    # Contar rechazos para cada nivel de significancia\n",
    "    for alpha in alpha_levels:\n",
    "        if p_value_1 < alpha:\n",
    "            rechazos_diseño1[alpha] += 1\n",
    "        if p_value_2 < alpha:\n",
    "            rechazos_diseño2[alpha] += 1\n",
    "\n",
    "# Calcular tasas de rechazo (poder del test)\n",
    "poder_diseño1 = {alpha: rechazos_diseño1[alpha] / n_simulaciones for alpha in alpha_levels}\n",
    "poder_diseño2 = {alpha: rechazos_diseño2[alpha] / n_simulaciones for alpha in alpha_levels}\n",
    "\n",
    "# Resultados\n",
    "print(\"Poder del test de White usando errores poblacionales:\")\n",
    "print(f\"Diseño 1 (Normalidad y heterocedasticidad): {poder_diseño1}\")\n",
    "print(f\"Diseño 2 (No-normalidad y heterocedasticidad): {poder_diseño2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sesgos relativos estimados (White):\n",
      "B0: -0.10264479591532095, B1: 0.07860651184160447, B2: -0.3352494413374326\n",
      "\n",
      "Sesgos relativos estimados (White con residuos originales):\n",
      "B0: -0.19168155137200596, B1: 0.02866193652607788, B2: -0.5327384396207882\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "# Fijar semilla para reproducibilidad\n",
    "np.random.seed(144)\n",
    "\n",
    "# Cantidad de observaciones\n",
    "n_obs = 20\n",
    "\n",
    "# Generar las variables\n",
    "x1 = np.linspace(-1.1, 0.9, n_obs)\n",
    "x2 = np.random.normal(size=n_obs)\n",
    "x0 = np.ones(n_obs)\n",
    "\n",
    "# Crear DataFrame\n",
    "data = pd.DataFrame({\"x0\": x0, \"x1\": x1, \"x2\": x2})\n",
    "\n",
    "# Generar la matriz X y X_2\n",
    "X = data[[\"x0\", \"x1\", \"x2\"]].values\n",
    "X_2 = data[[\"x1\", \"x2\", \"x0\"]].values\n",
    "\n",
    "# Generar las variables v, u y y\n",
    "v = np.exp(0.25 * x1 + 0.25 * x2)\n",
    "u = np.random.normal(0, 1, size=n_obs)\n",
    "y = 1 + x1 + x2 + np.sqrt(v) * u\n",
    "\n",
    "data[\"v\"] = v\n",
    "data[\"u\"] = u\n",
    "data[\"y\"] = y\n",
    "\n",
    "# Regresión: Incluimos explícitamente la constante\n",
    "model = OLS(y, X)  # Usamos X completo, incluyendo x0\n",
    "results = model.fit()\n",
    "\n",
    "# Obtener los parámetros estimados (B)\n",
    "B = results.params  # Esto ahora tendrá 3 elementos (x0, x1, x2)\n",
    "\n",
    "# Generar la matriz de omega de White\n",
    "Y = y.reshape(-1, 1)\n",
    "uhat = Y - X_2 @ B.reshape(-1, 1)  # Ahora X_2 y B tienen dimensiones compatibles\n",
    "\n",
    "# Continuar el cálculo como estaba\n",
    "data[\"uhat\"] = uhat.flatten()\n",
    "data[\"uhat_2\"] = data[\"uhat\"] ** 2\n",
    "uhat_2 = np.diag(data[\"uhat_2\"].values)\n",
    "\n",
    "omega_hat = uhat_2\n",
    "\n",
    "# Matriz de omega de White con residuos originales\n",
    "u_ori = (np.sqrt(v) * u) ** 2\n",
    "data[\"u_ori\"] = u_ori\n",
    "omega_wori = np.diag(u_ori)\n",
    "\n",
    "# Matrices de varianzas y covarianzas de los betas\n",
    "X_transpose_X_inv = np.linalg.inv(X.T @ X)\n",
    "\n",
    "Sigma = X_transpose_X_inv @ (X.T @ omega @ X) @ X_transpose_X_inv\n",
    "Sigma_hat = X_transpose_X_inv @ (X.T @ omega_hat @ X) @ X_transpose_X_inv\n",
    "Sigma_wori = X_transpose_X_inv @ (X.T @ omega_wori @ X) @ X_transpose_X_inv\n",
    "\n",
    "# Sesgos relativos\n",
    "bias_B0 = (Sigma_hat[0, 0] - Sigma[0, 0]) / Sigma[0, 0]\n",
    "bias_B1 = (Sigma_hat[1, 1] - Sigma[1, 1]) / Sigma[1, 1]\n",
    "bias_B2 = (Sigma_hat[2, 2] - Sigma[2, 2]) / Sigma[2, 2]\n",
    "\n",
    "# Sesgos relativos con White con residuos originales\n",
    "bias_ori_B0 = (Sigma_wori[0, 0] - Sigma[0, 0]) / Sigma[0, 0]\n",
    "bias_ori_B1 = (Sigma_wori[1, 1] - Sigma[1, 1]) / Sigma[1, 1]\n",
    "bias_ori_B2 = (Sigma_wori[2, 2] - Sigma[2, 2]) / Sigma[2, 2]\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Sesgos relativos estimados (White):\")\n",
    "print(f\"B0: {bias_B0}, B1: {bias_B1}, B2: {bias_B2}\")\n",
    "\n",
    "print(\"\\nSesgos relativos estimados (White con residuos originales):\")\n",
    "print(f\"B0: {bias_ori_B0}, B1: {bias_ori_B1}, B2: {bias_ori_B2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test: {0.01: 0.0, 0.05: 0.0012}\n",
      "Poder del test: {0: 0.9014, 0.4: 0.452}\n",
      "Estadísticas descriptivas:\n",
      "           beta_0    beta_1\n",
      "mean   -2.958325  0.798089\n",
      "median -2.890889  0.796406\n",
      "std     4.649156  0.162026\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para estimar por FGLS\n",
    "def fgls_estimation(x, y, omega):\n",
    "    W = np.linalg.inv(omega)  # Matriz de pesos inversos\n",
    "    X = np.column_stack((np.ones(len(x)), x))  # Matriz de diseño\n",
    "    beta_hat = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ y)  # Estimación FGLS\n",
    "    var_beta = np.linalg.inv(X.T @ W @ X)  # Varianza de los estimadores\n",
    "    return beta_hat, np.sqrt(np.diag(var_beta))  # Estimadores y errores estándar\n",
    "\n",
    "# Simulaciones\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    beta_hat, se_beta = fgls_estimation(x, y, omega)  # Estimación FGLS\n",
    "    t_stat = (beta_hat[1] - beta_1_true) / se_beta[1]  # Estadístico t para beta_1\n",
    "    p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_observations - 2))  # Valor p\n",
    "    results.append([beta_hat[0], beta_hat[1], se_beta[0], se_beta[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        beta_hat, se_beta = fgls_estimation(x, y, omega)\n",
    "        t_stat = (beta_hat[1] - beta_1_true) / se_beta[1]\n",
    "        p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_observations - 2))\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test: {0.01: 0.6018, 0.05: 0.8908}\n",
      "Poder del test: {0: 0.0532, 0.4: 0.5276}\n",
      "Estadísticas descriptivas:\n",
      "           beta_0    beta_1\n",
      "mean   -2.958325  0.798089\n",
      "median -2.890889  0.796406\n",
      "std     4.649156  0.162026\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para estimar por FGLS usando statsmodels\n",
    "def fgls_estimation(x, y, omega):\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    model = sm.GLS(y, X, sigma=omega)  # Estimación FGLS con sigma = omega\n",
    "    results = model.fit()  # Ajustar el modelo\n",
    "    return results\n",
    "\n",
    "# Simulaciones\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    result = fgls_estimation(x, y, omega)  # Estimación FGLS\n",
    "    t_stat = result.tvalues[1]  # Estadístico t para beta_1\n",
    "    p_value = result.pvalues[1]  # Valor p para beta_1\n",
    "    results.append([result.params[0], result.params[1], result.bse[0], result.bse[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        result = fgls_estimation(x, y, omega)\n",
    "        t_stat = result.tvalues[1]\n",
    "        p_value = result.pvalues[1]\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta_0_sm</th>\n",
       "      <th>beta_1_sm</th>\n",
       "      <th>se_beta_0_sm</th>\n",
       "      <th>se_beta_1_sm</th>\n",
       "      <th>beta_0_manual</th>\n",
       "      <th>beta_1_manual</th>\n",
       "      <th>se_beta_0_manual</th>\n",
       "      <th>se_beta_1_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.124395</td>\n",
       "      <td>0.803626</td>\n",
       "      <td>3.777375</td>\n",
       "      <td>0.136698</td>\n",
       "      <td>-3.124395</td>\n",
       "      <td>0.803626</td>\n",
       "      <td>3.777375</td>\n",
       "      <td>0.136698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>-3.050941</td>\n",
       "      <td>0.804804</td>\n",
       "      <td>3.135215</td>\n",
       "      <td>0.118106</td>\n",
       "      <td>-3.050941</td>\n",
       "      <td>0.804804</td>\n",
       "      <td>3.135215</td>\n",
       "      <td>0.118106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.543793</td>\n",
       "      <td>0.162838</td>\n",
       "      <td>2.673090</td>\n",
       "      <td>0.088475</td>\n",
       "      <td>4.543793</td>\n",
       "      <td>0.162838</td>\n",
       "      <td>2.673090</td>\n",
       "      <td>0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        beta_0_sm  beta_1_sm  se_beta_0_sm  se_beta_1_sm  beta_0_manual  \\\n",
       "mean    -3.124395   0.803626      3.777375      0.136698      -3.124395   \n",
       "median  -3.050941   0.804804      3.135215      0.118106      -3.050941   \n",
       "std      4.543793   0.162838      2.673090      0.088475       4.543793   \n",
       "\n",
       "        beta_1_manual  se_beta_0_manual  se_beta_1_manual  \n",
       "mean         0.803626          3.777375          0.136698  \n",
       "median       0.804804          3.135215          0.118106  \n",
       "std          0.162838          2.673090          0.088475  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Parámetros\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_observations = 5  # Pequeña muestra\n",
    "n_samples = 5000  # Número de simulaciones\n",
    "\n",
    "# Generar datos\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_data():\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    return x, y\n",
    "\n",
    "# Método 1: Estimación con FGLS usando statsmodels\n",
    "def fgls_statsmodels(x, y):\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    model = sm.GLS(y, X, sigma=omega)  # Estimación FGLS con sigma = omega\n",
    "    results = model.fit()  # Ajustar el modelo\n",
    "    return results\n",
    "\n",
    "# Método 2: Estimación manual FGLS\n",
    "def fgls_manual(x, y, omega):\n",
    "    X = np.column_stack([np.ones(n_observations), x])  # Matriz de diseño con intercepto\n",
    "    # Calcular la inversa de Omega\n",
    "    omega_inv = np.linalg.inv(omega)\n",
    "    # Estimar Beta usando la fórmula FGLS\n",
    "    beta_hat = np.linalg.inv(X.T @ omega_inv @ X) @ (X.T @ omega_inv @ y)\n",
    "    # Estimación de errores estándar (desviación estándar)\n",
    "    residuals = y - X @ beta_hat\n",
    "    sigma_hat = (residuals.T @ omega_inv @ residuals) / (n_observations - 2)  # Varianza de los errores\n",
    "    cov_beta_hat = np.linalg.inv(X.T @ omega_inv @ X) * sigma_hat  # Varianza de los coeficientes\n",
    "    se_beta_hat = np.sqrt(np.diag(cov_beta_hat))  # Desviación estándar de los coeficientes\n",
    "    return beta_hat, se_beta_hat\n",
    "\n",
    "# Comparación de ambos métodos\n",
    "results_fgls_sm = []\n",
    "results_fgls_manual = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x, y = generate_data()\n",
    "    \n",
    "    # Método 1: FGLS con statsmodels\n",
    "    result_sm = fgls_statsmodels(x, y)\n",
    "    results_fgls_sm.append([result_sm.params[0], result_sm.params[1], result_sm.bse[0], result_sm.bse[1]])\n",
    "    \n",
    "    # Método 2: Estimación manual\n",
    "    beta_hat_manual, se_beta_hat_manual = fgls_manual(x, y, omega)\n",
    "    results_fgls_manual.append([beta_hat_manual[0], beta_hat_manual[1], se_beta_hat_manual[0], se_beta_hat_manual[1]])\n",
    "\n",
    "# Convertir resultados a DataFrame para comparar\n",
    "results_fgls_sm_df = pd.DataFrame(results_fgls_sm, columns=['beta_0_sm', 'beta_1_sm', 'se_beta_0_sm', 'se_beta_1_sm'])\n",
    "results_fgls_manual_df = pd.DataFrame(results_fgls_manual, columns=['beta_0_manual', 'beta_1_manual', 'se_beta_0_manual', 'se_beta_1_manual'])\n",
    "\n",
    "# Comparar medias, medianas y desviaciones estándar\n",
    "comparison = pd.concat([\n",
    "    results_fgls_sm_df[['beta_0_sm', 'beta_1_sm', 'se_beta_0_sm', 'se_beta_1_sm']].agg(['mean', 'median', 'std']),\n",
    "    results_fgls_manual_df[['beta_0_manual', 'beta_1_manual', 'se_beta_0_manual', 'se_beta_1_manual']].agg(['mean', 'median', 'std'])\n",
    "], axis=1)\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test: {0.01: 0.298, 0.05: 0.6176}\n",
      "Poder del test: {0: 0.0282, 0.4: 0.2734}\n",
      "Estadísticas descriptivas:\n",
      "           beta_0    beta_1\n",
      "mean   -2.958325  0.798089\n",
      "median -2.890889  0.796406\n",
      "std     4.649156  0.162026\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para estimar por FGLS usando statsmodels con errores robustos\n",
    "def fgls_estimation(x, y, omega):\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    model = sm.GLS(y, X, sigma=omega)  # Estimación FGLS con sigma = omega\n",
    "    results = model.fit()  # Ajustar el modelo\n",
    "    robust_results = results.get_robustcov_results(cov_type='HC3')  # Errores estándar robustos (HC3)\n",
    "    return robust_results\n",
    "\n",
    "# Simulaciones\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    robust_results = fgls_estimation(x, y, omega)  # Estimación FGLS con errores robustos\n",
    "    t_stat = robust_results.tvalues[1]  # Estadístico t para beta_1\n",
    "    p_value = robust_results.pvalues[1]  # Valor p para beta_1\n",
    "    results.append([robust_results.params[0], robust_results.params[1], robust_results.bse[0], robust_results.bse[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        robust_results = fgls_estimation(x, y, omega)\n",
    "        t_stat = robust_results.tvalues[1]\n",
    "        p_value = robust_results.pvalues[1]\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sigma must be a scalar, 1d of length 5 or a 2d array of shape 5 x 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m u \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmultivariate_normal(mean\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(n_observations), cov\u001b[38;5;241m=\u001b[39momega)  \u001b[38;5;66;03m# u ~ N(0, omega)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m y \u001b[38;5;241m=\u001b[39m beta_0 \u001b[38;5;241m+\u001b[39m beta_1_true \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m u  \u001b[38;5;66;03m# Generar y\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m fgls_result \u001b[38;5;241m=\u001b[39m \u001b[43mfgls_white_estimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Estimación FGLS White\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Obtener estadísticos de interés\u001b[39;00m\n\u001b[0;32m     43\u001b[0m t_stat \u001b[38;5;241m=\u001b[39m fgls_result\u001b[38;5;241m.\u001b[39mtvalues[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Estadístico t para beta_1\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m, in \u001b[0;36mfgls_white_estimation\u001b[1;34m(x, y, omega)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Paso 3: Estimación FGLS con la matriz robusta\u001b[39;00m\n\u001b[0;32m     26\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(robust_cov)  \u001b[38;5;66;03m# Matriz de pesos (inversa de la matriz de varianzas-covarianzas)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m fgls_model \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Estimación FGLS usando la matriz robusta\u001b[39;00m\n\u001b[0;32m     28\u001b[0m fgls_results \u001b[38;5;241m=\u001b[39m fgls_model\u001b[38;5;241m.\u001b[39mfit()  \u001b[38;5;66;03m# Ajuste FGLS\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fgls_results\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:534\u001b[0m, in \u001b[0;36mGLS.__init__\u001b[1;34m(self, endog, exog, sigma, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_kwargs(kwargs)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# TODO: add options igls, for iterative fgls if sigma is None\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# TODO: default if sigma is none should be two-step GLS\u001b[39;00m\n\u001b[1;32m--> 534\u001b[0m sigma, cholsigmainv \u001b[38;5;241m=\u001b[39m \u001b[43m_get_sigma\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    537\u001b[0m                           hasconst\u001b[38;5;241m=\u001b[39mhasconst, sigma\u001b[38;5;241m=\u001b[39msigma,\n\u001b[0;32m    538\u001b[0m                           cholsigmainv\u001b[38;5;241m=\u001b[39mcholsigmainv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# store attribute names for data arrays\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:181\u001b[0m, in \u001b[0;36m_get_sigma\u001b[1;34m(sigma, nobs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sigma\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (nobs, nobs):\n\u001b[1;32m--> 181\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSigma must be a scalar, 1d of length \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m or a 2d \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray of shape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m x \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (nobs, nobs, nobs))\n\u001b[0;32m    183\u001b[0m     cholsigmainv, info \u001b[38;5;241m=\u001b[39m dtrtri(cholesky(sigma, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    184\u001b[0m                                 lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, overwrite_c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Sigma must be a scalar, 1d of length 5 or a 2d array of shape 5 x 5"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Parámetros del modelo\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas (heterocedasticidad)\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para realizar la estimación FGLS con errores robustos de White\n",
    "def fgls_white_estimation(x, y, omega):\n",
    "    # Paso 1: Estimación OLS inicial\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "    ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "    \n",
    "    # Paso 2: Estimación de la matriz de varianzas-covarianzas de los errores\n",
    "    residuals = ols_results.resid  # Residuos de la estimación OLS\n",
    "    # Usamos la matriz de varianzas-covarianzas robusta (White)\n",
    "    robust_cov = ols_results.get_robustcov_results(cov_type='HC3').cov_params()  # Matriz robusta de varianzas-covarianzas\n",
    "    \n",
    "    # Paso 3: Estimación FGLS con la matriz robusta\n",
    "    W = np.linalg.inv(robust_cov)  # Matriz de pesos (inversa de la matriz de varianzas-covarianzas)\n",
    "    fgls_model = sm.GLS(y, X, sigma=W)  # Estimación FGLS usando la matriz robusta\n",
    "    fgls_results = fgls_model.fit()  # Ajuste FGLS\n",
    "    \n",
    "    return fgls_results\n",
    "\n",
    "# Simulación de los datos\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    fgls_result = fgls_white_estimation(x, y, omega)  # Estimación FGLS White\n",
    "    \n",
    "    # Obtener estadísticos de interés\n",
    "    t_stat = fgls_result.tvalues[1]  # Estadístico t para beta_1\n",
    "    p_value = fgls_result.pvalues[1]  # Valor p para beta_1\n",
    "    results.append([fgls_result.params[0], fgls_result.params[1], fgls_result.bse[0], fgls_result.bse[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        fgls_result = fgls_white_estimation(x, y, omega)\n",
    "        p_value = fgls_result.pvalues[1]\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
