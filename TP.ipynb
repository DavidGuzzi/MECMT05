{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNIVERSIDAD TORCUATO DI TELLA**\n",
    "## **MAESTRÍA EN ECONOMETRÍA**\n",
    "\n",
    "---\n",
    "\n",
    "### **TRABAJO PRÁCTICO DE ECONOMETRÍA**\n",
    "\n",
    "- **Profesor:** González-Rozada, Martín  \n",
    "- **Ayudante:** Lening, Iara  \n",
    "- **Alumno:** Guzzi, David Alexander  (Legajo n°: 24H1970)  \n",
    "\n",
    "**Ciclo Lectivo:** Tercer Trimestre, 2024  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. PROPIEDADES DE MUESTRA FINITA DE FGLS (MCGE).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con un nivel de significatividad del 5%, el tamaño del test es 1.0000\n",
      "Con un nivel de significatividad del 1%, el tamaño del test es 1.0000\n",
      "Con un nivel de significatividad del 5%, cuando el valor verdadero de b es 0.5, la potencia del test es 1.0000\n",
      "Con un nivel de significatividad del 1%, cuando el valor verdadero de b es 0.5, la potencia del test es 1.0000\n",
      "Con un nivel de significatividad del 5%, cuando el valor verdadero de b es 0, la potencia del test es 0.0522\n",
      "Con un nivel de significatividad del 1%, cuando el valor verdadero de b es 0, la potencia del test es 0.0118\n",
      "             beta0        beta1    gamma_hat\n",
      "count  5000.000000  5000.000000  5000.000000\n",
      "mean      0.994148     1.001113     0.992565\n",
      "std       0.244381     0.076791     0.098951\n",
      "min      -0.034371     0.728956     0.670566\n",
      "50%       0.989151     1.003207     0.986854\n",
      "max       1.899646     1.343259     1.406991\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "np.random.seed(144)\n",
    "n_reps = 5000\n",
    "n_obs = 500\n",
    "\n",
    "# Función que ejecuta el procedimiento una vez\n",
    "def run_simulation():\n",
    "    x = np.random.uniform(1, 10, n_obs)\n",
    "    x2 = x ** 2\n",
    "    u = np.random.normal(scale=x)\n",
    "    \n",
    "    y0 = 1 + x + u\n",
    "    y1 = 1 + 0.5 * x + u\n",
    "    y2 = 1 + u\n",
    "    \n",
    "    def fgls(y, x):\n",
    "        ols = sm.OLS(y, sm.add_constant(x)).fit()\n",
    "        resid2 = ols.resid ** 2\n",
    "        gamma_hat = sm.OLS(resid2, x2).fit().params[0]\n",
    "        \n",
    "        uhat2_hat = gamma_hat * x2\n",
    "        weights = np.sqrt(uhat2_hat)\n",
    "        x_gls = x / weights\n",
    "        y_gls = y / weights\n",
    "        cons_gls = 1 / weights\n",
    "        gls = sm.OLS(y_gls, np.column_stack((cons_gls, x_gls))).fit()\n",
    "        \n",
    "        p_value = gls.pvalues[1]  # Para la prueba H0: beta = 1\n",
    "        return p_value, gls.params[0], gls.params[1], gamma_hat\n",
    "    \n",
    "    pv0, beta0, beta1, gamma_hat = fgls(y0, x)\n",
    "    pv1, _, _, _ = fgls(y1, x)\n",
    "    pv2, _, _, _ = fgls(y2, x)\n",
    "    \n",
    "    return pv0, pv1, pv2, beta0, beta1, gamma_hat\n",
    "\n",
    "# Ejecutamos la simulación\n",
    "results = np.array([run_simulation() for _ in range(n_reps)])\n",
    "\n",
    "# Extraemos los resultados\n",
    "pv0, pv1, pv2, beta0_vals, beta1_vals, gamma_hat_vals = results.T\n",
    "\n",
    "# Calculamos los tamaños del test y la potencia\n",
    "pv0_5 = np.mean(pv0 < 0.05)\n",
    "pv0_1 = np.mean(pv0 < 0.01)\n",
    "pv1_5 = np.mean(pv1 < 0.05)\n",
    "pv1_1 = np.mean(pv1 < 0.01)\n",
    "pv2_5 = np.mean(pv2 < 0.05)\n",
    "pv2_1 = np.mean(pv2 < 0.01)\n",
    "\n",
    "# Mostramos los resultados\n",
    "print(f\"Con un nivel de significatividad del 5%, el tamaño del test es {pv0_5:.4f}\")\n",
    "print(f\"Con un nivel de significatividad del 1%, el tamaño del test es {pv0_1:.4f}\")\n",
    "print(f\"Con un nivel de significatividad del 5%, cuando el valor verdadero de b es 0.5, la potencia del test es {pv1_5:.4f}\")\n",
    "print(f\"Con un nivel de significatividad del 1%, cuando el valor verdadero de b es 0.5, la potencia del test es {pv1_1:.4f}\")\n",
    "print(f\"Con un nivel de significatividad del 5%, cuando el valor verdadero de b es 0, la potencia del test es {pv2_5:.4f}\")\n",
    "print(f\"Con un nivel de significatividad del 1%, cuando el valor verdadero de b es 0, la potencia del test es {pv2_1:.4f}\")\n",
    "\n",
    "# Estadísticas de resumen\n",
    "summary_df = pd.DataFrame({\n",
    "    'beta0': beta0_vals,\n",
    "    'beta1': beta1_vals,\n",
    "    'gamma_hat': gamma_hat_vals\n",
    "})\n",
    "print(summary_df.describe(percentiles=[0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Función para realizar FGLS\n",
    "def fgls_estimation(x, y):\n",
    "    # DataFrame para almacenar las variables intermedias\n",
    "    df = pd.DataFrame({'x': x, 'y': y})\n",
    "    df['x2'] = df['x'] ** 2\n",
    "\n",
    "    # 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "    ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "    # 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "    df['u_hat'] = ols_results.resid\n",
    "    df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "    # 3. Dada la forma funcional de la heterocedasticidad de White para este modelo, sigma2 ~ x + x2, se estima por OLS usando u_hat2 como proxy de sigma2\n",
    "    aux_X = sm.add_constant(df[['x', 'x2']])\n",
    "    ols_model_aux = sm.OLS(df['u_hat2'], aux_X)\n",
    "    ols_model_aux_results = ols_model_aux.fit()\n",
    "    gamma_hat1 = ols_model_aux_results.params['x']\n",
    "    gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "    # 4. Usar las estimaciones de la regresión auxiliar y obtener las varianzas ajustadas (no consistentes)\n",
    "    df['sigma2_hat'] = ols_model_aux_results.predict(aux_X)\n",
    "\n",
    "    # 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por sigma2_hat y estimar por OLS\n",
    "    df['u_hat2_over_sigma2_hat'] = df['u_hat2'] / df['sigma2_hat']\n",
    "    df['const1'] = 1 / df['sigma2_hat']\n",
    "    df['x_over_sigma2_hat'] = df['x'] / df['sigma2_hat']\n",
    "    df['x2_over_sigma2_hat'] = df['x2'] / df['sigma2_hat']\n",
    "    \n",
    "\n",
    "    # Crear la matriz de diseño constante, x_over_sigma2_hat y x2_over_sigma2_hat\n",
    "    aux_X_2 = df[['const1', 'x_over_sigma2_hat', 'x2_over_sigma2_hat']]\n",
    "\n",
    "    ols_model_aux2 = sm.OLS(df['u_hat2_over_sigma2_hat'], aux_X_2)\n",
    "    ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "    gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "    gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "    # 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "    df['sigma2_tilde'] = ols_model_aux_results2.predict(aux_X_2)\n",
    "    \n",
    "    # Corregir valores negativos o muy pequeños\n",
    "    df['sigma2_tilde'] = df['sigma2_tilde'].clip(lower=1e-10)\n",
    "    df['sigma_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "    # Verificar si hay valores NaN\n",
    "    if df['sigma_tilde'].isna().any():\n",
    "        raise ValueError(\"Se encontraron valores NaN en sigma_tilde\")\n",
    "\n",
    "    # 7. Usar uno sobre sigma tilde como ponderador en la regresión de y ~ x\n",
    "    df['y_star'] = df['y'] / df['sigma_tilde']\n",
    "    df['const2'] = 1 / df['sigma_tilde']\n",
    "    df['x_star'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "    # Crear la matriz de diseño con constante y x_star\n",
    "    aux_X_star = df[['const2', 'x_star']]\n",
    "\n",
    "    # Ajustar el modelo OLS\n",
    "    final_ols_model = sm.OLS(df['y_star'], aux_X_star)\n",
    "    final_ols_results = final_ols_model.fit() \n",
    "\n",
    "    # Estimaciones finales\n",
    "    beta_0_hat = final_ols_results.params['const2']\n",
    "    beta_1_hat = final_ols_results.params['x_star']\n",
    "    se_beta1_hat = final_ols_results.bse.iloc[1]\n",
    "\n",
    "    return beta_0_hat, beta_1_hat, se_beta1_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Función para realizar FGLS\n",
    "def fgls_estimation2(x, y):\n",
    "    # DataFrame para almacenar las variables intermedias\n",
    "    df = pd.DataFrame({'x': x, 'y': y})\n",
    "    df['x2'] = df['x'] ** 2\n",
    "\n",
    "    # 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "    ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "    # 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "    df['u_hat'] = ols_results.resid\n",
    "    df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "    # 3. Dada la forma funcional de la heterocedasticidad de White para este modelo, sigma2 ~ x + x2, se estima por OLS usando u_hat2 como proxy de sigma2\n",
    "    aux_X = sm.add_constant(df[['x', 'x2']])\n",
    "    ols_model_aux = sm.OLS(df['u_hat2'], aux_X)\n",
    "    ols_model_aux_results = ols_model_aux.fit()\n",
    "    gamma_hat1 = ols_model_aux_results.params['x']\n",
    "    gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "    # 4. Usar las estimaciones de la regresión auxiliar y obtener las varianzas ajustadas (no consistentes)\n",
    "    df['sigma2_hat'] = ols_model_aux_results.predict(aux_X).clip(lower=1e-10)\n",
    "    df['sigma_hat'] = np.sqrt(df['sigma2_hat'])\n",
    "\n",
    "    # 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por sigma2_hat y estimar por OLS\n",
    "    df['y_over_sigma2_hat'] = df['y'] / df['sigma_hat']\n",
    "    df['const1'] = 1 / df['sigma_hat']\n",
    "    df['x_over_sigma2_hat'] = df['x'] / df['sigma_hat']\n",
    "    # df['x2_over_sigma2_hat'] = df['x2'] / df['sigma_hat']\n",
    "    \n",
    "\n",
    "    # Crear la matriz de diseño constante, x_over_sigma2_hat y x2_over_sigma2_hat\n",
    "    aux_X_2 = df[['const1', 'x_over_sigma2_hat']]\n",
    "\n",
    "    ols_model_aux2 = sm.OLS(df['y_over_sigma2_hat'], aux_X_2)\n",
    "    ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "    # gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "    # gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "    # Estimaciones finales\n",
    "    beta_0_hat = ols_model_aux_results2.params['const1']\n",
    "    beta_1_hat = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "    se_beta1_hat = ols_model_aux_results2.bse.iloc[1]\n",
    "\n",
    "    # # 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "    # df['sigma2_tilde'] = ols_model_aux_results2.predict(aux_X_2)\n",
    "    \n",
    "    # # Corregir valores negativos o muy pequeños\n",
    "    # df['sigma2_tilde'] = df['sigma2_tilde'].clip(lower=1e-10)\n",
    "    # df['sigma_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "    # # Verificar si hay valores NaN\n",
    "    # if df['sigma_tilde'].isna().any():\n",
    "    #     raise ValueError(\"Se encontraron valores NaN en sigma_tilde\")\n",
    "\n",
    "    # # 7. Usar uno sobre sigma tilde como ponderador en la regresión de y ~ x\n",
    "    # df['y_star'] = df['y'] / df['sigma_tilde']\n",
    "    # df['const2'] = 1 / df['sigma_tilde']\n",
    "    # df['x_star'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "    # # Crear la matriz de diseño con constante y x_star\n",
    "    # aux_X_star = df[['const2', 'x_star']]\n",
    "\n",
    "    # # Ajustar el modelo OLS\n",
    "    # final_ols_model = sm.OLS(df['y_star'], aux_X_star)\n",
    "    # final_ols_results = final_ols_model.fit() \n",
    "\n",
    "    # # Estimaciones finales\n",
    "    # beta_0_hat = final_ols_results.params['const2']\n",
    "    # beta_1_hat = final_ols_results.params['x_star']\n",
    "    # se_beta1_hat = final_ols_results.bse.iloc[1]\n",
    "\n",
    "    return beta_0_hat, beta_1_hat, se_beta1_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgls_estimation_numpy(x, y):\n",
    "    # 1. Estimación inicial por OLS\n",
    "    X = np.column_stack((np.ones_like(x), x))  # Agregar intercepto\n",
    "    beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y  # Estimación OLS\n",
    "    u_hat = y - X @ beta_ols  # Residuos\n",
    "    u_hat2 = u_hat ** 2  # Cuadrado de los residuos\n",
    "    \n",
    "    # 2. Regresión auxiliar para estimar la heterocedasticidad\n",
    "    X_aux = np.column_stack((np.ones_like(x), x, x**2))\n",
    "    gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ u_hat2\n",
    "    sigma2_hat = np.maximum(X_aux @ gamma_hat, 1e-10)  # Asegurar positividad\n",
    "    sigma_hat = np.sqrt(sigma2_hat)\n",
    "    \n",
    "    # 3. Transformación de variables y segunda estimación por OLS\n",
    "    y_trans = y / sigma_hat\n",
    "    X_trans = np.column_stack((np.ones_like(x) / sigma_hat, x / sigma_hat))\n",
    "    beta_fgls = np.linalg.inv(X_trans.T @ X_trans) @ X_trans.T @ y_trans\n",
    "    \n",
    "    # 4. Cálculo de errores estándar\n",
    "    residuals_fgls = y_trans - X_trans @ beta_fgls\n",
    "    sigma2_fgls = np.sum(residuals_fgls**2) / (len(x) - X_trans.shape[1])\n",
    "    var_beta_fgls = sigma2_fgls * np.linalg.inv(X_trans.T @ X_trans)\n",
    "    se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]\n",
    "    \n",
    "    return beta_fgls[0], beta_fgls[1], se_beta1_fgls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "def fgls_estimation_numpy2(x, y):\n",
    "    # 1. Estimación inicial por OLS\n",
    "    X = np.column_stack((np.ones_like(x), x))  # Agregar intercepto\n",
    "    beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y  # Estimación OLS\n",
    "    u_hat = y - X @ beta_ols  # Residuos\n",
    "    u_hat2 = u_hat ** 2  # Cuadrado de los residuos\n",
    "    \n",
    "    # 2. Regresión auxiliar para estimar la heterocedasticidad\n",
    "    X_aux = np.column_stack((np.ones_like(x), x, x**2))\n",
    "    gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ u_hat2\n",
    "    sigma2_hat = X_aux @ gamma_hat\n",
    "\n",
    "    X_aux2 = np.column_stack((np.ones_like(x) / sigma2_hat, x / sigma2_hat,  x**2 / sigma2_hat))\n",
    "    u_hat2_over_sigma2_hat = u_hat2 / sigma2_hat\n",
    "    gamma_hat2 = np.linalg.inv(X_aux2.T @ X_aux2) @ X_aux2.T @ u_hat2_over_sigma2_hat\n",
    "\n",
    "    sigma2_tilde = X_aux2 @ gamma_hat2  # Asegurar positividad\n",
    "    sigma_tilde = np.sqrt(sigma2_tilde)\n",
    "    \n",
    "    # 3. Transformación de variables y segunda estimación por OLS\n",
    "    y_trans = y / sigma_tilde\n",
    "    X_trans = np.column_stack((np.ones_like(x) / sigma_tilde, x / sigma_tilde))\n",
    "    beta_fgls = np.linalg.inv(X_trans.T @ X_trans) @ X_trans.T @ y_trans\n",
    "    \n",
    "    # 4. Cálculo de errores estándar\n",
    "    residuals_fgls = y_trans - X_trans @ beta_fgls\n",
    "    sigma2_fgls = np.sum(residuals_fgls**2) / (len(x) - X_trans.shape[1])\n",
    "    var_beta_fgls = sigma2_fgls * np.linalg.inv(X_trans.T @ X_trans)\n",
    "    se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]\n",
    "    \n",
    "    return beta_fgls[0], beta_fgls[1], se_beta1_fgls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "def fgls_estimation_numpy3(x, y):\n",
    "    # 1. Estimación inicial por OLS\n",
    "    X = np.column_stack((np.ones_like(x), x))  # Agregar intercepto\n",
    "    beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y  # Estimación OLS\n",
    "    u_hat = y - X @ beta_ols  # Residuos\n",
    "    u_hat2 = u_hat ** 2  # Cuadrado de los residuos\n",
    "    \n",
    "    # 2. Regresión auxiliar para estimar la heterocedasticidad\n",
    "    X_aux = np.column_stack((np.ones_like(x), x, x**2))\n",
    "    gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ u_hat2\n",
    "    sigma2_hat = X_aux @ gamma_hat\n",
    "\n",
    "    X_aux2 = np.column_stack((np.ones_like(x) / sigma2_hat, x / sigma2_hat,  x**2 / sigma2_hat))\n",
    "    u_hat2_over_sigma2_hat = u_hat2 / sigma2_hat\n",
    "    gamma_hat2 = np.linalg.inv(X_aux2.T @ X_aux2) @ X_aux2.T @ u_hat2_over_sigma2_hat\n",
    "\n",
    "    sigma2_tilde = X_aux @ gamma_hat2  # Asegurar positividad\n",
    "    # sigma_tilde = np.sqrt(sigma2_tilde)\n",
    "    omega_tilde_inv = np.linalg.inv(np.diag(sigma2_tilde))\n",
    "    \n",
    "    # # 3. Transformación de variables y segunda estimación por OLS\n",
    "    # y_trans = y / sigma_tilde\n",
    "    # X_trans = np.column_stack((np.ones_like(x) / sigma_tilde, x / sigma_tilde))\n",
    "    beta_fgls = np.linalg.inv(X.T @ omega_tilde_inv @ X) @ X.T @ omega_tilde_inv @ y\n",
    "    \n",
    "    # 4. Cálculo de errores estándar\n",
    "    residuals_fgls = y - X @ beta_fgls\n",
    "    s2_fgls = (residuals_fgls.T @ omega_tilde_inv @ residuals_fgls) / (len(x) - X.shape[1])\n",
    "    var_beta_fgls = s2_fgls * np.linalg.inv(X.T @ omega_tilde_inv @ X)\n",
    "    se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]   \n",
    "    \n",
    "    return beta_fgls[0], beta_fgls[1], se_beta1_fgls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "def fgls_estimation_numpy4(x, y):\n",
    "    \n",
    "    X = np.column_stack((np.ones_like(x), x)) \n",
    "    beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y \n",
    "    u_hat = y - X @ beta_ols\n",
    "    u_hat2 = u_hat ** 2  \n",
    "    log_u_hat2 = np.log(u_hat2)\n",
    "    \n",
    "    X_aux = np.column_stack((x, x**2))\n",
    "    gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ log_u_hat2\n",
    "    log_sigma2_hat = X_aux @ gamma_hat\n",
    "    sigma2_hat = np.exp(log_sigma2_hat)\n",
    "\n",
    "    X_aux2 = np.column_stack((x / sigma2_hat,  x**2 / sigma2_hat))\n",
    "    u_hat2_over_sigma2_hat = u_hat2 / sigma2_hat\n",
    "    gamma_hat2 = np.linalg.inv(X_aux2.T @ X_aux2) @ X_aux2.T @ u_hat2_over_sigma2_hat\n",
    "\n",
    "    sigma2_tilde = X_aux @ gamma_hat2 \n",
    "    sigma2_tilde = np.maximum(sigma2_tilde, 1e-6)\n",
    "    omega_tilde_inv = np.linalg.inv(np.diag(sigma2_tilde))\n",
    "    \n",
    "    \n",
    "    beta_fgls = np.linalg.inv(X.T @ omega_tilde_inv @ X) @ X.T @ omega_tilde_inv @ y\n",
    "    \n",
    "    # 4. Cálculo de errores estándar\n",
    "    residuals_fgls = y - X @ beta_fgls\n",
    "    s2_fgls = (residuals_fgls.T @ omega_tilde_inv @ residuals_fgls) / ( X.shape[0] - X.shape[1])\n",
    "    var_beta_fgls = s2_fgls * np.linalg.inv(X.T @ omega_tilde_inv @ X)\n",
    "    se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]   \n",
    "    \n",
    "    return beta_fgls[0], beta_fgls[1], se_beta1_fgls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_hat2: [3.83471800e+02 6.52349252e+01 9.49057339e+02 2.62215621e+03\n",
      " 1.24377760e+00 4.24173889e+02 1.21758542e+03 1.29058113e+02\n",
      " 5.15169808e-01 6.52869419e+01], log_u_hat2:[ 5.94926608  4.17799499  6.85546922  7.87175224  0.2181532   6.05014349\n",
      "  7.10462501  4.86026279 -0.66325871  4.17879205]\n",
      "log_sigma2_hat:[6.13997452 4.93045529 5.37286811 5.35910452 5.00926144 4.47892004\n",
      " 6.01499937 2.11954635 1.44664436 5.73142636], sigma2_hat:[464.04174684 138.44252969 215.48000259 212.53453985 149.79406395\n",
      "  88.13943444 409.52557907   8.32735889   4.248833   308.40885704]\n",
      "gamma_hat2:[-304.15735833   28.44531123   -0.57043071]\n",
      "sigma2_tilde: [1.00000000e-06 6.20145497e+01 1.00000000e-06 1.00000000e-06\n",
      " 1.32073453e-01 2.12969458e+13 1.00000000e-06 2.51776725e+09\n",
      " 1.62827018e-04 1.00000000e-06], omega_tilde_inv:[[1.00000000e+06 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.61252481e-02 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+06 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+06\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  7.57154430e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 4.69550896e-14 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 1.00000000e+06 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.97177301e-10\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  6.14148690e+03 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+06]]\n",
      "beta_fgls: [-70.69216297   2.29814816]\n",
      "se_beta1_fgls:3.568907474070603\n",
      "t_stat: 0.4197778079943931, tp_value:0.6856943090549232, np_value:0.6746477781607636, chip_value:0.5170479345710157\n",
      "beta_0 = -70.69216296926643, beta_1 = 2.2981481564001456, se_beta1 = 3.568907474070591\n",
      "pval_beta1 = 0.5376363744745631, t_test_res = 0.6856943090549261, t_statistic = 0.4197778079943893\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm, t, chi2\n",
    "\n",
    "np.random.seed(3649)\n",
    "\n",
    "\n",
    "n_obs = 10\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "\n",
    "x = np.random.uniform(1, 50, n_obs)\n",
    "u = np.random.normal(scale=x)\n",
    "y = beta_0_true + beta_1_true * x + u\n",
    "\n",
    "X = np.column_stack((np.ones_like(x), x)) \n",
    "beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y \n",
    "u_hat = y - X @ beta_ols\n",
    "u_hat2 = u_hat ** 2  \n",
    "log_u_hat2 = np.log(u_hat2)\n",
    "\n",
    "print(f\"u_hat2: {u_hat2}, log_u_hat2:{log_u_hat2}\")\n",
    "\n",
    "\n",
    "X_aux = np.column_stack((np.ones_like(x), x, x**2))\n",
    "gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ log_u_hat2\n",
    "log_sigma2_hat = X_aux @ gamma_hat\n",
    "sigma2_hat = np.exp(log_sigma2_hat)\n",
    "\n",
    "print(f\"log_sigma2_hat:{log_sigma2_hat}, sigma2_hat:{sigma2_hat}\")\n",
    "\n",
    "X_aux2 = np.column_stack((np.ones_like(x) / sigma2_hat, x / sigma2_hat,  x**2 / sigma2_hat))\n",
    "u_hat2_over_sigma2_hat = np.log(u_hat2 / sigma2_hat)\n",
    "gamma_hat2 = np.linalg.inv(X_aux2.T @ X_aux2) @ X_aux2.T @ u_hat2_over_sigma2_hat\n",
    "\n",
    "print(f\"gamma_hat2:{gamma_hat2}\")\n",
    "\n",
    "log_sigma2_tilde = X_aux @ gamma_hat2\n",
    "sigma2_tilde = np.maximum(np.exp(log_sigma2_tilde), 1e-6)\n",
    "# sigma2_tilde = np.maximum(sigma2_tilde, 1e-6)\n",
    "omega_tilde_inv = np.linalg.inv(np.diag(sigma2_tilde))\n",
    "\n",
    "print(f\"sigma2_tilde: {sigma2_tilde}, omega_tilde_inv:{omega_tilde_inv}\")\n",
    "\n",
    "beta_fgls = np.linalg.inv(X.T @ omega_tilde_inv @ X) @ X.T @ omega_tilde_inv @ y\n",
    "\n",
    "print(f\"beta_fgls: {beta_fgls}\")\n",
    "\n",
    "residuals_fgls = y - X @ beta_fgls\n",
    "s2_fgls = (residuals_fgls.T @ omega_tilde_inv @ residuals_fgls) / ( X.shape[0] - X.shape[1])\n",
    "var_beta_fgls = s2_fgls * np.linalg.inv(X.T @ omega_tilde_inv @ X)\n",
    "se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]\n",
    "\n",
    "print(f\"se_beta1_fgls:{se_beta1_fgls}\")\n",
    "\n",
    "t_stat = (beta_fgls[1] - 0.8) / se_beta1_fgls\n",
    "tp_value = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "np_value = 2 * (1 - norm.cdf(abs(t_stat)))\n",
    "chip_value = 1 - chi2.cdf(t_stat, df=1)\n",
    "\n",
    "print(f\"t_stat: {t_stat}, tp_value:{tp_value}, np_value:{np_value}, chip_value:{chip_value}\")\n",
    "\n",
    "\n",
    "# Ajustar el modelo GLS (equivalente a WLS si Omega es diagonal)\n",
    "model = sm.GLS(y, X, sigma=sigma2_tilde).fit()  # sigma es la inversa de los pesos\n",
    "\n",
    "# Extraer coeficientes y errores estándar\n",
    "beta_0, beta_1 = model.params\n",
    "se_beta1 = model.bse[1]\n",
    "pval_beta1 = model.pvalues[1]\n",
    "hypothesis = 'x1 = 0.8'  # Aquí 'x1' es el nombre de la variable correspondiente a beta1 en el modelo\n",
    "\n",
    "# Realizar la prueba t\n",
    "t_test = model.t_test(hypothesis)\n",
    "\n",
    "# Obtener el estadístico t y el p-valor\n",
    "t_statistic = t_test.tvalue[0][0]\n",
    "p_value_res = t_test.pvalue\n",
    "\n",
    "print(f\"beta_0 = {beta_0}, beta_1 = {beta_1}, se_beta1 = {se_beta1}\")\n",
    "\n",
    "print(f\"pval_beta1 = {pval_beta1}, t_test_res = {p_value_res}, t_statistic = {t_statistic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reject_1pct_list: 0.04\n",
      "reject_5pct_list: 0.12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "np.random.seed(3649)\n",
    "\n",
    "reject_1pct_list = []\n",
    "reject_5pct_list = []\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    n_obs = 10\n",
    "    beta_0_true = -3\n",
    "    beta_1_true = 0.8\n",
    "\n",
    "    x = np.random.uniform(1, 50, n_obs)\n",
    "    u = np.random.normal(scale=x)\n",
    "    y = beta_0_true + beta_1_true * x + u\n",
    "\n",
    "    X = np.column_stack((np.ones_like(x), x)) \n",
    "    beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y \n",
    "    u_hat = y - X @ beta_ols\n",
    "    u_hat2 = u_hat ** 2  \n",
    "    log_u_hat2 = np.log(u_hat2)\n",
    "\n",
    "    X_aux = np.column_stack((np.ones_like(x), x, x**2))\n",
    "    gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ log_u_hat2\n",
    "    log_sigma2_hat = X_aux @ gamma_hat\n",
    "    sigma2_hat = np.exp(log_sigma2_hat)\n",
    "\n",
    "    X_aux2 = np.column_stack((np.ones_like(x) / sigma2_hat, x / sigma2_hat,  x**2 / sigma2_hat))\n",
    "    u_hat2_over_sigma2_hat = u_hat2 / sigma2_hat\n",
    "    gamma_hat2 = np.linalg.inv(X_aux2.T @ X_aux2) @ X_aux2.T @ u_hat2_over_sigma2_hat\n",
    "\n",
    "    sigma2_tilde = X_aux @ gamma_hat2 \n",
    "    sigma2_tilde = np.maximum(sigma2_tilde, 1e-6)\n",
    "    omega_tilde_inv = np.linalg.inv(np.diag(sigma2_tilde))\n",
    "\n",
    "    beta_fgls = np.linalg.inv(X.T @ omega_tilde_inv @ X) @ X.T @ omega_tilde_inv @ y\n",
    "\n",
    "    residuals_fgls = y - X @ beta_fgls\n",
    "    s2_fgls = (residuals_fgls.T @ omega_tilde_inv @ residuals_fgls) / ( X.shape[0] - X.shape[1])\n",
    "    var_beta_fgls = s2_fgls * np.linalg.inv(X.T @ omega_tilde_inv @ X)\n",
    "    se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]\n",
    "\n",
    "    t_stat = (beta_fgls[1] - 0.8) / se_beta1_fgls\n",
    "    p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "    reject_1pct = p_value < 0.01\n",
    "    reject_5pct = p_value < 0.05\n",
    "\n",
    "    reject_1pct_list.append(reject_1pct)\n",
    "    reject_5pct_list.append(reject_5pct)\n",
    "\n",
    "print(f\"reject_1pct_list: {np.mean(reject_1pct_list)}\")\n",
    "print(f\"reject_5pct_list: {np.mean(reject_5pct_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-valor OLS (Python): 0.084999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Datos definidos manualmente\n",
    "X_values = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 50])\n",
    "y_values = np.array([2.5, 70.1, 3.8, 4.5, 5.2, 5.9, 6.5, 7.2, 7.8, 8.5])\n",
    "\n",
    "# Agregar constante\n",
    "X = sm.add_constant(X_values)\n",
    "\n",
    "# Ajustar modelo OLS\n",
    "model = sm.OLS(y_values, X)\n",
    "results = model.fit()\n",
    "\n",
    "hypothesis = 'x1 = 0.8'  # Aquí 'x1' es el nombre de la variable correspondiente a beta1 en el modelo\n",
    "\n",
    "# Realizar la prueba t\n",
    "t_test = results.t_test(hypothesis)\n",
    "\n",
    "# Obtener el estadístico t y el p-valor\n",
    "t_statistic = t_test.tvalue[0][0]\n",
    "p_value_res = t_test.pvalue\n",
    "\n",
    "print(f\"P-valor OLS (Python): {p_value_res:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estadístico F: 5.610111\n",
      "P-valor (distribución F): 0.045348\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f\n",
    "# Obtener estimaciones\n",
    "beta_1_hat = results.params[1]          # Coeficiente estimado\n",
    "se_beta1 = results.bse[1]                # Error estándar de beta_1\n",
    "n_obs = len(y_values)                     # Número de observaciones\n",
    "df_num = 1                                 # Grados de libertad del numerador\n",
    "df_den = n_obs - 2                         # Grados de libertad del denominador\n",
    "\n",
    "# Estadístico F\n",
    "H0 = 1  # Hipótesis nula beta_1 = 1\n",
    "F_stat = ((beta_1_hat - H0) ** 2) / (se_beta1 ** 2)\n",
    "\n",
    "# P-valor usando distribución F\n",
    "p_value_F = 1 - f.cdf(F_stat, df_num, df_den)\n",
    "\n",
    "print(f\"Estadístico F: {F_stat:.6f}\")\n",
    "print(f\"P-valor (distribución F): {p_value_F:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgls_estimation_numpy5(x, y):\n",
    "    \n",
    "    X = np.column_stack((np.ones_like(x), x)) \n",
    "    beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y \n",
    "    u_hat = y - X @ beta_ols\n",
    "    u_hat2 = u_hat ** 2  \n",
    "    log_u_hat2 = np.log(np.maximum(u_hat2, 1e-6))  # Evita log(0)\n",
    "    \n",
    "    # Estimación de la varianza condicional con un modelo cuadrático\n",
    "    X_aux = np.column_stack((np.ones_like(x), x, x**2))\n",
    "    gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ log_u_hat2\n",
    "    log_sigma2_hat = X_aux @ gamma_hat\n",
    "    sigma2_hat = np.exp(log_sigma2_hat)\n",
    "\n",
    "    # Asegurar que sigma2_hat no sea cero ni negativo\n",
    "    sigma2_hat = np.maximum(sigma2_hat, 1e-6)\n",
    "\n",
    "    # Matriz de pesos (diagonal)\n",
    "    omega_inv = np.diag(1 / sigma2_hat)\n",
    "\n",
    "    # Estimación FGLS\n",
    "    beta_fgls = np.linalg.inv(X.T @ omega_inv @ X) @ X.T @ omega_inv @ y\n",
    "\n",
    "    # Cálculo de errores estándar\n",
    "    residuals_fgls = y - X @ beta_fgls\n",
    "    s2_fgls = (residuals_fgls.T @ omega_inv @ residuals_fgls) / (len(x) - X.shape[1])\n",
    "    var_beta_fgls = s2_fgls * np.linalg.inv(X.T @ omega_inv @ X)\n",
    "    se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]   \n",
    "\n",
    "    return beta_fgls[0], beta_fgls[1], se_beta1_fgls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>mean_beta0</th>\n",
       "      <th>median_beta0</th>\n",
       "      <th>std_beta0</th>\n",
       "      <th>mean_beta1</th>\n",
       "      <th>median_beta1</th>\n",
       "      <th>std_beta1</th>\n",
       "      <th>test_size_1pct</th>\n",
       "      <th>test_size_5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>9.258171</td>\n",
       "      <td>-2.623278</td>\n",
       "      <td>893.446389</td>\n",
       "      <td>0.459207</td>\n",
       "      <td>0.786858</td>\n",
       "      <td>22.237986</td>\n",
       "      <td>0.1124</td>\n",
       "      <td>0.2204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>18.481732</td>\n",
       "      <td>-2.940235</td>\n",
       "      <td>994.586445</td>\n",
       "      <td>0.314504</td>\n",
       "      <td>0.795749</td>\n",
       "      <td>23.291711</td>\n",
       "      <td>0.1118</td>\n",
       "      <td>0.1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>-14.900211</td>\n",
       "      <td>-2.985833</td>\n",
       "      <td>630.791366</td>\n",
       "      <td>1.004283</td>\n",
       "      <td>0.801668</td>\n",
       "      <td>13.521860</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.1512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>-4.380422</td>\n",
       "      <td>-3.001054</td>\n",
       "      <td>88.507895</td>\n",
       "      <td>0.839335</td>\n",
       "      <td>0.800465</td>\n",
       "      <td>3.178552</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.1088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>-2.960183</td>\n",
       "      <td>-2.996994</td>\n",
       "      <td>8.543328</td>\n",
       "      <td>0.805510</td>\n",
       "      <td>0.799963</td>\n",
       "      <td>2.897765</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>-2.996185</td>\n",
       "      <td>-2.979176</td>\n",
       "      <td>2.097564</td>\n",
       "      <td>0.809875</td>\n",
       "      <td>0.798362</td>\n",
       "      <td>1.788327</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>700</td>\n",
       "      <td>-3.019977</td>\n",
       "      <td>-3.011490</td>\n",
       "      <td>1.855264</td>\n",
       "      <td>0.811589</td>\n",
       "      <td>0.799311</td>\n",
       "      <td>1.683692</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000</td>\n",
       "      <td>-3.005423</td>\n",
       "      <td>-2.993798</td>\n",
       "      <td>1.609359</td>\n",
       "      <td>0.808850</td>\n",
       "      <td>0.799982</td>\n",
       "      <td>1.456669</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.0606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1200</td>\n",
       "      <td>-2.985904</td>\n",
       "      <td>-2.994522</td>\n",
       "      <td>1.102379</td>\n",
       "      <td>0.791900</td>\n",
       "      <td>0.800001</td>\n",
       "      <td>0.993060</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0584</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  mean_beta0  median_beta0   std_beta0  mean_beta1  \\\n",
       "0               5    9.258171     -2.623278  893.446389    0.459207   \n",
       "1              10   18.481732     -2.940235  994.586445    0.314504   \n",
       "2              30  -14.900211     -2.985833  630.791366    1.004283   \n",
       "3             100   -4.380422     -3.001054   88.507895    0.839335   \n",
       "4             200   -2.960183     -2.996994    8.543328    0.805510   \n",
       "5             500   -2.996185     -2.979176    2.097564    0.809875   \n",
       "6             700   -3.019977     -3.011490    1.855264    0.811589   \n",
       "7            1000   -3.005423     -2.993798    1.609359    0.808850   \n",
       "8            1200   -2.985904     -2.994522    1.102379    0.791900   \n",
       "\n",
       "   median_beta1  std_beta1  test_size_1pct  test_size_5pct  \n",
       "0      0.786858  22.237986          0.1124          0.2204  \n",
       "1      0.795749  23.291711          0.1118          0.1968  \n",
       "2      0.801668  13.521860          0.0750          0.1512  \n",
       "3      0.800465   3.178552          0.0578          0.1088  \n",
       "4      0.799963   2.897765          0.0480          0.0952  \n",
       "5      0.798362   1.788327          0.0370          0.0768  \n",
       "6      0.799311   1.683692          0.0292          0.0690  \n",
       "7      0.799982   1.456669          0.0236          0.0606  \n",
       "8      0.800001   0.993060          0.0166          0.0584  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(3649)\n",
    "def simulate_fgls(n_samples, n_observations_list, beta_0_true, beta_1_true, beta1_H0):\n",
    "    \n",
    "    \"\"\"Realiza simulaciones de FGLS para distintos tamaños de muestra y reporta estadísticas de interés.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for n_obs in n_observations_list:\n",
    "        sample_results = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Generación de datos\n",
    "            x = np.random.uniform(1, 50, n_obs)\n",
    "            u = np.random.normal(scale=x)\n",
    "            y = beta_0_true + beta_1_true * x + u\n",
    "            \n",
    "            # Estimación FGLS\n",
    "            beta_0_hat, beta_1_hat, se_beta1_hat = fgls_estimation_numpy4(x, y)\n",
    "            \n",
    "            # Test de hipótesis para beta_1 = 0.8\n",
    "            t_stat = (beta_1_hat - beta1_H0) / se_beta1_hat\n",
    "            p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "            reject_1pct = p_value < 0.01\n",
    "            reject_5pct = p_value < 0.05\n",
    "            \n",
    "            sample_results.append([beta_0_hat, beta_1_hat, reject_1pct, reject_5pct])\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        df_results = pd.DataFrame(sample_results, columns=['beta_0_hat', 'beta_1_hat', 'reject_1pct', 'reject_5pct'])\n",
    "        \n",
    "        # Calcular estadísticas\n",
    "        mean_beta0 = df_results['beta_0_hat'].mean()\n",
    "        mean_beta1 = df_results['beta_1_hat'].mean()\n",
    "        median_beta0 = df_results['beta_0_hat'].median()\n",
    "        median_beta1 = df_results['beta_1_hat'].median()\n",
    "        std_beta0 = df_results['beta_0_hat'].std()\n",
    "        std_beta1 = df_results['beta_1_hat'].std()\n",
    "        test_size_1pct = df_results['reject_1pct'].mean()\n",
    "        test_size_5pct = df_results['reject_5pct'].mean()\n",
    "        \n",
    "        results.append([n_obs, mean_beta0, median_beta0, std_beta0, mean_beta1, median_beta1, std_beta1, test_size_1pct, test_size_5pct])\n",
    "    \n",
    "    # Crear DataFrame final\n",
    "    results_df = pd.DataFrame(results, columns=['n_observations', 'mean_beta0', 'median_beta0', 'std_beta0', 'mean_beta1', 'median_beta1', 'std_beta1', 'test_size_1pct', 'test_size_5pct'])\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Parámetros iniciales\n",
    "test_results = simulate_fgls(n_samples=5000, n_observations_list=[5, 10, 30, 100, 200, 500, 700, 1000, 1200], beta_0_true=-3, beta_1_true=0.8, beta1_H0=0.8)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>mean_beta0</th>\n",
       "      <th>median_beta0</th>\n",
       "      <th>std_beta0</th>\n",
       "      <th>mean_beta1</th>\n",
       "      <th>median_beta1</th>\n",
       "      <th>std_beta1</th>\n",
       "      <th>test_size_1pct</th>\n",
       "      <th>test_size_5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>9.258171</td>\n",
       "      <td>-2.623278</td>\n",
       "      <td>893.446389</td>\n",
       "      <td>0.059207</td>\n",
       "      <td>0.386858</td>\n",
       "      <td>22.237986</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.2364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>18.481732</td>\n",
       "      <td>-2.940235</td>\n",
       "      <td>994.586445</td>\n",
       "      <td>-0.085496</td>\n",
       "      <td>0.395749</td>\n",
       "      <td>23.291711</td>\n",
       "      <td>0.1356</td>\n",
       "      <td>0.2486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>-14.900211</td>\n",
       "      <td>-2.985833</td>\n",
       "      <td>630.791366</td>\n",
       "      <td>0.604283</td>\n",
       "      <td>0.401668</td>\n",
       "      <td>13.521860</td>\n",
       "      <td>0.2002</td>\n",
       "      <td>0.3870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>-4.380422</td>\n",
       "      <td>-3.001054</td>\n",
       "      <td>88.507895</td>\n",
       "      <td>0.439335</td>\n",
       "      <td>0.400465</td>\n",
       "      <td>3.178552</td>\n",
       "      <td>0.6858</td>\n",
       "      <td>0.8460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>-2.960183</td>\n",
       "      <td>-2.996994</td>\n",
       "      <td>8.543328</td>\n",
       "      <td>0.405510</td>\n",
       "      <td>0.399963</td>\n",
       "      <td>2.897765</td>\n",
       "      <td>0.9572</td>\n",
       "      <td>0.9864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  mean_beta0  median_beta0   std_beta0  mean_beta1  \\\n",
       "0               5    9.258171     -2.623278  893.446389    0.059207   \n",
       "1              10   18.481732     -2.940235  994.586445   -0.085496   \n",
       "2              30  -14.900211     -2.985833  630.791366    0.604283   \n",
       "3             100   -4.380422     -3.001054   88.507895    0.439335   \n",
       "4             200   -2.960183     -2.996994    8.543328    0.405510   \n",
       "\n",
       "   median_beta1  std_beta1  test_size_1pct  test_size_5pct  \n",
       "0      0.386858  22.237986          0.1230          0.2364  \n",
       "1      0.395749  23.291711          0.1356          0.2486  \n",
       "2      0.401668  13.521860          0.2002          0.3870  \n",
       "3      0.400465   3.178552          0.6858          0.8460  \n",
       "4      0.399963   2.897765          0.9572          0.9864  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(3649)\n",
    "def simulate_fgls(n_samples, n_observations_list, beta_0_true, beta_1_true, beta1_H0):\n",
    "    \n",
    "    \"\"\"Realiza simulaciones de FGLS para distintos tamaños de muestra y reporta estadísticas de interés.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for n_obs in n_observations_list:\n",
    "        sample_results = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Generación de datos\n",
    "            x = np.random.uniform(1, 50, n_obs)\n",
    "            u = np.random.normal(scale=x)\n",
    "            y = beta_0_true + beta_1_true * x + u\n",
    "            \n",
    "            # Estimación FGLS\n",
    "            beta_0_hat, beta_1_hat, se_beta1_hat = fgls_estimation_numpy4(x, y)\n",
    "            \n",
    "            # Test de hipótesis para beta_1 = 0.8\n",
    "            t_stat = (beta_1_hat - beta1_H0) / se_beta1_hat\n",
    "            p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "            reject_1pct = p_value < 0.01\n",
    "            reject_5pct = p_value < 0.05\n",
    "            \n",
    "            sample_results.append([beta_0_hat, beta_1_hat, reject_1pct, reject_5pct])\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        df_results = pd.DataFrame(sample_results, columns=['beta_0_hat', 'beta_1_hat', 'reject_1pct', 'reject_5pct'])\n",
    "        \n",
    "        # Calcular estadísticas\n",
    "        mean_beta0 = df_results['beta_0_hat'].mean()\n",
    "        mean_beta1 = df_results['beta_1_hat'].mean()\n",
    "        median_beta0 = df_results['beta_0_hat'].median()\n",
    "        median_beta1 = df_results['beta_1_hat'].median()\n",
    "        std_beta0 = df_results['beta_0_hat'].std()\n",
    "        std_beta1 = df_results['beta_1_hat'].std()\n",
    "        test_size_1pct = df_results['reject_1pct'].mean()\n",
    "        test_size_5pct = df_results['reject_5pct'].mean()\n",
    "        \n",
    "        results.append([n_obs, mean_beta0, median_beta0, std_beta0, mean_beta1, median_beta1, std_beta1, test_size_1pct, test_size_5pct])\n",
    "    \n",
    "    # Crear DataFrame final\n",
    "    results_df = pd.DataFrame(results, columns=['n_observations', 'mean_beta0', 'median_beta0', 'std_beta0', 'mean_beta1', 'median_beta1', 'std_beta1', 'test_size_1pct', 'test_size_5pct'])\n",
    "    \n",
    "    return results_df\n",
    "test_results = simulate_fgls(n_samples=5000, n_observations_list=[5, 10, 30, 100, 200], beta_0_true=-3, beta_1_true=0.4, beta1_H0=0.8)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promedio Beta1: 0.7996\n",
      "Promedio SE Beta1: 0.0155\n",
      "Proporción de rechazos H0 (p<0.05): 0.0106\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros\n",
    "n_obs = 500  # Número de observaciones por simulación\n",
    "n_sim = 5000  # Número de simulaciones\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "sigma = 1  # Desviación estándar del error\n",
    "\n",
    "# Almacenar resultados\n",
    "beta1_estimates = np.zeros(n_sim)\n",
    "se_beta1_estimates = np.zeros(n_sim)\n",
    "p_values = np.zeros(n_sim)\n",
    "\n",
    "for i in range(n_sim):\n",
    "    # Generar datos\n",
    "    x = np.random.uniform(0, 10, n_obs)\n",
    "    epsilon = np.random.normal(0, sigma, n_obs)\n",
    "    y = beta_0_true + beta_1_true * x + epsilon\n",
    "    \n",
    "    # Agregar constante para el intercepto\n",
    "    X = sm.add_constant(x)\n",
    "    \n",
    "    # Ajustar modelo OLS\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    \n",
    "    # Extraer beta1, su error estándar y calcular p-valor\n",
    "    beta1_estimates[i] = model.params[1]\n",
    "    se_beta1_estimates[i] = model.bse[1]\n",
    "    t_stat = (model.params[1] - 0.8) / model.bse[1]\n",
    "    p_values[i] = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "\n",
    "# Resultados promedio\n",
    "mean_beta1 = np.mean(beta1_estimates)\n",
    "mean_se_beta1 = np.mean(se_beta1_estimates)\n",
    "mean_p_value = np.mean(p_values)\n",
    "\n",
    "print(f\"Promedio Beta1: {mean_beta1:.4f}\")\n",
    "print(f\"Promedio SE Beta1: {mean_se_beta1:.4f}\")\n",
    "print(f\"Proporción de rechazos H0 (p<0.05): {np.mean(p_values < 0.01):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'beta_1_hat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'beta_1_hat'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m sns\u001b[38;5;241m.\u001b[39mhistplot(\u001b[43mtest_results\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbeta_1_hat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, kde\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39maxvline(\u001b[38;5;241m0.8\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValor de H0: 0.8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'beta_1_hat'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.histplot(df_results[\"beta_1_hat\"], kde=True)\n",
    "plt.axvline(beta1_H0, color='red', linestyle=\"--\", label=\"Valor de H0: 0.8\")\n",
    "plt.legend()\n",
    "plt.title(\"Distribución de Beta 1 Estimado\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>mean_beta0</th>\n",
       "      <th>median_beta0</th>\n",
       "      <th>std_beta0</th>\n",
       "      <th>mean_beta1</th>\n",
       "      <th>median_beta1</th>\n",
       "      <th>std_beta1</th>\n",
       "      <th>test_size_1pct</th>\n",
       "      <th>test_size_5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>7.886475</td>\n",
       "      <td>-3.021852</td>\n",
       "      <td>811.247071</td>\n",
       "      <td>-0.161186</td>\n",
       "      <td>0.438020</td>\n",
       "      <td>44.853066</td>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>-4.736752</td>\n",
       "      <td>-2.840837</td>\n",
       "      <td>716.166070</td>\n",
       "      <td>-0.040182</td>\n",
       "      <td>0.379416</td>\n",
       "      <td>48.722276</td>\n",
       "      <td>0.2140</td>\n",
       "      <td>0.3140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>-2.923546</td>\n",
       "      <td>-3.069659</td>\n",
       "      <td>162.486675</td>\n",
       "      <td>0.118178</td>\n",
       "      <td>0.391275</td>\n",
       "      <td>34.192519</td>\n",
       "      <td>0.3958</td>\n",
       "      <td>0.5120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>-3.011968</td>\n",
       "      <td>-2.999841</td>\n",
       "      <td>134.097435</td>\n",
       "      <td>0.240242</td>\n",
       "      <td>0.399580</td>\n",
       "      <td>26.483190</td>\n",
       "      <td>0.5702</td>\n",
       "      <td>0.6856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>-2.114750</td>\n",
       "      <td>-2.927595</td>\n",
       "      <td>86.312413</td>\n",
       "      <td>0.120036</td>\n",
       "      <td>0.393478</td>\n",
       "      <td>17.257694</td>\n",
       "      <td>0.6822</td>\n",
       "      <td>0.7818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>-2.666872</td>\n",
       "      <td>-2.989617</td>\n",
       "      <td>62.486918</td>\n",
       "      <td>0.329250</td>\n",
       "      <td>0.396693</td>\n",
       "      <td>12.473935</td>\n",
       "      <td>0.8006</td>\n",
       "      <td>0.8490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  mean_beta0  median_beta0   std_beta0  mean_beta1  \\\n",
       "0               5    7.886475     -3.021852  811.247071   -0.161186   \n",
       "1              10   -4.736752     -2.840837  716.166070   -0.040182   \n",
       "2              30   -2.923546     -3.069659  162.486675    0.118178   \n",
       "3             100   -3.011968     -2.999841  134.097435    0.240242   \n",
       "4             200   -2.114750     -2.927595   86.312413    0.120036   \n",
       "5             500   -2.666872     -2.989617   62.486918    0.329250   \n",
       "\n",
       "   median_beta1  std_beta1  test_size_1pct  test_size_5pct  \n",
       "0      0.438020  44.853066          0.0710          0.1600  \n",
       "1      0.379416  48.722276          0.2140          0.3140  \n",
       "2      0.391275  34.192519          0.3958          0.5120  \n",
       "3      0.399580  26.483190          0.5702          0.6856  \n",
       "4      0.393478  17.257694          0.6822          0.7818  \n",
       "5      0.396693  12.473935          0.8006          0.8490  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simulate_fgls(n_samples, n_observations_list, beta_0_true, beta_1_true, beta1_H0):\n",
    "    \n",
    "    \"\"\"Realiza simulaciones de FGLS para distintos tamaños de muestra y reporta estadísticas de interés.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for n_obs in n_observations_list:\n",
    "        sample_results = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            # Generación de datos\n",
    "            x = np.random.uniform(1, 50, n_obs)\n",
    "            u = np.random.normal(scale=x)\n",
    "            y = beta_0_true + beta_1_true * x + u\n",
    "            \n",
    "            # Estimación FGLS\n",
    "            beta_0_hat, beta_1_hat, se_beta1_hat = fgls_estimation(x, y)\n",
    "            \n",
    "            # Test de hipótesis para beta_1 = 0.8\n",
    "            t_stat = (beta_1_hat - beta1_H0) / se_beta1_hat\n",
    "            p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "            reject_1pct = p_value < 0.01\n",
    "            reject_5pct = p_value < 0.05\n",
    "            \n",
    "            sample_results.append([beta_0_hat, beta_1_hat, reject_1pct, reject_5pct])\n",
    "        \n",
    "        # Convertir a DataFrame\n",
    "        df_results = pd.DataFrame(sample_results, columns=['beta_0_hat', 'beta_1_hat', 'reject_1pct', 'reject_5pct'])\n",
    "        \n",
    "        # Calcular estadísticas\n",
    "        mean_beta0 = df_results['beta_0_hat'].mean()\n",
    "        mean_beta1 = df_results['beta_1_hat'].mean()\n",
    "        median_beta0 = df_results['beta_0_hat'].median()\n",
    "        median_beta1 = df_results['beta_1_hat'].median()\n",
    "        std_beta0 = df_results['beta_0_hat'].std()\n",
    "        std_beta1 = df_results['beta_1_hat'].std()\n",
    "        test_size_1pct = df_results['reject_1pct'].mean()\n",
    "        test_size_5pct = df_results['reject_5pct'].mean()\n",
    "        \n",
    "        results.append([n_obs, mean_beta0, median_beta0, std_beta0, mean_beta1, median_beta1, std_beta1, test_size_1pct, test_size_5pct])\n",
    "    \n",
    "    # Crear DataFrame final\n",
    "    results_df = pd.DataFrame(results, columns=['n_observations', 'mean_beta0', 'median_beta0', 'std_beta0', 'mean_beta1', 'median_beta1', 'std_beta1', 'test_size_1pct', 'test_size_5pct'])\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Parámetros iniciales\n",
    "test_results = simulate_fgls(n_samples=5000, n_observations_list=[5, 10, 30, 100, 200, 500], beta_0_true=-3, beta_1_true=0.4, beta1_H0=0.8)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "n_observations = 5  # Observaciones por muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "# Matriz omega (varianzas)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "\n",
    "# Descomposición de Cholesky de omega\n",
    "P = np.linalg.cholesky(omega)\n",
    "\n",
    "# Inversa de P\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "# Contenedores para almacenar muestras y estimadores\n",
    "samples = []\n",
    "results = []\n",
    "\n",
    "# Simulación de M muestras\n",
    "for sample_id in range(n_samples):\n",
    "    # 1. Generar datos\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "    # Almacenar muestra\n",
    "    samples.append(pd.DataFrame({'sample': i, 'x': x, 'y': y, 'u': u}))\n",
    "\n",
    "    # 2. Transformar para MCG\n",
    "    y_tilde = P_inv @ y\n",
    "    X = np.column_stack((np.ones(n_observations), x))  # Matriz de diseño original\n",
    "    X_tilde = P_inv @ X\n",
    "\n",
    "    # 3. Ajustar modelo transformado con MCO\n",
    "    beta_hat = np.linalg.inv(X_tilde.T @ X_tilde) @ (X_tilde.T @ y_tilde)\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append([i, beta_0_hat, beta_1_hat, se_beta1_hat])\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "# Crear DataFrame con los resultados de FGLS\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    'sample', 'beta_0_hat', 'beta_1_hat', 'se_beta1_hat'\n",
    "])\n",
    "\n",
    "# Mostrar ejemplos de los DataFrames generados\n",
    "print(\"Ejemplo de all_samples:\")\n",
    "print(all_samples.head())\n",
    "print(\"\\nEjemplo de results_df:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analizar.\n",
    "def fgls_estimation(x, y):\n",
    "    \"\"\"Función de estimación FGLS (suponiendo que ya está definida en el código original).\"\"\"\n",
    "    X = np.vstack([np.ones_like(x), x]).T\n",
    "    beta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    residuals = y - X @ beta_hat\n",
    "    weights = 1 / (x ** 2)\n",
    "    W = np.diag(weights)\n",
    "    beta_hat_wls = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ y)\n",
    "    residuals_wls = y - X @ beta_hat_wls\n",
    "    sigma_sq = np.sum(residuals_wls**2) / (len(y) - 2)\n",
    "    var_beta = sigma_sq * np.linalg.inv(X.T @ W @ X)\n",
    "    se_beta1_hat = np.sqrt(var_beta[1, 1])\n",
    "    return beta_hat_wls[0], beta_hat_wls[1], se_beta1_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6867746  8.01926913 4.94568308 7.5111866  9.80190561] [ 0.15715015 25.66899652 -0.77641616 -2.17923096 19.85642126]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "# Definir número de observaciones\n",
    "n = 5  # Ajusta según necesidad\n",
    "\n",
    "# Generar datos\n",
    "x = (10 - 1) * np.random.uniform(size=n) + 1\n",
    "u = np.random.normal(size=n) * x\n",
    "print(x,u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6867746  8.01926913 4.94568308 7.5111866  9.80190561] [ 0.15715015 25.66899652 -0.77641616 -2.17923096 19.85642126]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "x = np.random.uniform(1, 10, n)\n",
    "u = np.random.normal(scale=x)\n",
    "print(x,u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.57784606,  1.22254885,  0.13128065, -2.32968685, 10.14315422])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(7)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "u = np.random.multivariate_normal(mean=np.zeros(n), cov=omega)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mean and cov must have same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, n_observations)  \u001b[38;5;66;03m# x ~ U[1, 50]\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# u ~ N(0, omega)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m y \u001b[38;5;241m=\u001b[39m beta_0_true \u001b[38;5;241m+\u001b[39m beta_1_true \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m u\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:4210\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: mean and cov must have same length"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "u = np.random.multivariate_normal(mean=np.zeros(10), cov=omega)  # u ~ N(0, omega)\n",
    "y = beta_0_true + beta_1_true * x + u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mean and cov must have same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m omega \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag([\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m36\u001b[39m][:\u001b[38;5;241m10\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m u\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:4210\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: mean and cov must have same length"
     ]
    }
   ],
   "source": [
    "omega = np.diag([4, 9, 16, 25, 36][:10])\n",
    "u = np.random.multivariate_normal(mean=np.zeros(10), cov=omega)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "mean and cov must have same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m u\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:4210\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.multivariate_normal\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: mean and cov must have same length"
     ]
    }
   ],
   "source": [
    "u = np.random.multivariate_normal(mean=np.zeros(10), cov=omega)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>u</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44.691948</td>\n",
       "      <td>0.394367</td>\n",
       "      <td>33.147926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33.945501</td>\n",
       "      <td>-2.739349</td>\n",
       "      <td>21.417052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>37.364044</td>\n",
       "      <td>-5.117829</td>\n",
       "      <td>21.773406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>37.251050</td>\n",
       "      <td>1.502358</td>\n",
       "      <td>28.303198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>34.525174</td>\n",
       "      <td>6.038023</td>\n",
       "      <td>30.658162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.409759</td>\n",
       "      <td>0.030947</td>\n",
       "      <td>-0.241246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14.352028</td>\n",
       "      <td>-0.504023</td>\n",
       "      <td>7.977600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>30.112575</td>\n",
       "      <td>2.001504</td>\n",
       "      <td>23.091564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.797610</td>\n",
       "      <td>3.095406</td>\n",
       "      <td>4.733495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>20.579967</td>\n",
       "      <td>8.232626</td>\n",
       "      <td>21.696599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample  index          x         u          y\n",
       "0       0      0  44.691948  0.394367  33.147926\n",
       "1       0      1  33.945501 -2.739349  21.417052\n",
       "2       0      2  37.364044 -5.117829  21.773406\n",
       "3       0      3  37.251050  1.502358  28.303198\n",
       "4       0      4  34.525174  6.038023  30.658162\n",
       "5       1      0   3.409759  0.030947  -0.241246\n",
       "6       1      1  14.352028 -0.504023   7.977600\n",
       "7       1      2  30.112575  2.001504  23.091564\n",
       "8       1      3   5.797610  3.095406   4.733495\n",
       "9       1      4  20.579967  8.232626  21.696599"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parámetros del modelo\n",
    "beta_0 = -3\n",
    "beta_1 = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas y covarianzas de u\n",
    "n_observations = 5  # Tamaño de cada muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "\n",
    "# Generación de muestras\n",
    "samples = []\n",
    "np.random.seed(3649)  #Últimos 4 números de mi documento de identidad.\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1 * x + u  # Generar y\n",
    "    samples.append(pd.DataFrame({'x': x, 'u': u, 'y': y}))\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples, keys=range(n_samples), names=['sample', 'index']).reset_index()\n",
    "\n",
    "# Mostrar algunas filas\n",
    "all_samples.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma_hat: 1.0190342558308039\n",
      "beta1 (coeficiente de x): 1.049125141002301\n",
      "beta0 (intercepto): 1.1894337904253731\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "\n",
    "x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "x2 = x**2\n",
    "u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "# Creamos un DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'x2': x2,\n",
    "    'u': u,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "# 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "# 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "df['u_hat'] = ols_results.resid\n",
    "df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "# 3. Dada la forma funcional de la heterocedasticidad de White para este modelo, sigma2 ~ x x2, se estima por OLS usando u_hat2 como proxy de sigma2\n",
    "ols_model_aux = sm.OLS(df['u_hat2'], df[['x', 'x2']]) #Ver de agregar intercepto.\n",
    "ols_model_aux_results = ols_model_aux.fit()\n",
    "gamma_hat1 = ols_model_aux_results.params['x']\n",
    "gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "# 4. Usar las estimaciones de la regresión auxiliar y obtener las varianzas ajustadas (no consistentes)\n",
    "df['sigma2_hat'] = ols_model_aux_results.predict(df[['x', 'x2']])\n",
    "\n",
    "# 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por la raíz de sigma2_hat y estimar por OLS\n",
    "df['u_hat2_over_sigma2_hat'] = df['u_hat2'] / df['sigma2_hat']\n",
    "df['x_over_sigma2_hat'] = df['x'] / df['sigma2_hat']\n",
    "df['x2_over_sigma2_hat'] = df['x2'] / df['sigma2_hat']\n",
    "\n",
    "ols_model_aux2 = sm.OLS(df['u_hat2_over_sigma2_hat'], df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']]) #Ver de agregar intercepto.\n",
    "ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "# 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "df['sigma2_tilde'] = ols_model_aux_results2.predict(df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']])\n",
    "df['sigma2_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "# 7. Usar uno sobre sigma tilde como ponderador en la regresión de y sobre x\n",
    "df['y_estrella'] = df['y'] / df['sigma_tilde']\n",
    "df['x_estrella'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "X_estrella = sm.add_constant(df['x_over_sigma_tilde']) \n",
    "final_ols_model = sm.OLS(df['y_estrella'], X_estrella)\n",
    "final_ols_results = final_ols_model.fit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated beta_0: 19.1022, beta_1: 0.1914\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "#n_samples = 5000\n",
    "n_observations = 5\n",
    "\n",
    "x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "x2 = x**2\n",
    "u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "# Creamos un DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x': x,\n",
    "    'x2': x2,\n",
    "    'u': u,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "# 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo\n",
    "X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "\n",
    "# 2. Calcular los residuos del modelo y elevarlos al cuadrado\n",
    "df['u_hat'] = ols_results.resid\n",
    "df['u_hat2'] = df['u_hat']**2\n",
    "\n",
    "# 3. Regresión auxiliar: u_hat2 ~ x + x2 (incluyendo intercepto)\n",
    "aux_X = sm.add_constant(df[['x', 'x2']])\n",
    "ols_model_aux = sm.OLS(df['u_hat2'], aux_X)\n",
    "ols_model_aux_results = ols_model_aux.fit()\n",
    "gamma_hat1 = ols_model_aux_results.params['x']\n",
    "gamma_hat2 = ols_model_aux_results.params['x2']\n",
    "\n",
    "# 4. Usar las estimaciones para obtener las varianzas ajustadas\n",
    "df['sigma2_hat'] = ols_model_aux_results.predict(aux_X)\n",
    "\n",
    "# 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por la raíz de sigma2_hat y estimar por OLS\n",
    "df['u_hat2_over_sigma2_hat'] = df['u_hat2'] / df['sigma2_hat']\n",
    "df['x_over_sigma2_hat'] = df['x'] / df['sigma2_hat']\n",
    "df['x2_over_sigma2_hat'] = df['x2'] / df['sigma2_hat']\n",
    "\n",
    "aux_X_2 = sm.add_constant(df[['x_over_sigma2_hat', 'x2_over_sigma2_hat']])\n",
    "ols_model_aux2 = sm.OLS(df['u_hat2_over_sigma2_hat'], aux_X_2)\n",
    "ols_model_aux_results2 = ols_model_aux2.fit()\n",
    "gamma_tilde1 = ols_model_aux_results2.params['x_over_sigma2_hat']\n",
    "gamma_tilde2 = ols_model_aux_results2.params['x2_over_sigma2_hat']\n",
    "\n",
    "# 6. Usar las estimaciones de la regresión auxiliar 2 y obtener las varianzas ajustadas (consistentes)\n",
    "df['sigma2_tilde'] = ols_model_aux_results2.predict(aux_X_2)\n",
    "df['sigma_tilde'] = np.sqrt(df['sigma2_tilde'])\n",
    "\n",
    "# 7. Usar uno sobre sigma tilde como ponderador en la regresión de y sobre x\n",
    "df['y_star'] = df['y'] / df['sigma_tilde']\n",
    "df['x_star'] = df['x'] / df['sigma_tilde']\n",
    "\n",
    "aux_X_star = sm.add_constant(df['x_star']) \n",
    "final_ols_model = sm.OLS(df['y_star'], aux_X_star)\n",
    "final_ols_results = final_ols_model.fit() \n",
    "\n",
    "# Estimaciones finales\n",
    "beta_0_hat = final_ols_results.params['const']\n",
    "beta_1_hat = final_ols_results.params['x_star']\n",
    "print(f\"Estimated beta_0: {beta_0_hat:.4f}, beta_1: {beta_1_hat:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de all_samples:\n",
      "   sample          x          y         u\n",
      "0       0  44.691948  33.147926  0.394367\n",
      "1       0  33.945501  21.417052 -2.739349\n",
      "2       0  37.364044  21.773406 -5.117829\n",
      "3       0  37.251050  28.303198  1.502358\n",
      "4       0  34.525174  30.658162  6.038023\n",
      "\n",
      "Ejemplo de results_df:\n",
      "   sample  beta_0_hat  beta_1_hat  se_beta1_hat\n",
      "0       0   19.102219    0.191446      1.192612\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>beta_0_hat</th>\n",
       "      <th>beta_1_hat</th>\n",
       "      <th>se_beta1_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19.102219</td>\n",
       "      <td>0.191446</td>\n",
       "      <td>1.192612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample  beta_0_hat  beta_1_hat  se_beta1_hat\n",
       "0       0   19.102219    0.191446      1.192612"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7429979282613803"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['beta_1_hat'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimación por MCC (beta_0, beta_1):\n",
      "[-0.10411783 -0.04700603  0.00813422  0.03736534  0.06757768]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definimos parámetros\n",
    "beta_0 = -3\n",
    "beta_1 = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "N = 5  # Observaciones por muestra\n",
    "M = 5000  # Cantidad de muestras\n",
    "\n",
    "# Generación de datos\n",
    "np.random.seed(3649)\n",
    "x = np.random.uniform(1, 50, size=(M, N))  # x ~ U[1, 50]\n",
    "chol_omega = np.linalg.cholesky(omega)     # P = Cholesky de omega\n",
    "u = chol_omega @ np.random.randn(N, M)    # u ~ N(0, omega)\n",
    "u = u.T  # Transpuesta para mantener dimensiones MxN\n",
    "y = beta_0 + beta_1 * x + u               # y_i = beta_0 + beta_1 * x_i + u_i\n",
    "\n",
    "# Transformación para MCC\n",
    "P_inv = np.linalg.inv(chol_omega)         # Inversa de P\n",
    "y_star = y @ P_inv.T                      # y* = P^-1 * y\n",
    "x_star = x @ P_inv.T                      # X* = P^-1 * X\n",
    "x_star = np.hstack([np.ones((M, 1)), x_star])  # Agregar constante a X*\n",
    "\n",
    "# Estimación MCC\n",
    "beta_mcc = np.linalg.inv(x_star.T @ x_star) @ x_star.T @ y_star\n",
    "\n",
    "# Resultados\n",
    "print(\"Estimación por MCC (beta_0, beta_1):\")\n",
    "print(beta_mcc.mean(axis=0))  # Promedio de betas estimadas en todas las muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.41230844, -1.08667462, -0.74686156, -0.56526369, -0.38246838])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_mcc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44.69194777, 33.9455005 , 37.36404406, 37.25104984, 34.52517416])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack([np.ones((M, 1)), x]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  0,  0,  0,  0],\n",
       "       [ 0,  9,  0,  0,  0],\n",
       "       [ 0,  0, 16,  0,  0],\n",
       "       [ 0,  0,  0, 25,  0],\n",
       "       [ 0,  0,  0,  0, 36]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0., 0., 0.],\n",
       "       [0., 3., 0., 0., 0.],\n",
       "       [0., 0., 4., 0., 0.],\n",
       "       [0., 0., 0., 5., 0.],\n",
       "       [0., 0., 0., 0., 6.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(omega, chol_omega)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Matriz omega (varianzas)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "\n",
    "# Descomposición de Cholesky de omega\n",
    "P = np.linalg.cholesky(omega)\n",
    "\n",
    "# Inversa de P\n",
    "P_inv = np.linalg.inv(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.33333333, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.25      , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.2       , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.16666667]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'beta_0_hat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     beta_hat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(X_tilde\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m X_tilde) \u001b[38;5;241m@\u001b[39m (X_tilde\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m y_tilde)\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Almacenar resultados\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend([i, \u001b[43mbeta_0_hat\u001b[49m, beta_1_hat, se_beta1_hat])\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Combinar todas las muestras en un único DataFrame\u001b[39;00m\n\u001b[0;32m     48\u001b[0m all_samples \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(samples)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'beta_0_hat' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "n_observations = 5  # Observaciones por muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "# Matriz omega (varianzas)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "\n",
    "# Descomposición de Cholesky de omega\n",
    "P = np.linalg.cholesky(omega)\n",
    "\n",
    "# Inversa de P\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "# Contenedores para almacenar muestras y estimadores\n",
    "samples = []\n",
    "results = []\n",
    "\n",
    "# Simulación de M muestras\n",
    "for sample_id in range(n_samples):\n",
    "    # 1. Generar datos\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "    # Almacenar muestra\n",
    "    samples.append(pd.DataFrame({'sample': i, 'x': x, 'y': y, 'u': u}))\n",
    "\n",
    "    # 2. Transformar para MCG\n",
    "    y_tilde = P_inv @ y\n",
    "    X = np.column_stack((np.ones(n_observations), x))  # Matriz de diseño original\n",
    "    X_tilde = P_inv @ X\n",
    "\n",
    "    # 3. Ajustar modelo transformado con MCO\n",
    "    beta_hat = np.linalg.inv(X_tilde.T @ X_tilde) @ (X_tilde.T @ y_tilde)\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append([i, beta_0_hat, beta_1_hat, se_beta1_hat])\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "# Crear DataFrame con los resultados de FGLS\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    'sample', 'beta_0_hat', 'beta_1_hat', 'se_beta1_hat'\n",
    "])\n",
    "\n",
    "# Mostrar ejemplos de los DataFrames generados\n",
    "print(\"Ejemplo de all_samples:\")\n",
    "print(all_samples.head())\n",
    "print(\"\\nEjemplo de results_df:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo de all_samples:\n",
      "   sample          x          y         u\n",
      "0       0  44.691948  33.147926  0.394367\n",
      "1       0  33.945501  21.417052 -2.739349\n",
      "2       0  37.364044  21.773406 -5.117829\n",
      "3       0  37.251050  28.303198  1.502358\n",
      "4       0  34.525174  30.658162  6.038023\n",
      "\n",
      "Ejemplo de results_df:\n",
      "   sample  beta_0_hat  beta_1_hat  se_beta1_hat\n",
      "0       0  -11.213024    0.989465      0.301298\n",
      "1       1   -3.104349    0.893636      0.119634\n",
      "2       2    1.983310    0.630313      0.192682\n",
      "3       3   -1.290987    0.670113      0.142292\n",
      "4       4   -0.085546    0.737440      0.122292\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Configuración inicial\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0_true = -3\n",
    "beta_1_true = 0.8\n",
    "n_observations = 5  # Observaciones por muestra\n",
    "n_samples = 5000  # Número de muestras\n",
    "\n",
    "# Matriz omega (varianzas)\n",
    "omega = np.diag([4, 9, 16, 25, 36])\n",
    "\n",
    "# Descomposición de Cholesky de omega\n",
    "P = np.linalg.cholesky(omega)\n",
    "\n",
    "# Inversa de P\n",
    "P_inv = np.linalg.inv(P)\n",
    "\n",
    "# Contenedores para almacenar muestras y estimadores\n",
    "samples = []\n",
    "results = []\n",
    "\n",
    "# Simulación de M muestras\n",
    "for i in range(n_samples):\n",
    "    # 1. Generar datos\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0_true + beta_1_true * x + u  # Generar y\n",
    "\n",
    "    # Almacenar muestra\n",
    "    samples.append(pd.DataFrame({'sample': i, 'x': x, 'y': y, 'u': u}))\n",
    "\n",
    "    # 2. Transformar para MCG\n",
    "    y_tilde = P_inv @ y\n",
    "    X = sm.add_constant(x)  # Agregar columna de unos automáticamente\n",
    "    X_tilde = P_inv @ X\n",
    "\n",
    "    # 3. Ajustar modelo transformado con statsmodels\n",
    "    model = sm.OLS(y_tilde, X_tilde)\n",
    "    results_model = model.fit()\n",
    "\n",
    "    # Extraer coeficientes y error estándar\n",
    "    beta_0_hat, beta_1_hat = results_model.params\n",
    "    se_beta1_hat = results_model.bse[1]  # Error estándar del coeficiente beta_1\n",
    "\n",
    "    # Almacenar resultados\n",
    "    results.append([i, beta_0_hat, beta_1_hat, se_beta1_hat])\n",
    "\n",
    "# Combinar todas las muestras en un único DataFrame\n",
    "all_samples = pd.concat(samples).reset_index(drop=True)\n",
    "\n",
    "# Crear DataFrame con los resultados de FGLS\n",
    "results_df = pd.DataFrame(results, columns=[\n",
    "    'sample', 'beta_0_hat', 'beta_1_hat', 'se_beta1_hat'\n",
    "])\n",
    "\n",
    "# Mostrar ejemplos de los DataFrames generados\n",
    "print(\"Ejemplo de all_samples:\")\n",
    "print(all_samples.head())\n",
    "print(\"\\nEjemplo de results_df:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7964059833013949"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['beta_1_hat'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(3639)\n",
    "n_observations = 20\n",
    "\n",
    "# Generar x1 y x2\n",
    "x1_core = np.linspace(-1, 1, 18)  # 18 puntos entre -1 y 1\n",
    "x1 = np.concatenate([[-1.1], x1_core, [1.1]])  # Agregar extremos -1.1 y 1.1\n",
    "x2 = np.random.normal(0, 1, n_observations)\n",
    "X = sm.add_constant(np.column_stack((x1, x2)))\n",
    "\n",
    "# Coeficientes\n",
    "beta0, beta1, beta2 = 1, 1, 1\n",
    "\n",
    "#Errores\n",
    "un = np.random.normal(0, 1, n_observations) #un: u normal.\n",
    "v1 = np.ones(n_observations) #v1: v = 1\n",
    "ut = np.random.standard_t(5, n_observations) #ut: u t-student.\n",
    "v2 = np.exp(0.25 * x1 + 0.25 * x2) #v2: v = exp(0.25*x1 + 0.25*x2)\n",
    "\n",
    "#Diseños\n",
    "# Diseño 0: normalidad y homocedasticidad\n",
    "y0 = beta0 + beta1 * x1 + beta2 * x2 + np.sqrt(v1) * un\n",
    "# Diseño 1: normalidad y heterocedasticidad\n",
    "y1 = beta0 + beta1 * x1 + beta2 * x2 + np.sqrt(v2) * un\n",
    "# Diseño 2: t-student y heterocedasticidad\n",
    "y3 = beta0 + beta1 * x1 + beta2 * x2 + np.sqrt(v2) * ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test al 1%: 0.0024\n",
      "Tamaño del test al 5%: 0.0358\n",
      "Tamaño del test al 10%: 0.0848\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "np.random.seed(3639)\n",
    "n_observations = 20\n",
    "n_simulations = 5000\n",
    "\n",
    "# Generar x1 y x2\n",
    "x1_core = np.linspace(-1, 1, 18)  # 18 puntos entre -1 y 1\n",
    "x1 = np.concatenate([[-1.1], x1_core, [1.1]])  # Agregar extremos -1.1 y 1.1\n",
    "x2 = np.random.normal(0, 1, n_observations)\n",
    "X = sm.add_constant(np.column_stack((x1, x2)))\n",
    "\n",
    "# Coeficientes\n",
    "beta0, beta1, beta2 = 1, 1, 1\n",
    "\n",
    "# Test de White\n",
    "significance_levels = [0.01, 0.05, 0.10]\n",
    "rejection_counts = {alpha: 0 for alpha in significance_levels}\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    # Errores\n",
    "    un = np.random.normal(0, 1, n_observations)\n",
    "    v1 = np.ones(n_observations)\n",
    "    \n",
    "    # Diseño 0: normalidad y homocedasticidad\n",
    "    y0 = beta0 + beta1 * x1 + beta2 * x2 + np.sqrt(v1) * un\n",
    "    \n",
    "    # Estimación por MCO\n",
    "    modelo = sm.OLS(y0, X).fit()\n",
    "    \n",
    "    # Test de White\n",
    "    white_test = het_white(modelo.resid, X)\n",
    "    p_value = white_test[1]  # p-valor de la estadística LM\n",
    "    \n",
    "    # Contar rechazos de H0 para cada nivel de significancia\n",
    "    for alpha in significance_levels:\n",
    "        if p_value < alpha:\n",
    "            rejection_counts[alpha] += 1\n",
    "\n",
    "# Reportar tamaños del test\n",
    "for alpha, count in rejection_counts.items():\n",
    "    print(f\"Tamaño del test al {int(alpha * 100)}%: {count / n_simulations:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poder del test de White usando errores poblacionales:\n",
      "Diseño 1 (Normalidad y heterocedasticidad): {0.01: 0.0142, 0.05: 0.0894, 0.1: 0.165}\n",
      "Diseño 2 (No-normalidad y heterocedasticidad): {0.01: 0.0194, 0.05: 0.0888, 0.1: 0.148}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "np.random.seed(3639)\n",
    "n_simulaciones = 5000\n",
    "n = 20\n",
    "\n",
    "# Niveles de significancia\n",
    "alpha_levels = [0.01, 0.05, 0.10]\n",
    "\n",
    "# Contadores de rechazos\n",
    "rechazos_diseño1 = {alpha: 0 for alpha in alpha_levels}\n",
    "rechazos_diseño2 = {alpha: 0 for alpha in alpha_levels}\n",
    "\n",
    "# Simulaciones\n",
    "for _ in range(n_simulaciones):\n",
    "    # Generar x1 y x2\n",
    "    x1_core = np.linspace(-1, 1, 18)  # 18 puntos entre -1 y 1\n",
    "    x1 = np.concatenate([[-1.1], x1_core, [1.1]])  # Agregar extremos -1.1 y 1.1\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "\n",
    "    # Generar errores poblacionales\n",
    "    un = np.random.normal(0, 1, n)  # Normal(0,1)\n",
    "    ut = np.random.standard_t(5, n)  # t-student con 5 grados de libertad\n",
    "    v2 = np.exp(0.25 * x1 + 0.25 * x2)  # Heterocedasticidad\n",
    "\n",
    "    # Diseños:\n",
    "    u_diseño1 = np.sqrt(v2) * un  # Normalidad y heterocedasticidad\n",
    "    u_diseño2 = np.sqrt(v2) * ut  # No-normalidad y heterocedasticidad\n",
    "\n",
    "    # Aplicar test de White sobre los errores poblacionales\n",
    "    X_aux = sm.add_constant(np.column_stack((x1, x2)))\n",
    "    \n",
    "    # Diseño 1\n",
    "    white_test_1 = het_white(u_diseño1, X_aux)\n",
    "    p_value_1 = white_test_1[1]  # p-valor del test de White\n",
    "\n",
    "    # Diseño 2\n",
    "    white_test_2 = het_white(u_diseño2, X_aux)\n",
    "    p_value_2 = white_test_2[1]  # p-valor del test de White\n",
    "\n",
    "    # Contar rechazos para cada nivel de significancia\n",
    "    for alpha in alpha_levels:\n",
    "        if p_value_1 < alpha:\n",
    "            rechazos_diseño1[alpha] += 1\n",
    "        if p_value_2 < alpha:\n",
    "            rechazos_diseño2[alpha] += 1\n",
    "\n",
    "# Calcular tasas de rechazo (poder del test)\n",
    "poder_diseño1 = {alpha: rechazos_diseño1[alpha] / n_simulaciones for alpha in alpha_levels}\n",
    "poder_diseño2 = {alpha: rechazos_diseño2[alpha] / n_simulaciones for alpha in alpha_levels}\n",
    "\n",
    "# Resultados\n",
    "print(\"Poder del test de White usando errores poblacionales:\")\n",
    "print(f\"Diseño 1 (Normalidad y heterocedasticidad): {poder_diseño1}\")\n",
    "print(f\"Diseño 2 (No-normalidad y heterocedasticidad): {poder_diseño2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poder del test de White usando errores poblacionales:\n",
      "Diseño 1 (Normalidad y heterocedasticidad): {0.01: 0.0142, 0.05: 0.0894, 0.1: 0.165}\n",
      "Diseño 2 (No-normalidad y heterocedasticidad): {0.01: 0.0194, 0.05: 0.0888, 0.1: 0.148}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_white\n",
    "\n",
    "np.random.seed(3639)\n",
    "n_simulaciones = 5000\n",
    "n = 20\n",
    "\n",
    "# Niveles de significancia\n",
    "alpha_levels = [0.01, 0.05, 0.10]\n",
    "\n",
    "# Contadores de rechazos\n",
    "rechazos_diseño1 = {alpha: 0 for alpha in alpha_levels}\n",
    "rechazos_diseño2 = {alpha: 0 for alpha in alpha_levels}\n",
    "\n",
    "# Simulaciones\n",
    "for _ in range(n_simulaciones):\n",
    "    # Generar x1 y x2\n",
    "    x1_core = np.linspace(-1, 1, n-2)  # 18 puntos entre -1 y 1\n",
    "    x1 = np.concatenate([[-1.1], x1_core, [1.1]])  # Agregar extremos -1.1 y 1.1\n",
    "    x2 = np.random.normal(0, 1, n)\n",
    "\n",
    "    # Generar errores poblacionales\n",
    "    un = np.random.normal(0, 1, n)  # Normal(0,1)\n",
    "    ut = np.random.standard_t(5, n)  # t-student con 5 grados de libertad\n",
    "    v2 = np.exp(0.25 * x1 + 0.25 * x2)  # Heterocedasticidad\n",
    "\n",
    "    # Diseños:\n",
    "    u_diseño1 = np.sqrt(v2) * un  # Normalidad y heterocedasticidad\n",
    "    u_diseño2 = np.sqrt(v2) * ut  # No-normalidad y heterocedasticidad\n",
    "\n",
    "    # Aplicar test de White sobre los errores poblacionales\n",
    "    X_aux = sm.add_constant(np.column_stack((x1, x2)))\n",
    "    \n",
    "    # Diseño 1\n",
    "    white_test_1 = het_white(u_diseño1, X_aux)\n",
    "    p_value_1 = white_test_1[1]  # p-valor del test de White\n",
    "\n",
    "    # Diseño 2\n",
    "    white_test_2 = het_white(u_diseño2, X_aux)\n",
    "    p_value_2 = white_test_2[1]  # p-valor del test de White\n",
    "\n",
    "    # Contar rechazos para cada nivel de significancia\n",
    "    for alpha in alpha_levels:\n",
    "        if p_value_1 < alpha:\n",
    "            rechazos_diseño1[alpha] += 1\n",
    "        if p_value_2 < alpha:\n",
    "            rechazos_diseño2[alpha] += 1\n",
    "\n",
    "# Calcular tasas de rechazo (poder del test)\n",
    "poder_diseño1 = {alpha: rechazos_diseño1[alpha] / n_simulaciones for alpha in alpha_levels}\n",
    "poder_diseño2 = {alpha: rechazos_diseño2[alpha] / n_simulaciones for alpha in alpha_levels}\n",
    "\n",
    "# Resultados\n",
    "print(\"Poder del test de White usando errores poblacionales:\")\n",
    "print(f\"Diseño 1 (Normalidad y heterocedasticidad): {poder_diseño1}\")\n",
    "print(f\"Diseño 2 (No-normalidad y heterocedasticidad): {poder_diseño2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sesgos relativos estimados (White):\n",
      "B0: -0.10264479591532095, B1: 0.07860651184160447, B2: -0.3352494413374326\n",
      "\n",
      "Sesgos relativos estimados (White con residuos originales):\n",
      "B0: -0.19168155137200596, B1: 0.02866193652607788, B2: -0.5327384396207882\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "# Fijar semilla para reproducibilidad\n",
    "np.random.seed(144)\n",
    "\n",
    "# Cantidad de observaciones\n",
    "n_obs = 20\n",
    "\n",
    "# Generar las variables\n",
    "x1 = np.linspace(-1.1, 0.9, n_obs)\n",
    "x2 = np.random.normal(size=n_obs)\n",
    "x0 = np.ones(n_obs)\n",
    "\n",
    "# Crear DataFrame\n",
    "data = pd.DataFrame({\"x0\": x0, \"x1\": x1, \"x2\": x2})\n",
    "\n",
    "# Generar la matriz X y X_2\n",
    "X = data[[\"x0\", \"x1\", \"x2\"]].values\n",
    "X_2 = data[[\"x1\", \"x2\", \"x0\"]].values\n",
    "\n",
    "# Generar las variables v, u y y\n",
    "v = np.exp(0.25 * x1 + 0.25 * x2)\n",
    "u = np.random.normal(0, 1, size=n_obs)\n",
    "y = 1 + x1 + x2 + np.sqrt(v) * u\n",
    "\n",
    "data[\"v\"] = v\n",
    "data[\"u\"] = u\n",
    "data[\"y\"] = y\n",
    "\n",
    "# Regresión: Incluimos explícitamente la constante\n",
    "model = OLS(y, X)  # Usamos X completo, incluyendo x0\n",
    "results = model.fit()\n",
    "\n",
    "# Obtener los parámetros estimados (B)\n",
    "B = results.params  # Esto ahora tendrá 3 elementos (x0, x1, x2)\n",
    "\n",
    "# Generar la matriz de omega de White\n",
    "Y = y.reshape(-1, 1)\n",
    "uhat = Y - X_2 @ B.reshape(-1, 1)  # Ahora X_2 y B tienen dimensiones compatibles\n",
    "\n",
    "# Continuar el cálculo como estaba\n",
    "data[\"uhat\"] = uhat.flatten()\n",
    "data[\"uhat_2\"] = data[\"uhat\"] ** 2\n",
    "uhat_2 = np.diag(data[\"uhat_2\"].values)\n",
    "\n",
    "omega_hat = uhat_2\n",
    "\n",
    "# Matriz de omega de White con residuos originales\n",
    "u_ori = (np.sqrt(v) * u) ** 2\n",
    "data[\"u_ori\"] = u_ori\n",
    "omega_wori = np.diag(u_ori)\n",
    "\n",
    "# Matrices de varianzas y covarianzas de los betas\n",
    "X_transpose_X_inv = np.linalg.inv(X.T @ X)\n",
    "\n",
    "Sigma = X_transpose_X_inv @ (X.T @ omega @ X) @ X_transpose_X_inv\n",
    "Sigma_hat = X_transpose_X_inv @ (X.T @ omega_hat @ X) @ X_transpose_X_inv\n",
    "Sigma_wori = X_transpose_X_inv @ (X.T @ omega_wori @ X) @ X_transpose_X_inv\n",
    "\n",
    "# Sesgos relativos\n",
    "bias_B0 = (Sigma_hat[0, 0] - Sigma[0, 0]) / Sigma[0, 0]\n",
    "bias_B1 = (Sigma_hat[1, 1] - Sigma[1, 1]) / Sigma[1, 1]\n",
    "bias_B2 = (Sigma_hat[2, 2] - Sigma[2, 2]) / Sigma[2, 2]\n",
    "\n",
    "# Sesgos relativos con White con residuos originales\n",
    "bias_ori_B0 = (Sigma_wori[0, 0] - Sigma[0, 0]) / Sigma[0, 0]\n",
    "bias_ori_B1 = (Sigma_wori[1, 1] - Sigma[1, 1]) / Sigma[1, 1]\n",
    "bias_ori_B2 = (Sigma_wori[2, 2] - Sigma[2, 2]) / Sigma[2, 2]\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Sesgos relativos estimados (White):\")\n",
    "print(f\"B0: {bias_B0}, B1: {bias_B1}, B2: {bias_B2}\")\n",
    "\n",
    "print(\"\\nSesgos relativos estimados (White con residuos originales):\")\n",
    "print(f\"B0: {bias_ori_B0}, B1: {bias_ori_B1}, B2: {bias_ori_B2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test: {0.01: 0.0, 0.05: 0.0012}\n",
      "Poder del test: {0: 0.9014, 0.4: 0.452}\n",
      "Estadísticas descriptivas:\n",
      "           beta_0    beta_1\n",
      "mean   -2.958325  0.798089\n",
      "median -2.890889  0.796406\n",
      "std     4.649156  0.162026\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para estimar por FGLS\n",
    "def fgls_estimation(x, y, omega):\n",
    "    W = np.linalg.inv(omega)  # Matriz de pesos inversos\n",
    "    X = np.column_stack((np.ones(len(x)), x))  # Matriz de diseño\n",
    "    beta_hat = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ y)  # Estimación FGLS\n",
    "    var_beta = np.linalg.inv(X.T @ W @ X)  # Varianza de los estimadores\n",
    "    return beta_hat, np.sqrt(np.diag(var_beta))  # Estimadores y errores estándar\n",
    "\n",
    "# Simulaciones\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    beta_hat, se_beta = fgls_estimation(x, y, omega)  # Estimación FGLS\n",
    "    t_stat = (beta_hat[1] - beta_1_true) / se_beta[1]  # Estadístico t para beta_1\n",
    "    p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_observations - 2))  # Valor p\n",
    "    results.append([beta_hat[0], beta_hat[1], se_beta[0], se_beta[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        beta_hat, se_beta = fgls_estimation(x, y, omega)\n",
    "        t_stat = (beta_hat[1] - beta_1_true) / se_beta[1]\n",
    "        p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_observations - 2))\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test: {0.01: 0.6018, 0.05: 0.8908}\n",
      "Poder del test: {0: 0.0532, 0.4: 0.5276}\n",
      "Estadísticas descriptivas:\n",
      "           beta_0    beta_1\n",
      "mean   -2.958325  0.798089\n",
      "median -2.890889  0.796406\n",
      "std     4.649156  0.162026\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para estimar por FGLS usando statsmodels\n",
    "def fgls_estimation(x, y, omega):\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    model = sm.GLS(y, X, sigma=omega)  # Estimación FGLS con sigma = omega\n",
    "    results = model.fit()  # Ajustar el modelo\n",
    "    return results\n",
    "\n",
    "# Simulaciones\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    result = fgls_estimation(x, y, omega)  # Estimación FGLS\n",
    "    t_stat = result.tvalues[1]  # Estadístico t para beta_1\n",
    "    p_value = result.pvalues[1]  # Valor p para beta_1\n",
    "    results.append([result.params[0], result.params[1], result.bse[0], result.bse[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        result = fgls_estimation(x, y, omega)\n",
    "        t_stat = result.tvalues[1]\n",
    "        p_value = result.pvalues[1]\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta_0_sm</th>\n",
       "      <th>beta_1_sm</th>\n",
       "      <th>se_beta_0_sm</th>\n",
       "      <th>se_beta_1_sm</th>\n",
       "      <th>beta_0_manual</th>\n",
       "      <th>beta_1_manual</th>\n",
       "      <th>se_beta_0_manual</th>\n",
       "      <th>se_beta_1_manual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-3.124395</td>\n",
       "      <td>0.803626</td>\n",
       "      <td>3.777375</td>\n",
       "      <td>0.136698</td>\n",
       "      <td>-3.124395</td>\n",
       "      <td>0.803626</td>\n",
       "      <td>3.777375</td>\n",
       "      <td>0.136698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>-3.050941</td>\n",
       "      <td>0.804804</td>\n",
       "      <td>3.135215</td>\n",
       "      <td>0.118106</td>\n",
       "      <td>-3.050941</td>\n",
       "      <td>0.804804</td>\n",
       "      <td>3.135215</td>\n",
       "      <td>0.118106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.543793</td>\n",
       "      <td>0.162838</td>\n",
       "      <td>2.673090</td>\n",
       "      <td>0.088475</td>\n",
       "      <td>4.543793</td>\n",
       "      <td>0.162838</td>\n",
       "      <td>2.673090</td>\n",
       "      <td>0.088475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        beta_0_sm  beta_1_sm  se_beta_0_sm  se_beta_1_sm  beta_0_manual  \\\n",
       "mean    -3.124395   0.803626      3.777375      0.136698      -3.124395   \n",
       "median  -3.050941   0.804804      3.135215      0.118106      -3.050941   \n",
       "std      4.543793   0.162838      2.673090      0.088475       4.543793   \n",
       "\n",
       "        beta_1_manual  se_beta_0_manual  se_beta_1_manual  \n",
       "mean         0.803626          3.777375          0.136698  \n",
       "median       0.804804          3.135215          0.118106  \n",
       "std          0.162838          2.673090          0.088475  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "\n",
    "# Parámetros\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_observations = 5  # Pequeña muestra\n",
    "n_samples = 5000  # Número de simulaciones\n",
    "\n",
    "# Generar datos\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_data():\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    return x, y\n",
    "\n",
    "# Método 1: Estimación con FGLS usando statsmodels\n",
    "def fgls_statsmodels(x, y):\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    model = sm.GLS(y, X, sigma=omega)  # Estimación FGLS con sigma = omega\n",
    "    results = model.fit()  # Ajustar el modelo\n",
    "    return results\n",
    "\n",
    "# Método 2: Estimación manual FGLS\n",
    "def fgls_manual(x, y, omega):\n",
    "    X = np.column_stack([np.ones(n_observations), x])  # Matriz de diseño con intercepto\n",
    "    # Calcular la inversa de Omega\n",
    "    omega_inv = np.linalg.inv(omega)\n",
    "    # Estimar Beta usando la fórmula FGLS\n",
    "    beta_hat = np.linalg.inv(X.T @ omega_inv @ X) @ (X.T @ omega_inv @ y)\n",
    "    # Estimación de errores estándar (desviación estándar)\n",
    "    residuals = y - X @ beta_hat\n",
    "    sigma_hat = (residuals.T @ omega_inv @ residuals) / (n_observations - 2)  # Varianza de los errores\n",
    "    cov_beta_hat = np.linalg.inv(X.T @ omega_inv @ X) * sigma_hat  # Varianza de los coeficientes\n",
    "    se_beta_hat = np.sqrt(np.diag(cov_beta_hat))  # Desviación estándar de los coeficientes\n",
    "    return beta_hat, se_beta_hat\n",
    "\n",
    "# Comparación de ambos métodos\n",
    "results_fgls_sm = []\n",
    "results_fgls_manual = []\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x, y = generate_data()\n",
    "    \n",
    "    # Método 1: FGLS con statsmodels\n",
    "    result_sm = fgls_statsmodels(x, y)\n",
    "    results_fgls_sm.append([result_sm.params[0], result_sm.params[1], result_sm.bse[0], result_sm.bse[1]])\n",
    "    \n",
    "    # Método 2: Estimación manual\n",
    "    beta_hat_manual, se_beta_hat_manual = fgls_manual(x, y, omega)\n",
    "    results_fgls_manual.append([beta_hat_manual[0], beta_hat_manual[1], se_beta_hat_manual[0], se_beta_hat_manual[1]])\n",
    "\n",
    "# Convertir resultados a DataFrame para comparar\n",
    "results_fgls_sm_df = pd.DataFrame(results_fgls_sm, columns=['beta_0_sm', 'beta_1_sm', 'se_beta_0_sm', 'se_beta_1_sm'])\n",
    "results_fgls_manual_df = pd.DataFrame(results_fgls_manual, columns=['beta_0_manual', 'beta_1_manual', 'se_beta_0_manual', 'se_beta_1_manual'])\n",
    "\n",
    "# Comparar medias, medianas y desviaciones estándar\n",
    "comparison = pd.concat([\n",
    "    results_fgls_sm_df[['beta_0_sm', 'beta_1_sm', 'se_beta_0_sm', 'se_beta_1_sm']].agg(['mean', 'median', 'std']),\n",
    "    results_fgls_manual_df[['beta_0_manual', 'beta_1_manual', 'se_beta_0_manual', 'se_beta_1_manual']].agg(['mean', 'median', 'std'])\n",
    "], axis=1)\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del test: {0.01: 0.298, 0.05: 0.6176}\n",
      "Poder del test: {0: 0.0282, 0.4: 0.2734}\n",
      "Estadísticas descriptivas:\n",
      "           beta_0    beta_1\n",
      "mean   -2.958325  0.798089\n",
      "median -2.890889  0.796406\n",
      "std     4.649156  0.162026\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "\n",
    "# Parámetros iniciales\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para estimar por FGLS usando statsmodels con errores robustos\n",
    "def fgls_estimation(x, y, omega):\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    model = sm.GLS(y, X, sigma=omega)  # Estimación FGLS con sigma = omega\n",
    "    results = model.fit()  # Ajustar el modelo\n",
    "    robust_results = results.get_robustcov_results(cov_type='HC3')  # Errores estándar robustos (HC3)\n",
    "    return robust_results\n",
    "\n",
    "# Simulaciones\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    robust_results = fgls_estimation(x, y, omega)  # Estimación FGLS con errores robustos\n",
    "    t_stat = robust_results.tvalues[1]  # Estadístico t para beta_1\n",
    "    p_value = robust_results.pvalues[1]  # Valor p para beta_1\n",
    "    results.append([robust_results.params[0], robust_results.params[1], robust_results.bse[0], robust_results.bse[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        robust_results = fgls_estimation(x, y, omega)\n",
    "        t_stat = robust_results.tvalues[1]\n",
    "        p_value = robust_results.pvalues[1]\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sigma must be a scalar, 1d of length 5 or a 2d array of shape 5 x 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m u \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmultivariate_normal(mean\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mzeros(n_observations), cov\u001b[38;5;241m=\u001b[39momega)  \u001b[38;5;66;03m# u ~ N(0, omega)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m y \u001b[38;5;241m=\u001b[39m beta_0 \u001b[38;5;241m+\u001b[39m beta_1_true \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m u  \u001b[38;5;66;03m# Generar y\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m fgls_result \u001b[38;5;241m=\u001b[39m \u001b[43mfgls_white_estimation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momega\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Estimación FGLS White\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Obtener estadísticos de interés\u001b[39;00m\n\u001b[0;32m     43\u001b[0m t_stat \u001b[38;5;241m=\u001b[39m fgls_result\u001b[38;5;241m.\u001b[39mtvalues[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Estadístico t para beta_1\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m, in \u001b[0;36mfgls_white_estimation\u001b[1;34m(x, y, omega)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Paso 3: Estimación FGLS con la matriz robusta\u001b[39;00m\n\u001b[0;32m     26\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(robust_cov)  \u001b[38;5;66;03m# Matriz de pesos (inversa de la matriz de varianzas-covarianzas)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m fgls_model \u001b[38;5;241m=\u001b[39m \u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGLS\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Estimación FGLS usando la matriz robusta\u001b[39;00m\n\u001b[0;32m     28\u001b[0m fgls_results \u001b[38;5;241m=\u001b[39m fgls_model\u001b[38;5;241m.\u001b[39mfit()  \u001b[38;5;66;03m# Ajuste FGLS\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fgls_results\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:534\u001b[0m, in \u001b[0;36mGLS.__init__\u001b[1;34m(self, endog, exog, sigma, missing, hasconst, **kwargs)\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_kwargs(kwargs)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# TODO: add options igls, for iterative fgls if sigma is None\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;66;03m# TODO: default if sigma is none should be two-step GLS\u001b[39;00m\n\u001b[1;32m--> 534\u001b[0m sigma, cholsigmainv \u001b[38;5;241m=\u001b[39m \u001b[43m_get_sigma\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(endog, exog, missing\u001b[38;5;241m=\u001b[39mmissing,\n\u001b[0;32m    537\u001b[0m                           hasconst\u001b[38;5;241m=\u001b[39mhasconst, sigma\u001b[38;5;241m=\u001b[39msigma,\n\u001b[0;32m    538\u001b[0m                           cholsigmainv\u001b[38;5;241m=\u001b[39mcholsigmainv, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# store attribute names for data arrays\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\statsmodels\\regression\\linear_model.py:181\u001b[0m, in \u001b[0;36m_get_sigma\u001b[1;34m(sigma, nobs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sigma\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (nobs, nobs):\n\u001b[1;32m--> 181\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSigma must be a scalar, 1d of length \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m or a 2d \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray of shape \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m x \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (nobs, nobs, nobs))\n\u001b[0;32m    183\u001b[0m     cholsigmainv, info \u001b[38;5;241m=\u001b[39m dtrtri(cholesky(sigma, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m    184\u001b[0m                                 lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, overwrite_c\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Sigma must be a scalar, 1d of length 5 or a 2d array of shape 5 x 5"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Parámetros del modelo\n",
    "beta_0 = -3\n",
    "beta_1_true = 0.8\n",
    "omega = np.diag([4, 9, 16, 25, 36])  # Matriz de varianzas (heterocedasticidad)\n",
    "n_samples = 5000\n",
    "n_observations = 5\n",
    "alpha_levels = [0.01, 0.05]  # Niveles de significancia\n",
    "\n",
    "# Función para realizar la estimación FGLS con errores robustos de White\n",
    "def fgls_white_estimation(x, y, omega):\n",
    "    # Paso 1: Estimación OLS inicial\n",
    "    X = sm.add_constant(x)  # Matriz de diseño con intercepto\n",
    "    ols_model = sm.OLS(y, X)  # Estimación OLS\n",
    "    ols_results = ols_model.fit()  # Ajuste de OLS\n",
    "    \n",
    "    # Paso 2: Estimación de la matriz de varianzas-covarianzas de los errores\n",
    "    residuals = ols_results.resid  # Residuos de la estimación OLS\n",
    "    # Usamos la matriz de varianzas-covarianzas robusta (White)\n",
    "    robust_cov = ols_results.get_robustcov_results(cov_type='HC3').cov_params()  # Matriz robusta de varianzas-covarianzas\n",
    "    \n",
    "    # Paso 3: Estimación FGLS con la matriz robusta\n",
    "    W = np.linalg.inv(robust_cov)  # Matriz de pesos (inversa de la matriz de varianzas-covarianzas)\n",
    "    fgls_model = sm.GLS(y, X, sigma=W)  # Estimación FGLS usando la matriz robusta\n",
    "    fgls_results = fgls_model.fit()  # Ajuste FGLS\n",
    "    \n",
    "    return fgls_results\n",
    "\n",
    "# Simulación de los datos\n",
    "results = []\n",
    "np.random.seed(3649)\n",
    "\n",
    "for _ in range(n_samples):\n",
    "    x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "    u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "    y = beta_0 + beta_1_true * x + u  # Generar y\n",
    "    fgls_result = fgls_white_estimation(x, y, omega)  # Estimación FGLS White\n",
    "    \n",
    "    # Obtener estadísticos de interés\n",
    "    t_stat = fgls_result.tvalues[1]  # Estadístico t para beta_1\n",
    "    p_value = fgls_result.pvalues[1]  # Valor p para beta_1\n",
    "    results.append([fgls_result.params[0], fgls_result.params[1], fgls_result.bse[0], fgls_result.bse[1], t_stat, p_value])\n",
    "\n",
    "# Convertir resultados a DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['beta_0', 'beta_1', 'se_beta_0', 'se_beta_1', 't_stat', 'p_value'])\n",
    "\n",
    "# Tamaño del test para diferentes niveles de significancia\n",
    "size_test = {\n",
    "    alpha: (results_df['p_value'] < alpha).mean() for alpha in alpha_levels\n",
    "}\n",
    "\n",
    "# Poder del test para diferentes valores alternativos de beta_1\n",
    "power_results = {}\n",
    "for beta_1_alt in [0, 0.4]:\n",
    "    power = []\n",
    "    for _ in range(n_samples):\n",
    "        x = np.random.uniform(1, 50, n_observations)\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)\n",
    "        y = beta_0 + beta_1_alt * x + u\n",
    "        fgls_result = fgls_white_estimation(x, y, omega)\n",
    "        p_value = fgls_result.pvalues[1]\n",
    "        power.append(p_value < 0.05)\n",
    "    power_results[beta_1_alt] = np.mean(power)\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "descriptive_stats = results_df[['beta_0', 'beta_1']].agg(['mean', 'median', 'std'])\n",
    "\n",
    "# Resultados finales\n",
    "print(\"Tamaño del test:\", size_test)\n",
    "print(\"Poder del test:\", power_results)\n",
    "print(\"Estadísticas descriptivas:\\n\", descriptive_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
