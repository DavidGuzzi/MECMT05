{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **UNIVERSIDAD TORCUATO DI TELLA**\n",
    "## **MAESTRÍA EN ECONOMETRÍA**\n",
    "\n",
    "---\n",
    "\n",
    "### **TRABAJO PRÁCTICO DE ECONOMETRÍA**\n",
    "\n",
    "- **Profesor:** González-Rozada, Martín  \n",
    "- **Ayudante:** Lening, Iara  \n",
    "- **Alumno:** Guzzi, David Alexander  (Legajo n°: 24H1970, DNI: 37.703.649)  \n",
    "\n",
    "**Ciclo Lectivo:** Tercer Trimestre, 2024  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. PROPIEDADES DE MUESTRA FINITA DE FGLS (MCGE).**\n",
    "\n",
    "Considere el siguiente modelo:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_i = \\beta_0 + \\beta_1 x_i + u_i, \\quad i = 1, 2, \\dots, 5N\n",
    "\\tag{1}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $\\beta_0 = -3$\n",
    "- $\\beta_1 = 0.8$\n",
    "- $x_j \\sim U[1, 50]$\n",
    "- $u_j \\sim N(0, \\omega \\cdot I_{N \\times N})$, con la matriz $\\Omega$ definida como:\n",
    "\n",
    "$$\n",
    "\\Omega = \\begin{bmatrix}\n",
    "4 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 9 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 16 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 25 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 36\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t, chi2\n",
    "\n",
    "# Semilla para la generación de números aleatorios.\n",
    "np.random.seed(3649)\n",
    "\n",
    "# Configuración de pandas (4 decimales).\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se presentan tres funciones que se utilizarán en el apartado 1:**\n",
    "- **fgls_estimation:** realiza estimación FGLS White;\n",
    "- **simulate_fgls:** realiza simulaciones utilizando FGLS White;\n",
    "- **gls_estimation:** realiza estimación y simulaciones para GLS.\n",
    "\n",
    "**Observación:** se prescinde del uso de librerías como *statsmodels*, que incluyen implementaciones de OLS y GLS, con el fin de aplicar las ecuaciones vistas en clase, lo que aporta mayor claridad. Para operaciones matriciales, se utiliza la librería *numpy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgls_estimation(x: np.ndarray, y: np.ndarray) -> tuple[float, float, float]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Realiza una estimación de Feasible Generalized Least Squares (FGLS) \n",
    "    para un modelo lineal con errores heterocedásticos.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        Variable regresora.\n",
    "    y : np.ndarray\n",
    "        Variable de respuesta.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    tuple[float, float, float]\n",
    "        beta0_fgls : float\n",
    "            Intercepto estimado.\n",
    "        beta1_fgls : float\n",
    "            Coeficiente estimado para el regresor.\n",
    "        se_beta1_fgls : float\n",
    "            Error estándar del coeficiente estimado para el regresor.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Estimar modelo inicial y ~ x por OLS y obtener las estimaciones de los parámetros del modelo.\n",
    "    X = np.column_stack((np.ones_like(x), x)) # Se construye matriz de regresores, contemplando intercepto.\n",
    "    beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y # Una alternativa es usar np.linalg.lstsq(X, y, rcond=None)[0] (más eficiente). \n",
    "\n",
    "    # 2. Calcular los residuos del modelo, elevarlos al cuadrado y obtener el logaritmo de los mismos.\n",
    "    u_hat = y - X @ beta_ols\n",
    "    u_hat2 = u_hat ** 2  \n",
    "    log_u_hat2 = np.log(u_hat2) #Obs.: Luego de varias iteraciones, se considera que de esta forma se obtienen resultados más coherentes.\n",
    "\n",
    "    # 3. Dada la forma funcional de la heterocedasticidad de White para este modelo, sigma2 ~ x + x^2, \n",
    "    # se estima por OLS usando u_hat2 como proxy de sigma2.\n",
    "    X_aux = np.column_stack((x, x**2)) # Se construye matriz de regresores, sin contemplar intercepto.\n",
    "    gamma_hat = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ log_u_hat2\n",
    "    \n",
    "    # 4. Usar las estimaciones de la regresión auxiliar y obtener las varianzas ajustadas (no consistentes).\n",
    "    log_sigma2_hat = X_aux @ gamma_hat\n",
    "    sigma2_hat = np.exp(log_sigma2_hat)\n",
    "\n",
    "    # 5. Transformar las variables de la forma funcional de la heterocedasticidad de White dividiéndolas por sigma2_hat y estimar por OLS.\n",
    "    X_aux2 = np.column_stack((x / sigma2_hat,  x**2 / sigma2_hat)) # Se construye matriz de regresores, sin contemplar intercepto.\n",
    "    u_hat2_over_sigma2_hat = u_hat2 / sigma2_hat # Se omite el logaritmo, ya que se considera que no es necesario y así se obtienen resultados más coherentes.\n",
    "    gamma_hat2 = np.linalg.inv(X_aux2.T @ X_aux2) @ X_aux2.T @ u_hat2_over_sigma2_hat\n",
    "\n",
    "    # 6. Usar las estimaciones de la segunda regresión auxiliar y obtener las varianzas ajustadas (consistentes).\n",
    "    sigma2_tilde = np.maximum(X_aux @ gamma_hat2, 1e-6) # Se asegura no negatividad de las varianzas. \n",
    "    omega_tilde_inv = np.linalg.inv(np.diag(sigma2_tilde))\n",
    "    \n",
    "    # 7. Aplicar GLS en la regresión de y ~ x, utilizando como ponderador la matriz omega estimada.\n",
    "    # Obs.: También se podría usar uno sobre la raíz cuadrada de sigma2 tilde como ponderador en la regresión OLS de y ~ x.\n",
    "    \n",
    "    # Se obtienen las estimaciones de los parámetros.\n",
    "    beta_fgls = np.linalg.inv(X.T @ omega_tilde_inv @ X) @ X.T @ omega_tilde_inv @ y\n",
    "    \n",
    "    # Se obtienen las estimaciones de los errores estándar de los parámetros.\n",
    "    residuals_fgls = y - X @ beta_fgls\n",
    "    s2_fgls = (residuals_fgls.T @ omega_tilde_inv @ residuals_fgls) / ( X.shape[0] - X.shape[1])\n",
    "    var_beta_fgls = s2_fgls * np.linalg.inv(X.T @ omega_tilde_inv @ X)\n",
    "    se_beta1_fgls = np.sqrt(np.diag(var_beta_fgls))[1]   \n",
    "    \n",
    "    return beta_fgls[0], beta_fgls[1], se_beta1_fgls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_fgls(n_samples: int, n_observations_list: list[int], beta_0_true: float, beta_1_true: float, beta1_H0: float) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Simula la estimación de FGLS para distintos tamaños de muestra, y evalúa la validez de un test de hipótesis.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Número de simulaciones a realizar para cada tamaño de muestra.\n",
    "    n_observations_list : list of int\n",
    "        Lista con los distintos tamaños de muestra a evaluar.\n",
    "    beta_0_true : float\n",
    "        Valor verdadero del intercepto en la simulación.\n",
    "    beta_1_true : float\n",
    "        Valor verdadero del coeficiente del regresor en la simulación.\n",
    "    beta1_H0 : float\n",
    "        Valor hipotético de beta_1 bajo H0 para el test de hipótesis.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    results_df : pandas.DataFrame\n",
    "        DataFrame con estadísticas de los coeficientes estimados y tasas de rechazo del test de hipótesis.\n",
    "        Columnas:\n",
    "        - 'n_observations': Tamaño de la muestra.\n",
    "        - 'mean_beta0': Media de las estimaciones de beta_0.\n",
    "        - 'median_beta0': Mediana de las estimaciones de beta_0.\n",
    "        - 'std_beta0': Desviación estándar de las estimaciones de beta_0.\n",
    "        - 'mean_beta1': Media de las estimaciones de beta_1.\n",
    "        - 'median_beta1': Mediana de las estimaciones de beta_1.\n",
    "        - 'std_beta1': Desviación estándar de las estimaciones de beta_1.\n",
    "        - 'test_size_1pct': Proporción de rechazos de H0 al 1% de significancia.\n",
    "        - 'test_size_5pct': Proporción de rechazos de H0 al 5% de significancia.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    results = []\n",
    "    # Iteración sobre los tamaños de muestra.\n",
    "    for n_obs in n_observations_list:\n",
    "        sample_results = []\n",
    "        \n",
    "        # Iteración/simulaciones sobre cada observación de la muestra.\n",
    "        for _ in range(n_samples):\n",
    "            # Generación de datos.\n",
    "            x = np.random.uniform(1, 50, n_obs) # x ~ U[1, 50].\n",
    "            u = np.random.normal(scale=x) # u ~ N(0, omega), con varianza heterocedástica dependiendo de x.\n",
    "            y = beta_0_true + beta_1_true * x + u\n",
    "            \n",
    "            # Estimación FGLS.\n",
    "            beta_0_hat, beta_1_hat, se_beta1_hat = fgls_estimation(x, y)\n",
    "            \n",
    "            # Test de hipótesis para beta_1 = beta1_H0.\n",
    "            t_stat = (beta_1_hat - beta1_H0) / se_beta1_hat\n",
    "            p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_obs - 2))\n",
    "            reject_1pct = p_value < 0.01 # Rechazar H0 al 1% de significancia.\n",
    "            reject_5pct = p_value < 0.05 # Rechazar H0 al 5% de significancia.\n",
    "            \n",
    "            sample_results.append([beta_0_hat, beta_1_hat, reject_1pct, reject_5pct])\n",
    "        \n",
    "        # Convertir a DataFrame.\n",
    "        df_results = pd.DataFrame(sample_results, columns=['beta_0_hat', 'beta_1_hat', 'reject_1pct', 'reject_5pct'])\n",
    "        \n",
    "        # Calcular estadísticas.\n",
    "        mean_beta0 = df_results['beta_0_hat'].mean()\n",
    "        mean_beta1 = df_results['beta_1_hat'].mean()\n",
    "        median_beta0 = df_results['beta_0_hat'].median()\n",
    "        median_beta1 = df_results['beta_1_hat'].median()\n",
    "        std_beta0 = df_results['beta_0_hat'].std()\n",
    "        std_beta1 = df_results['beta_1_hat'].std()\n",
    "        test_1pct = df_results['reject_1pct'].mean() #Equivale a la proporción de rechazos de las 5000 simulaciones al 1% de significancia.\n",
    "        test_5pct = df_results['reject_5pct'].mean() #Equivale a la proporción de rechazos de las 5000 simulaciones al 5% de significancia.\n",
    "        \n",
    "        results.append([n_obs, mean_beta0, median_beta0, std_beta0, mean_beta1, median_beta1, std_beta1, test_1pct, test_5pct])\n",
    "    \n",
    "    results_df = pd.DataFrame(results, columns=['n_observations', 'mean_beta0', 'median_beta0', 'std_beta0', 'mean_beta1', 'median_beta1', 'std_beta1', 'test_1pct', 'test_5pct'])\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_gls(n_samples: int, n_observations: int, beta_0_true: float, beta_1_true: float, beta1_H0: float, omega: np.ndarray) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Simula la estimación de GLS para un determinado tamaño de muestra, y evalúa la validez de un test de hipótesis.\n",
    "\n",
    "    Parámteros:\n",
    "    ----------\n",
    "    n_samples : int\n",
    "        Número de simulaciones a realizar para cada tamaño de muestra.\n",
    "    n_observations : int\n",
    "        Tamaño de la muestra.\n",
    "    beta_0_true : float\n",
    "        Valor verdadero del intercepto en la simulación.\n",
    "    beta_1_true : float\n",
    "        Valor verdadero del coeficiente del regresor en la simulación.\n",
    "    beta1_H0 : float\n",
    "        Valor hipotético de beta_1 bajo H0 para el test de hipótesis.\n",
    "    omega : np.ndarray\n",
    "        Matriz de varianzas y covarianzas de los errores.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame con estadísticas de los coeficientes estimados y tasas de rechazo del test de hipótesis.\n",
    "        Columnas:\n",
    "        - 'n_observations': Tamaño de la muestra.\n",
    "        - 'mean_beta0': Media de las estimaciones de beta_0.\n",
    "        - 'median_beta0': Mediana de las estimaciones de beta_0.\n",
    "        - 'std_beta0': Desviación estándar de las estimaciones de beta_0.\n",
    "        - 'mean_beta1': Media de las estimaciones de beta_1.\n",
    "        - 'median_beta1': Mediana de las estimaciones de beta_1.\n",
    "        - 'std_beta1': Desviación estándar de las estimaciones de beta_1.\n",
    "        - 'test_size_1pct': Proporción de rechazos de H0 al 1% de significancia.\n",
    "        - 'test_size_5pct': Proporción de rechazos de H0 al 5% de significancia.\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_results = []\n",
    "\n",
    "    # Descomposición de Cholesky de omega.\n",
    "    omega = np.diag(omega)\n",
    "    P = np.linalg.cholesky(omega)\n",
    "    P_inv = np.linalg.inv(P)\n",
    "\n",
    "    # Iteración/simulaciones sobre cada observación de la muestra.\n",
    "    for _ in range(n_samples):\n",
    "        # Generación de datos.\n",
    "        x = np.random.uniform(1, 50, n_observations)  # x ~ U[1, 50]\n",
    "        u = np.random.multivariate_normal(mean=np.zeros(n_observations), cov=omega)  # u ~ N(0, omega)\n",
    "        y = beta_0_true + beta_1_true * x + u\n",
    "\n",
    "        # Transformación para GLS.\n",
    "        y_tilde = P_inv @ y\n",
    "        X = np.column_stack((np.ones(n_observations), x)) # Se construye matriz de regresores, contemplando intercepto.\n",
    "        X_tilde = P_inv @ X\n",
    "\n",
    "        # Ajuste modelo transformado con OLS para obtener estimadores y desvío estándar de los mismos.\n",
    "        beta_hat = np.linalg.inv(X_tilde.T @ X_tilde) @ (X_tilde.T @ y_tilde)\n",
    "        residuals = y_tilde - X_tilde @ beta_hat\n",
    "        s2 = (residuals @ residuals) / (n_observations - X_tilde.shape[1])\n",
    "        var_beta_hat = s2 * np.linalg.inv(X_tilde.T @ X_tilde)\n",
    "        se_beta1_hat = np.sqrt(np.diag(var_beta_hat))[1]\n",
    "\n",
    "        # Test de hipótesis para beta_1 = beta1_H0 (tamaño del test).\n",
    "        t_stat = (beta_hat[1] - beta1_H0) / se_beta1_hat\n",
    "        p_value = 2 * (1 - t.cdf(abs(t_stat), df=n_observations - 2))\n",
    "        reject_1pct = p_value < 0.01  # Rechazar H0 al 1% de significancia.\n",
    "        reject_5pct = p_value < 0.05  # Rechazar H0 al 5% de significancia.\n",
    "\n",
    "        sample_results.append([beta_hat[0], beta_hat[1], reject_1pct, reject_5pct])\n",
    "\n",
    "    # Convertir a DataFrame.\n",
    "    df_results = pd.DataFrame(sample_results, columns=['beta_0_hat', 'beta_1_hat', 'reject_1pct', 'reject_5pct'])\n",
    "\n",
    "    # Calcular estadísticas.\n",
    "    results = {\n",
    "        'n_observations': n_observations,\n",
    "        'mean_beta0': df_results['beta_0_hat'].mean(),\n",
    "        'median_beta0': df_results['beta_0_hat'].median(),\n",
    "        'std_beta0': df_results['beta_0_hat'].std(),\n",
    "        'mean_beta1': df_results['beta_1_hat'].mean(),\n",
    "        'median_beta1': df_results['beta_1_hat'].median(),\n",
    "        'std_beta1': df_results['beta_1_hat'].std(),\n",
    "        'test_size_1pct': df_results['reject_1pct'].mean(),\n",
    "        'test_size_5pct': df_results['reject_5pct'].mean()\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame([results])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios.**\n",
    "1. Genere **5000 muestras de 5N = 5 observaciones** de corte transversal a partir del modelo (1):\n",
    "     \n",
    "   **a)** Para cada muestra estime por **FGLS** los parámetros del modelo y realice un test de hipótesis para contrastar que $H_0$ : $\\beta_1$  = 0.8. Reporte **tamaño del test** al 1% y 5% y el **poder del test** cuando $\\beta_1$ = 0 y $\\beta_1$ = 0.4. Adicionalmente, reporte la media, mediana y desvio estándar de las estimaciones de $\\beta_0$ y $\\beta_1$.\n",
    "\n",
    "   **b)** Utilice la **descomposición de Cholesky** para encontrar una matriz $\\Omega = P P'$ y aplique la transformación correspondiente a **GLS** al modelo (1) y estime el modelo por **OLS**, comente si cambia algún resultado.\n",
    "\n",
    "2. Repita el punto anterior (sólo el inciso a)) con **5N = 10**.\n",
    "\n",
    "3. Repita el punto anterior (sólo el inciso a)) con **5N = 30**.\n",
    "\n",
    "4. Repita el punto anterior (sólo el inciso a)) con **5N = 100**.\n",
    "\n",
    "5. Repita el punto anterior (sólo el inciso a)) con **5N = 200**.\n",
    "\n",
    "6. Repita el punto anterior (sólo el inciso a)) con **5N = 500**.\n",
    "\n",
    "7. Describa detalladamente las **propiedades de muestra finita de FGLS** de acuerdo a lo que observó de los cuatro puntos anteriores. En especial, explique **cómo cambia el tamaño y el poder de los tests** a medida que aumenta el tamaño de muestra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios 1a, 2, 3, 4, 5, 6.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>mean_beta0</th>\n",
       "      <th>median_beta0</th>\n",
       "      <th>std_beta0</th>\n",
       "      <th>mean_beta1</th>\n",
       "      <th>median_beta1</th>\n",
       "      <th>std_beta1</th>\n",
       "      <th>test_1pct</th>\n",
       "      <th>test_5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>9.2582</td>\n",
       "      <td>-2.6233</td>\n",
       "      <td>893.4464</td>\n",
       "      <td>0.4592</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>22.2380</td>\n",
       "      <td>0.1124</td>\n",
       "      <td>0.2204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>18.4817</td>\n",
       "      <td>-2.9402</td>\n",
       "      <td>994.5864</td>\n",
       "      <td>0.3145</td>\n",
       "      <td>0.7957</td>\n",
       "      <td>23.2917</td>\n",
       "      <td>0.1118</td>\n",
       "      <td>0.1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>-14.9002</td>\n",
       "      <td>-2.9858</td>\n",
       "      <td>630.7914</td>\n",
       "      <td>1.0043</td>\n",
       "      <td>0.8017</td>\n",
       "      <td>13.5219</td>\n",
       "      <td>0.0750</td>\n",
       "      <td>0.1512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>-4.3804</td>\n",
       "      <td>-3.0011</td>\n",
       "      <td>88.5079</td>\n",
       "      <td>0.8393</td>\n",
       "      <td>0.8005</td>\n",
       "      <td>3.1786</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>0.1088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>-2.9602</td>\n",
       "      <td>-2.9970</td>\n",
       "      <td>8.5433</td>\n",
       "      <td>0.8055</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>2.8978</td>\n",
       "      <td>0.0480</td>\n",
       "      <td>0.0952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>-2.9962</td>\n",
       "      <td>-2.9792</td>\n",
       "      <td>2.0976</td>\n",
       "      <td>0.8099</td>\n",
       "      <td>0.7984</td>\n",
       "      <td>1.7883</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.0768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  mean_beta0  median_beta0  std_beta0  mean_beta1  \\\n",
       "0               5      9.2582       -2.6233   893.4464      0.4592   \n",
       "1              10     18.4817       -2.9402   994.5864      0.3145   \n",
       "2              30    -14.9002       -2.9858   630.7914      1.0043   \n",
       "3             100     -4.3804       -3.0011    88.5079      0.8393   \n",
       "4             200     -2.9602       -2.9970     8.5433      0.8055   \n",
       "5             500     -2.9962       -2.9792     2.0976      0.8099   \n",
       "\n",
       "   median_beta1  std_beta1  test_1pct  test_5pct  \n",
       "0        0.7869    22.2380     0.1124     0.2204  \n",
       "1        0.7957    23.2917     0.1118     0.1968  \n",
       "2        0.8017    13.5219     0.0750     0.1512  \n",
       "3        0.8005     3.1786     0.0578     0.1088  \n",
       "4        0.8000     2.8978     0.0480     0.0952  \n",
       "5        0.7984     1.7883     0.0370     0.0768  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se generan 5.000 muestras para 5N = 5, 10, 30, 100, 200, 500. Se fija beta_0 = -3 y beta_1 = 0.8, y se obtienen estadísticas descriptivas. Se evalúa H0: beta_1 = 0.8 (tamaño del test). \n",
    "df_1 = simulate_fgls(n_samples=5000, n_observations_list=[5, 10, 30, 100, 200, 500], beta_0_true=-3, beta_1_true=0.8, beta1_H0=0.8)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>mean_beta0</th>\n",
       "      <th>median_beta0</th>\n",
       "      <th>std_beta0</th>\n",
       "      <th>mean_beta1</th>\n",
       "      <th>median_beta1</th>\n",
       "      <th>std_beta1</th>\n",
       "      <th>test_1pct</th>\n",
       "      <th>test_5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>700</td>\n",
       "      <td>-3.0200</td>\n",
       "      <td>-3.0115</td>\n",
       "      <td>1.8553</td>\n",
       "      <td>0.8116</td>\n",
       "      <td>0.7993</td>\n",
       "      <td>1.6837</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>0.0690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>-3.0054</td>\n",
       "      <td>-2.9938</td>\n",
       "      <td>1.6094</td>\n",
       "      <td>0.8088</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>1.4567</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.0606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1200</td>\n",
       "      <td>-2.9859</td>\n",
       "      <td>-2.9945</td>\n",
       "      <td>1.1024</td>\n",
       "      <td>0.7919</td>\n",
       "      <td>0.8000</td>\n",
       "      <td>0.9931</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1500</td>\n",
       "      <td>-2.9848</td>\n",
       "      <td>-3.0003</td>\n",
       "      <td>0.8170</td>\n",
       "      <td>0.7858</td>\n",
       "      <td>0.8009</td>\n",
       "      <td>0.7410</td>\n",
       "      <td>0.0146</td>\n",
       "      <td>0.0528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>-2.9913</td>\n",
       "      <td>-2.9926</td>\n",
       "      <td>0.2693</td>\n",
       "      <td>0.7976</td>\n",
       "      <td>0.7992</td>\n",
       "      <td>0.1837</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.0472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  mean_beta0  median_beta0  std_beta0  mean_beta1  \\\n",
       "0             700     -3.0200       -3.0115     1.8553      0.8116   \n",
       "1            1000     -3.0054       -2.9938     1.6094      0.8088   \n",
       "2            1200     -2.9859       -2.9945     1.1024      0.7919   \n",
       "3            1500     -2.9848       -3.0003     0.8170      0.7858   \n",
       "4            2000     -2.9913       -2.9926     0.2693      0.7976   \n",
       "\n",
       "   median_beta1  std_beta1  test_1pct  test_5pct  \n",
       "0        0.7993     1.6837     0.0292     0.0690  \n",
       "1        0.8000     1.4567     0.0236     0.0606  \n",
       "2        0.8000     0.9931     0.0166     0.0584  \n",
       "3        0.8009     0.7410     0.0146     0.0528  \n",
       "4        0.7992     0.1837     0.0112     0.0472  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adicionalmente, se generan 5.000 muestras para 5N = 700, 1000, 1200, 1500, 2000 para observar en qué tamaño de muestra se obtiene un tamaño de test más cercano al nivel de significancia.\n",
    "df_2 = simulate_fgls(n_samples=5000, n_observations_list=[700, 1000, 1200, 1500, 2000], beta_0_true=-3, beta_1_true=0.8, beta1_H0=0.8)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>mean_beta0</th>\n",
       "      <th>median_beta0</th>\n",
       "      <th>std_beta0</th>\n",
       "      <th>mean_beta1</th>\n",
       "      <th>median_beta1</th>\n",
       "      <th>std_beta1</th>\n",
       "      <th>test_1pct</th>\n",
       "      <th>test_5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>-7.0125</td>\n",
       "      <td>-3.1413</td>\n",
       "      <td>886.7962</td>\n",
       "      <td>0.8833</td>\n",
       "      <td>0.8005</td>\n",
       "      <td>20.4901</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.2260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20.5396</td>\n",
       "      <td>-3.3416</td>\n",
       "      <td>3967.3825</td>\n",
       "      <td>0.4204</td>\n",
       "      <td>0.8421</td>\n",
       "      <td>80.2601</td>\n",
       "      <td>0.1296</td>\n",
       "      <td>0.2424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>4.6121</td>\n",
       "      <td>-3.0040</td>\n",
       "      <td>817.8146</td>\n",
       "      <td>0.6478</td>\n",
       "      <td>0.7996</td>\n",
       "      <td>16.6626</td>\n",
       "      <td>0.1978</td>\n",
       "      <td>0.3780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>-3.2893</td>\n",
       "      <td>-2.9813</td>\n",
       "      <td>683.9996</td>\n",
       "      <td>0.7708</td>\n",
       "      <td>0.7974</td>\n",
       "      <td>13.9426</td>\n",
       "      <td>0.6706</td>\n",
       "      <td>0.8470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>-3.0105</td>\n",
       "      <td>-2.9966</td>\n",
       "      <td>9.5504</td>\n",
       "      <td>0.8725</td>\n",
       "      <td>0.7969</td>\n",
       "      <td>2.2672</td>\n",
       "      <td>0.9576</td>\n",
       "      <td>0.9860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>-3.0284</td>\n",
       "      <td>-3.0087</td>\n",
       "      <td>2.2056</td>\n",
       "      <td>0.8177</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>1.9367</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>0.9978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  mean_beta0  median_beta0  std_beta0  mean_beta1  \\\n",
       "0               5     -7.0125       -3.1413   886.7962      0.8833   \n",
       "1              10     20.5396       -3.3416  3967.3825      0.4204   \n",
       "2              30      4.6121       -3.0040   817.8146      0.6478   \n",
       "3             100     -3.2893       -2.9813   683.9996      0.7708   \n",
       "4             200     -3.0105       -2.9966     9.5504      0.8725   \n",
       "5             500     -3.0284       -3.0087     2.2056      0.8177   \n",
       "\n",
       "   median_beta1  std_beta1  test_1pct  test_5pct  \n",
       "0        0.8005    20.4901     0.1140     0.2260  \n",
       "1        0.8421    80.2601     0.1296     0.2424  \n",
       "2        0.7996    16.6626     0.1978     0.3780  \n",
       "3        0.7974    13.9426     0.6706     0.8470  \n",
       "4        0.7969     2.2672     0.9576     0.9860  \n",
       "5        0.8014     1.9367     0.9972     0.9978  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se generan 5.000 muestras para 5N = 5, 10, 30, 100, 200, 500. Se fija beta_0 = -3 y beta_1 = 0.8, y se obtienen estadísticas descriptivas. Se evalúa H0: beta_1 = 0.4 (poder del test). \n",
    "df_3 = simulate_fgls(n_samples=5000, n_observations_list=[5, 10, 30, 100, 200, 500], beta_0_true=-3, beta_1_true=0.8, beta1_H0=0.4)\n",
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>mean_beta0</th>\n",
       "      <th>median_beta0</th>\n",
       "      <th>std_beta0</th>\n",
       "      <th>mean_beta1</th>\n",
       "      <th>median_beta1</th>\n",
       "      <th>std_beta1</th>\n",
       "      <th>test_1pct</th>\n",
       "      <th>test_5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0561</td>\n",
       "      <td>-2.7573</td>\n",
       "      <td>608.1473</td>\n",
       "      <td>0.5681</td>\n",
       "      <td>0.7524</td>\n",
       "      <td>15.2896</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>0.2664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>20.0800</td>\n",
       "      <td>-2.9821</td>\n",
       "      <td>1546.4503</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>40.0468</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.3890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>0.6493</td>\n",
       "      <td>-3.0896</td>\n",
       "      <td>486.4660</td>\n",
       "      <td>0.7494</td>\n",
       "      <td>0.8189</td>\n",
       "      <td>10.1715</td>\n",
       "      <td>0.6284</td>\n",
       "      <td>0.7958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>-1.2233</td>\n",
       "      <td>-3.0248</td>\n",
       "      <td>96.8196</td>\n",
       "      <td>0.8041</td>\n",
       "      <td>0.8019</td>\n",
       "      <td>4.3415</td>\n",
       "      <td>0.9866</td>\n",
       "      <td>0.9908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200</td>\n",
       "      <td>-2.9545</td>\n",
       "      <td>-2.9852</td>\n",
       "      <td>3.6733</td>\n",
       "      <td>0.7654</td>\n",
       "      <td>0.7985</td>\n",
       "      <td>2.4708</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.9968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>-2.9836</td>\n",
       "      <td>-2.9985</td>\n",
       "      <td>2.6011</td>\n",
       "      <td>0.7830</td>\n",
       "      <td>0.8022</td>\n",
       "      <td>2.2992</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>0.9970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  mean_beta0  median_beta0  std_beta0  mean_beta1  \\\n",
       "0               5      5.0561       -2.7573   608.1473      0.5681   \n",
       "1              10     20.0800       -2.9821  1546.4503      0.2030   \n",
       "2              30      0.6493       -3.0896   486.4660      0.7494   \n",
       "3             100     -1.2233       -3.0248    96.8196      0.8041   \n",
       "4             200     -2.9545       -2.9852     3.6733      0.7654   \n",
       "5             500     -2.9836       -2.9985     2.6011      0.7830   \n",
       "\n",
       "   median_beta1  std_beta1  test_1pct  test_5pct  \n",
       "0        0.7524    15.2896     0.1304     0.2664  \n",
       "1        0.8014    40.0468     0.2144     0.3890  \n",
       "2        0.8189    10.1715     0.6284     0.7958  \n",
       "3        0.8019     4.3415     0.9866     0.9908  \n",
       "4        0.7985     2.4708     0.9948     0.9968  \n",
       "5        0.8022     2.2992     0.9962     0.9970  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se generan 5.000 muestras para 5N = 5, 10, 30, 100, 200, 500. Se fija beta_0 = -3 y beta_1 = 0.8, y se obtienen estadísticas descriptivas. Se evalúa H0: beta_1 = 0 (poder del test). \n",
    "df_4 = simulate_fgls(n_samples=5000, n_observations_list=[5, 10, 30, 100, 200, 500], beta_0_true=-3, beta_1_true=0.8, beta1_H0=0)\n",
    "df_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1b.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>mean_beta0</th>\n",
       "      <th>median_beta0</th>\n",
       "      <th>std_beta0</th>\n",
       "      <th>mean_beta1</th>\n",
       "      <th>median_beta1</th>\n",
       "      <th>std_beta1</th>\n",
       "      <th>test_size_1pct</th>\n",
       "      <th>test_size_5pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>-3.0237</td>\n",
       "      <td>-2.9613</td>\n",
       "      <td>4.8242</td>\n",
       "      <td>0.8009</td>\n",
       "      <td>0.8001</td>\n",
       "      <td>0.1678</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  mean_beta0  median_beta0  std_beta0  mean_beta1  \\\n",
       "0               5     -3.0237       -2.9613     4.8242      0.8009   \n",
       "\n",
       "   median_beta1  std_beta1  test_size_1pct  test_size_5pct  \n",
       "0        0.8001     0.1678          0.0090          0.0518  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se generan 5.000 muestras para 5N = 5. Se fija beta_0 = -3 y beta_1 = 0.8, Opcionalmente se obtienen estadísticas descriptivas, y se evalúa H0: beta_1 = 0.8 (tamaño del test).\n",
    "df_5 = simulate_gls(n_samples=5000, n_observations=5, beta_0_true=-3, beta_1_true=0.8, beta1_H0=0.8, omega=[4, 9, 16, 25, 36])\n",
    "df_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. PROPIEDADES DE MUESTRA FINITA DEL TEST DE WHITE Y DE LA CORRECIÓN POR HETEROCEDASTICIDAD.**\n",
    "\n",
    "Consideremos el siguiente modelo:\n",
    "\n",
    "$$\n",
    "y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\sqrt{v_i} u_i, \\quad i = 1, \\dots, n \\tag{2}\n",
    "$$\n",
    "\n",
    "Para $n = 20$, $x_1$ se determina como una secuencia de 18 puntos espaciados uniformemente entre -1 y 1 con puntos extremos dados por -1.1 y 1.1, mientras que  $x_2$ son cuantiles de una distribución normal estándar elegidos aleatoriamente. Las observaciones se repiten tres veces para obtener una muestra de $n = 60$, cinco veces para una muestra de $n = 100$, diez veces para una muestra de $n = 200$, veinte veces para una muestra de $n = 400$ y treinta veces para una muestra de $n = 600$.La generación de los datos de la variable dependiente se hace con  $\\beta_0 = \\beta_1  = \\beta_2$ = 1.\n",
    "\n",
    "Existen tres diseños:\n",
    "\n",
    "1. **Diseño 0**: $u_i \\sim N(0,1)$, y $v_i = 1$ (Normalidad y homocedasticidad);\n",
    "\n",
    "2. **Diseño 1**: $u_i \\sim N(0,1)$, y $v_i = e^{0.25 x_{1i} + 0.25 x_{2i}}$ (Normalidad y heterocedasticidad);\n",
    "\n",
    "3. **Diseño 2**: $u_i \\sim t_5$, y $v_i = e^{0.25 x_{1i} + 0.25 x_{2i}}$ (No-normalidad y heterocedasticidad).\n",
    "\n",
    "Todas las simulaciones se basan en **5.000 replicaciones**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se presentan dos funciones que se utilizarán en el apartado 2:**\n",
    "- **white_het_test:** realiza simulaciones para evaluar la validez del test de heterocedasticidad de White;\n",
    "- **white_vcov_correction:** evalúa el sesgo en la estimación de la Matriz de Varianzas y Covarianzas de los coeficientes mediante la corrección de White.\n",
    "\n",
    "**Observación:** se prescinde del uso de librerías como *statsmodels*, que incluyen implementaciones del Test de White y corrección de Matriz de Varianzas y Covarianzas de White, con el fin de aplicar las ecuaciones vistas en clase, lo que aporta mayor claridad. Para operaciones matriciales, se utiliza la librería *numpy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_het_test(n_samples: int, n_observations_list: list[int], beta_0_true: float, beta_1_true: float, beta_2_true: float, design: int) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Realiza simulaciones para evaluar la validez del test de heterocedasticidad de White \n",
    "    bajo distintos tamaños muestrales y diseños de error.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Número de simulaciones a realizar para cada tamaño de muestra.\n",
    "    n_observations_list : list[int]\n",
    "        Lista con los diferentes tamaños muestrales a evaluar.\n",
    "    beta_0_true : float\n",
    "        Valor verdadero del intercepto en la simulación.\n",
    "    beta_1_true : float\n",
    "        Valor verdadero del coeficiente del primer regresor en la simulación.\n",
    "    beta_2_true : float\n",
    "        Valor verdadero del coeficiente del segundo regresor en la simulación.\n",
    "    design : int\n",
    "        Especifica el diseño de la heterocedasticidad:\n",
    "        - 0: Homocedástico (u ~ N(0,1), v = 1).\n",
    "        - 1: Heterocedástico (u ~ N(0,1), v = exp(0.25*x1 + 0.25*x2)).\n",
    "        - 2: Heterocedástico y distribución t (u ~ t(5), v = exp(0.25*x1 + 0.25*x2)).\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame con las tasas de rechazo del test de heterocedasticidad de White \n",
    "        para cada tamaño muestral considerado.\n",
    "        Columnas:\n",
    "        - 'n_observations': Tamaño de la muestra.\n",
    "        - 'test_1pct': Proporción de rechazos de H0 al 1% de significancia.\n",
    "        - 'test_5pct': Proporción de rechazos de H0 al 5% de significancia.\n",
    "        - 'test_10pct': Proporción de rechazos de H0 al 10% de significancia.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    # Iteración sobre los tamaños de muestra.\n",
    "    for n_obs in n_observations_list:\n",
    "        \n",
    "        test_results = []\n",
    "\n",
    "        # Iteración/simulaciones sobre cada observación de la muestra.\n",
    "        for _ in range(n_samples):\n",
    "\n",
    "            # Generación de datos.\n",
    "            x1_base = np.linspace(-1, 1, 18)  # 18 puntos uniformes entre -1 y 1.\n",
    "            x1_base = np.concatenate([[-1.1, 1.1], x1_base])  # Agregar extremos -1.1 y 1.1.\n",
    "            repetitions = n_obs // 20 # Calcula el número de repeticiones.\n",
    "            x1 = np.tile(x1_base, repetitions) # Replica las 20 observaciones para obtener el tamaño correcto.\n",
    "            x2_base = np.random.normal(0, 1, 20) # x2 ~ N(0,1).\n",
    "            x2 = np.tile(x2_base, repetitions) # Replica las 20 observaciones para obtener el tamaño correcto.\n",
    "            X = np.column_stack((np.ones_like(x1), x1, x2)) #Se agrega intercepto.\n",
    "\n",
    "            if design == 0:\n",
    "                u = np.random.normal(0, 1, n_obs) # u: normal.\n",
    "                v = np.ones(n_obs)  # v: v = 1.\n",
    "            elif design == 1:\n",
    "                u = np.random.normal(0, 1, n_obs) # u: normal.\n",
    "                v = np.exp(0.25 * x1 + 0.25 * x2) #v: v = exp(0.25*x1 + 0.25*x2).\n",
    "            elif design == 2:\n",
    "                u = np.random.standard_t(5, n_obs)# u: t-student.\n",
    "                v = np.exp(0.25 * x1 + 0.25 * x2) #v: v = exp(0.25*x1 + 0.25*x2).\n",
    "            else:\n",
    "                raise ValueError(\"Invalid design parameter. Choose 0, 1 or 2.\")\n",
    "\n",
    "            y = beta_0_true + beta_1_true * x1 + beta_2_true * x2 + np.sqrt(v) * u\n",
    "\n",
    "            # Construcción del estadístico LM para el test de White.\n",
    "            # 1. Estimar modelo inicial y ~ x por OLS y obtener la serie de residuos y de sus cuadrados.\n",
    "            beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "            u_hat = y - X @ beta_ols\n",
    "            u_hat2 = u_hat ** 2\n",
    "\n",
    "            # 2. Dada la forma funcional de la heterocedasticidad de White para este modelo, sigma2 ~ x + x^2 + x2 + x2^2 + x1*x2, \n",
    "            # se estima por OLS usando u_hat2 como proxy de sigma2.\n",
    "            X_aux_wc = np.column_stack((x1, x2, x1 ** 2, x2 ** 2, x1 * x2))\n",
    "            X_aux = np.column_stack((X, x1 ** 2, x2 ** 2, x1 * x2))\n",
    "            beta_ols_aux = np.linalg.inv(X_aux.T @ X_aux) @ X_aux.T @ u_hat2\n",
    "\n",
    "            # 3. Construcción del estadístico.\n",
    "            # R^2 de la regresión auxiliar.\n",
    "            u_hat_aux = u_hat2 - X_aux @ beta_ols_aux\n",
    "            ss_total = np.sum((u_hat2 - np.mean(u_hat2)) ** 2)\n",
    "            ss_residual = np.sum(u_hat_aux ** 2)\n",
    "            r_squared = 1 - (ss_residual / ss_total)\n",
    "\n",
    "            # Estadístico LM.\n",
    "            lm_stat = n_obs * r_squared\n",
    "\n",
    "            # P-valor de LM.\n",
    "            p_value = chi2.sf(lm_stat, X_aux_wc.shape[1])\n",
    "\n",
    "            reject_1pct = p_value < 0.01 # Rechazar H0 al 1% de significancia.\n",
    "            reject_5pct = p_value < 0.05 # Rechazar H0 al 5% de significancia.\n",
    "            reject_10pct = p_value < 0.10 # Rechazar H0 al 10% de significancia.\n",
    "\n",
    "            test_results.append([reject_1pct, reject_5pct, reject_10pct])\n",
    "\n",
    "        # Convertir los resultados a un DataFrame\n",
    "        test_results_df = pd.DataFrame(test_results, columns=['reject_1pct', 'reject_5pct', 'reject_10pct'])\n",
    "        \n",
    "        # Calcular estadísticas.\n",
    "        test_1pct = test_results_df['reject_1pct'].mean() #Equivale a la proporción de rechazos de las 5000 simulaciones al 1% de significancia.\n",
    "        test_5pct = test_results_df['reject_5pct'].mean() #Equivale a la proporción de rechazos de las 5000 simulaciones al 5% de significancia.\n",
    "        test_10pct = test_results_df['reject_10pct'].mean() #Equivale a la proporción de rechazos de las 5000 simulaciones al 10% de significancia.\n",
    "        \n",
    "        results.append([n_obs, test_1pct, test_5pct, test_10pct])\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=['n_observations', 'test_1pct', 'test_5pct', 'test_10pct'])\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_vcov_correction(n_samples: int, n_observations_list: list[int], beta_0_true: float, beta_1_true: float, beta_2_true: float, design: int) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Evalúa el sesgo en la estimación de la Matriz de Varianzas y Covarianzas de los coeficientes\n",
    "    mediante la corrección de White bajo diferentes tamaños muestrales y diseños de error.\n",
    "\n",
    "    Parámetros:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Número de simulaciones a realizar por cada tamaño de muestra.\n",
    "    n_observations_list : list[int]\n",
    "        Lista de tamaños muestrales a evaluar.\n",
    "    beta_0_true : float\n",
    "        Valor verdadero del intercepto en la simulación.\n",
    "    beta_1_true : float\n",
    "        Valor verdadero del coeficiente del primer regresor.\n",
    "    beta_2_true : float\n",
    "        Valor verdadero del coeficiente del segundo regresor.\n",
    "    design : int\n",
    "        Especifica la distribución del error:\n",
    "        - 1: Homocedástico (u ~ N(0,1)).\n",
    "        - 2: Heterocedástico con distribución t (u ~ t(5)).\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    results_df : pd.DataFrame\n",
    "        DataFrame con el sesgo relativo promedio en la estimación de la varianza de los coeficientes.\n",
    "        Columnas:\n",
    "        - 'n_observations': Tamaño de la muestra.\n",
    "        - 'bias_var_beta0': Sesgo (White con residuos vs. Poblacional) relativo en la varianza del intercepto.\n",
    "        - 'bias_var_beta1': Sesgo (White con residuos vs. Poblacional) relativo en la varianza del coeficiente de x1.\n",
    "        - 'bias_var_beta2': Sesgo (White con residuos vs. Poblacional) relativo en la varianza del coeficiente de x2.\n",
    "        - 'bias_var_total': Suma de los sesgos anteriores.\n",
    "        - 'bias_var_beta0_pop': Sesgo (White con errores vs. Poblacional) relativo varianza del intercepto.\n",
    "        - 'bias_var_beta1_pop': Sesgo (White con errores vs. Poblacional) relativo varianza del coeficiente de x1.\n",
    "        - 'bias_var_beta2_pop': Sesgo (White con errores vs. Poblacional) relativo varianza del coeficiente de x2.\n",
    "        - 'bias_var_total_pop': Suma de los sesgos anteriores.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    for n_obs in n_observations_list:\n",
    "        \n",
    "        bias_results = []\n",
    "\n",
    "        # Iteración/simulaciones sobre cada observación de la muestra.\n",
    "        for _ in range(n_samples):\n",
    "\n",
    "            # Generación de datos.\n",
    "            x1_base = np.linspace(-1, 1, 18)  # 18 puntos uniformes entre -1 y 1.\n",
    "            x1_base = np.concatenate([[-1.1, 1.1], x1_base])  # Agregar extremos -1.1 y 1.1.\n",
    "            repetitions = n_obs // 20 # Calcula el número de repeticiones.\n",
    "            x1 = np.tile(x1_base, repetitions) # Replica las 20 observaciones para obtener el tamaño correcto.\n",
    "            x2_base = np.random.normal(0, 1, 20) # x2 ~ N(0,1).\n",
    "            x2 = np.tile(x2_base, repetitions) # Replica las 20 observaciones para obtener el tamaño correcto.\n",
    "            X = np.column_stack((np.ones_like(x1), x1, x2)) #Se agrega intercepto.\n",
    "\n",
    "            if design == 1:\n",
    "                u = np.random.normal(0, 1, n_obs) # u ~ N(0, 1).\n",
    "                var_u_t = 1  # Varianza teórica.\n",
    "            elif design == 2:\n",
    "                u = np.random.standard_t(5, n_obs)# u ~ t-student(5).\n",
    "                var_u_t = 5 / 3 # Varianza teórica: (var(u) = df / (df - 2)); df: degree of freedom.\n",
    "            else:\n",
    "                raise ValueError(\"Invalid design parameter. Choose 1 or 2.\")\n",
    "\n",
    "            v = np.exp(0.25 * x1 + 0.25 * x2) #v: v = exp(0.25*x1 + 0.25*x2)\n",
    "            y = beta_0_true + beta_1_true * x1 + beta_2_true * x2 + np.sqrt(v) * u\n",
    "\n",
    "            # Estimar modelo inicial y ~ x por OLS y obtener la serie de residuos y de sus cuadrados.\n",
    "            beta_ols = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "            u_hat = y - X @ beta_ols\n",
    "            u_hat2 = u_hat ** 2\n",
    "\n",
    "            # Matriz de varianzas y covarianzas de los errores propuesta por White.\n",
    "            omega_white = np.diag(u_hat2)\n",
    "            # Matriz de varianzas y covarianzas de los errores poblacional.\n",
    "            omega_pop = var_u_t * np.diag(v)\n",
    "            # Matriz de varianzas y covarianzas de los errores propuesta por White, reemplazando residuos por errores.\n",
    "            omega_white_pop = np.diag(np.power(np.sqrt(v)*u, 2))\n",
    "\n",
    "            #Matriz de varianzas y covarianzas de los parámetros, para cada matriz de varianzas y covarianzas de los errores.\n",
    "            vcov_beta_white = np.linalg.inv(X.T @ X) @ X.T @ omega_white @ X @ np.linalg.inv(X.T @ X)\n",
    "            vcov_beta_pop = np.linalg.inv(X.T @ X) @ X.T @ omega_pop @ X @ np.linalg.inv(X.T @ X)\n",
    "            vcov_beta_white_pop = np.linalg.inv(X.T @ X) @ X.T @ omega_white_pop @ X @ np.linalg.inv(X.T @ X)\n",
    "\n",
    "            # Varianzas de los parámetros.\n",
    "            var_beta0_white, var_beta1_white, var_beta2_white = np.diag(vcov_beta_white)\n",
    "            var_beta0_pop, var_beta1_pop, var_beta2_pop = np.diag(vcov_beta_pop)\n",
    "            var_beta0_white_pop, var_beta1_white_pop, var_beta2_white_pop = np.diag(vcov_beta_white_pop)\n",
    "\n",
    "            # Sesgo relativo y sesgos relativos totales de la estimación de las varianzas de los parámetros (White con residuos vs. Poblacional).\n",
    "            bias_var_beta0 = (var_beta0_white - var_beta0_pop) / var_beta0_pop\n",
    "            bias_var_beta1 = (var_beta1_white - var_beta1_pop) / var_beta1_pop\n",
    "            bias_var_beta2 = (var_beta2_white - var_beta2_pop) / var_beta2_pop\n",
    "\n",
    "            # Sesgo relativo y sesgos relativos totales de la estimación de las varianzas de los parámetros (White con errores vs. Poblacional).\n",
    "            bias_var_beta0_pop = (var_beta0_white_pop - var_beta0_pop) / var_beta0_pop\n",
    "            bias_var_beta1_pop = (var_beta1_white_pop - var_beta1_pop) / var_beta1_pop\n",
    "            bias_var_beta2_pop = (var_beta2_white_pop - var_beta2_pop) / var_beta2_pop\n",
    "\n",
    "            bias_results.append([bias_var_beta0, bias_var_beta1, bias_var_beta2, bias_var_beta0_pop, bias_var_beta1_pop, bias_var_beta2_pop])\n",
    "        \n",
    "        # Convertir los resultados a un DataFrame.\n",
    "        bias_results_df = pd.DataFrame(bias_results, columns=['bias_var_beta0', 'bias_var_beta1', 'bias_var_beta2', 'bias_var_beta0_pop', 'bias_var_beta1_pop', 'bias_var_beta2_pop'])\n",
    "        \n",
    "        # Calcular estadísticas.\n",
    "        bias_var_beta0 = bias_results_df['bias_var_beta0'].mean() \n",
    "        bias_var_beta1 = bias_results_df['bias_var_beta1'].mean()\n",
    "        bias_var_beta2 = bias_results_df['bias_var_beta2'].mean()\n",
    "        bias_var_total = abs(bias_var_beta0) + abs(bias_var_beta1) + abs(bias_var_beta2)\n",
    "        bias_var_beta0_pop = bias_results_df['bias_var_beta0_pop'].mean()\n",
    "        bias_var_beta1_pop = bias_results_df['bias_var_beta1_pop'].mean()\n",
    "        bias_var_beta2_pop = bias_results_df['bias_var_beta2_pop'].mean()\n",
    "        bias_var_pop_total = abs(bias_var_beta0_pop) + abs(bias_var_beta1_pop) + abs(bias_var_beta2_pop)\n",
    "        \n",
    "        results.append([n_obs, bias_var_beta0, bias_var_beta1, bias_var_beta2, bias_var_total, bias_var_beta0_pop, bias_var_beta1_pop, bias_var_beta2_pop, bias_var_pop_total])\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=['n_observations', 'bias_var_beta0', 'bias_var_beta1', 'bias_var_beta2', 'bias_var_total', 'bias_var_beta0_pop', 'bias_var_beta1_pop', 'bias_var_pop_total', 'bias_var_total_pop'])\n",
    "        \n",
    "    # Formato de columnas.\n",
    "    cols_ex = ['n_observations']\n",
    "    cols_in = [col for col in results_df.columns if col not in cols_ex]\n",
    "\n",
    "    return results_df.style.format({col: '{:.2%}' for col in cols_in})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.1. TEST DE HETEROCEDASTICIDAD DE WHITE.**\n",
    "\n",
    "**Ejercicios.**\n",
    "\n",
    "**a)** Para el **diseño 0**, genere 5.000 muestras de $n = 20$ observaciones a partir del modelo (2). Para cada muestra, **estime por OLS** los parámetros del modelo y realice el **test de hipótesis de White** para contrastar que $H_0$ : No hay heterocedasticidad. Reporte el **tamaño del test** al 1%, 5%, 10%.\n",
    "\n",
    "**b)** Para los **diseños 1 y 2**, genere 5.000 muestras de $n = 20$ observaciones a partir del modelo (2) y reporte el **poder del test cuando el diseño utilizado es el modelo poblacional verdadero**.\n",
    "\n",
    "**c)** Repita el punto anterior con $n = 60$.\n",
    "\n",
    "**d)** Repita el punto anterior con $n = 100$.\n",
    "\n",
    "**e)** Repita el punto anterior con $n = 200$.\n",
    "\n",
    "**f)** Repita el punto anterior con $n = 400$.\n",
    "\n",
    "**g)** Repita el punto anterior con $n = 600$.\n",
    "\n",
    "**h)** Describa detalladamente las **propiedades de muestra finita del test de White** de acuerdo a lo observado en los puntos anteriores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios 2.1.a.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>test_1pct</th>\n",
       "      <th>test_5pct</th>\n",
       "      <th>test_10pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.0884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  test_1pct  test_5pct  test_10pct\n",
       "0              20     0.0044     0.0412      0.0884"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se generan 5.000 muestras para n = 20. Se fija beta_0 = beta_1 = beta_1 = 1, y diseño de errores normales y homocedásticos. \n",
    "# Se evalúa H0: No hay heterocedasticidad (tamaño del test).\n",
    "df_6 = white_het_test(n_samples=5000, n_observations_list=[20], beta_0_true=1, beta_1_true=1, beta_2_true=1, design=0)\n",
    "df_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>test_1pct</th>\n",
       "      <th>test_5pct</th>\n",
       "      <th>test_10pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.0912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.0980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0524</td>\n",
       "      <td>0.0958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>400</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.0984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>600</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0448</td>\n",
       "      <td>0.0938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  test_1pct  test_5pct  test_10pct\n",
       "0              60     0.0106     0.0470      0.0912\n",
       "1             100     0.0124     0.0518      0.0980\n",
       "2             200     0.0132     0.0524      0.0958\n",
       "3             400     0.0118     0.0488      0.0984\n",
       "4             600     0.0102     0.0448      0.0938"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adicionalmente, se generan 5.000 muestras para n = 60, 100, 200, 400, 600 para comprobar que el tamaño del test se estabiliza en torno al nivel de significancia.\n",
    "df_7 = white_het_test(n_samples=5000, n_observations_list=[60, 100, 200, 400, 600], beta_0_true=1, beta_1_true=1, beta_2_true=1, design=0)\n",
    "df_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios 2.1.b-g, diseño 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>test_1pct</th>\n",
       "      <th>test_5pct</th>\n",
       "      <th>test_10pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0574</td>\n",
       "      <td>0.1238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>0.2398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.1030</td>\n",
       "      <td>0.2508</td>\n",
       "      <td>0.3584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>0.2838</td>\n",
       "      <td>0.5172</td>\n",
       "      <td>0.6418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400</td>\n",
       "      <td>0.6724</td>\n",
       "      <td>0.8360</td>\n",
       "      <td>0.9028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>600</td>\n",
       "      <td>0.8734</td>\n",
       "      <td>0.9516</td>\n",
       "      <td>0.9706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  test_1pct  test_5pct  test_10pct\n",
       "0              20     0.0046     0.0574      0.1238\n",
       "1              60     0.0506     0.1488      0.2398\n",
       "2             100     0.1030     0.2508      0.3584\n",
       "3             200     0.2838     0.5172      0.6418\n",
       "4             400     0.6724     0.8360      0.9028\n",
       "5             600     0.8734     0.9516      0.9706"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se generan 5.000 muestras para n = 20, 60, 100, 200, 400, 600. Se fija beta_0 = beta_1 = beta_1 = 1, y diseño de errores normales y heterocedásticos. \n",
    "# Se evalúa H0: No hay heterocedasticidad (poder del test).\n",
    "df_8 = white_het_test(n_samples=5000, n_observations_list=[20, 60, 100, 200, 400, 600], beta_0_true=1, beta_1_true=1, beta_2_true=1, design=1)\n",
    "df_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios 2.1.b-g, diseño 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_observations</th>\n",
       "      <th>test_1pct</th>\n",
       "      <th>test_5pct</th>\n",
       "      <th>test_10pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.1118</td>\n",
       "      <td>0.1832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.2232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>0.1136</td>\n",
       "      <td>0.2492</td>\n",
       "      <td>0.3564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>400</td>\n",
       "      <td>0.2518</td>\n",
       "      <td>0.4528</td>\n",
       "      <td>0.5720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>600</td>\n",
       "      <td>0.4060</td>\n",
       "      <td>0.6048</td>\n",
       "      <td>0.7002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_observations  test_1pct  test_5pct  test_10pct\n",
       "0              20     0.0088     0.0608      0.1174\n",
       "1              60     0.0362     0.1118      0.1832\n",
       "2             100     0.0534     0.1440      0.2232\n",
       "3             200     0.1136     0.2492      0.3564\n",
       "4             400     0.2518     0.4528      0.5720\n",
       "5             600     0.4060     0.6048      0.7002"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se generan 5.000 muestras para n = 20, 60, 100, 200, 400, 600. Se fija beta_0 = beta_1 = beta_1 = 1, y diseño de errores no-normales y heterocedásticos. \n",
    "# Se evalúa H0: No hay heterocedasticidad (poder del test).\n",
    "df_9 = white_het_test(n_samples=5000, n_observations_list=[20, 60, 100, 200, 400, 600], beta_0_true=1, beta_1_true=1, beta_2_true=1, design=2)\n",
    "df_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2.2. CORRECIÓN DE LA MATRIZ DE VARIANZAS Y COVARIANZAS EN PRESENCIA DE HETEROCEDASTICIDAD.**\n",
    "\n",
    "En el modelo de regresión lineal (2) la estimación de MCC es:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2)' = (X'X)^{-1}X'y \\tag{3}\n",
    "$$\n",
    "\n",
    "donde $X$ es la matriz de variables explicativas de dimensión $n$ x $3$ e $y$ es el vector de observaciones de la variable dependiente de dimensión $n$ x $1$. Bajo heterocedasticidad, la matriz de varianzas y covarianzas de los estimadores de MCC es: $\\text{Var}(\\hat{\\beta}) = (X'X)^{-1}X'\\Omega X (X'X)^{-1}$ con $\\Omega$ = $\\text{diag}(\\sigma_1^2, \\dots, \\sigma_n^2)$. Una estimación consistente de esta matriz está dada por la denominada matriz de White (White, 1980): $\\text{Var}(\\hat{\\beta}) = (X'X)^{-1}X'\\hat{\\Omega} X (X'X)^{-1}$ con $\\hat{\\Omega}$ = $\\text{diag}(\\hat{u}_1^2, \\dots, \\hat{u}_n^2)$ y $\\hat{u}$ el vector de dimensión $n$ x 1 de residuos de la estimación por MCC.\n",
    "\n",
    "**Ejercicios.**\n",
    "\n",
    "**a)** Para cada uno de los diseños 1 y 2 genere 5000 muestras de $n = 20$ observaciones a partir del modelo (2). Para cada muestra estime por MCC los parámetros del modelo y reporte el sesgo relativo de la estimación de las varianzas de $\\hat{\\beta}_0$, $\\hat{\\beta}_1$ y $\\hat{\\beta}_2$ y los sesgos relativos totales. El sesgo relativo se define como el promedio, a lo largo de las 5.000 simulaciones, de la varianza estimada menos la varianza verdadera divididas por la varianza verdadera y el sesgo relativototal es la suma del valor absoluto de b0; b1 y b2. El sesgo relativo total es una medida del sesgo agregado de las tres varianzas.\n",
    "\n",
    "**b)** Repita el punto anterior con $n = 60$.\n",
    "\n",
    "**c)** Repita el punto anterior con $n = 100$.\n",
    "\n",
    "**d)** Repita el punto anterior con $n = 200$.\n",
    "\n",
    "**e)** Repita el punto anterior con $n = 400$.\n",
    "\n",
    "**f)** Repita el punto anterior con $n = 600$.\n",
    "\n",
    "**g)** Repita los puntos (a) a (f) pero ahora construya la estimación de la matriz de White usando una matriz diagonal con los errores verdaderos elevados al cuadrado en lugar de utilizar la matriz diagonal con los residuos elevados al cuadrado.\n",
    "\n",
    "**h)** Describa detalladamente las propiedades de muestra finita de la estimación de las varianzas de los coeficientes de MCC robustas ante la presencia de heterocedasticidad (con el procedimiento de White) de acuerdo a lo que observado en los puntos anteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios 2.2.a-g, diseño 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f38d4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f38d4_level0_col0\" class=\"col_heading level0 col0\" >n_observations</th>\n",
       "      <th id=\"T_f38d4_level0_col1\" class=\"col_heading level0 col1\" >bias_var_beta0</th>\n",
       "      <th id=\"T_f38d4_level0_col2\" class=\"col_heading level0 col2\" >bias_var_beta1</th>\n",
       "      <th id=\"T_f38d4_level0_col3\" class=\"col_heading level0 col3\" >bias_var_beta2</th>\n",
       "      <th id=\"T_f38d4_level0_col4\" class=\"col_heading level0 col4\" >bias_var_total</th>\n",
       "      <th id=\"T_f38d4_level0_col5\" class=\"col_heading level0 col5\" >bias_var_beta0_pop</th>\n",
       "      <th id=\"T_f38d4_level0_col6\" class=\"col_heading level0 col6\" >bias_var_beta1_pop</th>\n",
       "      <th id=\"T_f38d4_level0_col7\" class=\"col_heading level0 col7\" >bias_var_pop_total</th>\n",
       "      <th id=\"T_f38d4_level0_col8\" class=\"col_heading level0 col8\" >bias_var_total_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f38d4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f38d4_row0_col0\" class=\"data row0 col0\" >20</td>\n",
       "      <td id=\"T_f38d4_row0_col1\" class=\"data row0 col1\" >-15.30%</td>\n",
       "      <td id=\"T_f38d4_row0_col2\" class=\"data row0 col2\" >-18.35%</td>\n",
       "      <td id=\"T_f38d4_row0_col3\" class=\"data row0 col3\" >-24.16%</td>\n",
       "      <td id=\"T_f38d4_row0_col4\" class=\"data row0 col4\" >57.81%</td>\n",
       "      <td id=\"T_f38d4_row0_col5\" class=\"data row0 col5\" >0.40%</td>\n",
       "      <td id=\"T_f38d4_row0_col6\" class=\"data row0 col6\" >1.09%</td>\n",
       "      <td id=\"T_f38d4_row0_col7\" class=\"data row0 col7\" >0.12%</td>\n",
       "      <td id=\"T_f38d4_row0_col8\" class=\"data row0 col8\" >1.60%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f38d4_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f38d4_row1_col0\" class=\"data row1 col0\" >60</td>\n",
       "      <td id=\"T_f38d4_row1_col1\" class=\"data row1 col1\" >-5.07%</td>\n",
       "      <td id=\"T_f38d4_row1_col2\" class=\"data row1 col2\" >-6.60%</td>\n",
       "      <td id=\"T_f38d4_row1_col3\" class=\"data row1 col3\" >-8.23%</td>\n",
       "      <td id=\"T_f38d4_row1_col4\" class=\"data row1 col4\" >19.90%</td>\n",
       "      <td id=\"T_f38d4_row1_col5\" class=\"data row1 col5\" >0.07%</td>\n",
       "      <td id=\"T_f38d4_row1_col6\" class=\"data row1 col6\" >-0.31%</td>\n",
       "      <td id=\"T_f38d4_row1_col7\" class=\"data row1 col7\" >-0.12%</td>\n",
       "      <td id=\"T_f38d4_row1_col8\" class=\"data row1 col8\" >0.49%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f38d4_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f38d4_row2_col0\" class=\"data row2 col0\" >100</td>\n",
       "      <td id=\"T_f38d4_row2_col1\" class=\"data row2 col1\" >-3.31%</td>\n",
       "      <td id=\"T_f38d4_row2_col2\" class=\"data row2 col2\" >-3.80%</td>\n",
       "      <td id=\"T_f38d4_row2_col3\" class=\"data row2 col3\" >-4.93%</td>\n",
       "      <td id=\"T_f38d4_row2_col4\" class=\"data row2 col4\" >12.04%</td>\n",
       "      <td id=\"T_f38d4_row2_col5\" class=\"data row2 col5\" >-0.17%</td>\n",
       "      <td id=\"T_f38d4_row2_col6\" class=\"data row2 col6\" >0.04%</td>\n",
       "      <td id=\"T_f38d4_row2_col7\" class=\"data row2 col7\" >-0.17%</td>\n",
       "      <td id=\"T_f38d4_row2_col8\" class=\"data row2 col8\" >0.38%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f38d4_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f38d4_row3_col0\" class=\"data row3 col0\" >200</td>\n",
       "      <td id=\"T_f38d4_row3_col1\" class=\"data row3 col1\" >-1.82%</td>\n",
       "      <td id=\"T_f38d4_row3_col2\" class=\"data row3 col2\" >-2.06%</td>\n",
       "      <td id=\"T_f38d4_row3_col3\" class=\"data row3 col3\" >-2.63%</td>\n",
       "      <td id=\"T_f38d4_row3_col4\" class=\"data row3 col4\" >6.51%</td>\n",
       "      <td id=\"T_f38d4_row3_col5\" class=\"data row3 col5\" >-0.31%</td>\n",
       "      <td id=\"T_f38d4_row3_col6\" class=\"data row3 col6\" >-0.17%</td>\n",
       "      <td id=\"T_f38d4_row3_col7\" class=\"data row3 col7\" >-0.29%</td>\n",
       "      <td id=\"T_f38d4_row3_col8\" class=\"data row3 col8\" >0.77%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f38d4_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f38d4_row4_col0\" class=\"data row4 col0\" >400</td>\n",
       "      <td id=\"T_f38d4_row4_col1\" class=\"data row4 col1\" >-0.80%</td>\n",
       "      <td id=\"T_f38d4_row4_col2\" class=\"data row4 col2\" >-0.88%</td>\n",
       "      <td id=\"T_f38d4_row4_col3\" class=\"data row4 col3\" >-1.02%</td>\n",
       "      <td id=\"T_f38d4_row4_col4\" class=\"data row4 col4\" >2.69%</td>\n",
       "      <td id=\"T_f38d4_row4_col5\" class=\"data row4 col5\" >-0.03%</td>\n",
       "      <td id=\"T_f38d4_row4_col6\" class=\"data row4 col6\" >0.04%</td>\n",
       "      <td id=\"T_f38d4_row4_col7\" class=\"data row4 col7\" >0.14%</td>\n",
       "      <td id=\"T_f38d4_row4_col8\" class=\"data row4 col8\" >0.22%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f38d4_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f38d4_row5_col0\" class=\"data row5 col0\" >600</td>\n",
       "      <td id=\"T_f38d4_row5_col1\" class=\"data row5 col1\" >-0.57%</td>\n",
       "      <td id=\"T_f38d4_row5_col2\" class=\"data row5 col2\" >-0.71%</td>\n",
       "      <td id=\"T_f38d4_row5_col3\" class=\"data row5 col3\" >-0.81%</td>\n",
       "      <td id=\"T_f38d4_row5_col4\" class=\"data row5 col4\" >2.08%</td>\n",
       "      <td id=\"T_f38d4_row5_col5\" class=\"data row5 col5\" >-0.06%</td>\n",
       "      <td id=\"T_f38d4_row5_col6\" class=\"data row5 col6\" >-0.07%</td>\n",
       "      <td id=\"T_f38d4_row5_col7\" class=\"data row5 col7\" >-0.01%</td>\n",
       "      <td id=\"T_f38d4_row5_col8\" class=\"data row5 col8\" >0.14%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2121754e900>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se generan 5.000 muestras para n = 20, 60, 100, 200, 400, 600. Se fija beta_0 = beta_1 = beta_1 = 1, y diseño de errores normales y heterocedásticos. \n",
    "# Se reporta el sesto relativo y el sesgo relativo total en la estimación de las varianzas de beta_0, beta_1 y beta_2, comparando White con residuos y White con errores vs. Poblacional.\n",
    "df_10 = white_vcov_correction(n_samples=5000, n_observations_list=[20, 60, 100, 200, 400, 600], beta_0_true=1, beta_1_true=1, beta_2_true=1, design=1)\n",
    "df_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicios 2.2.a-g, diseño 2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_f858a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f858a_level0_col0\" class=\"col_heading level0 col0\" >n_observations</th>\n",
       "      <th id=\"T_f858a_level0_col1\" class=\"col_heading level0 col1\" >bias_var_beta0</th>\n",
       "      <th id=\"T_f858a_level0_col2\" class=\"col_heading level0 col2\" >bias_var_beta1</th>\n",
       "      <th id=\"T_f858a_level0_col3\" class=\"col_heading level0 col3\" >bias_var_beta2</th>\n",
       "      <th id=\"T_f858a_level0_col4\" class=\"col_heading level0 col4\" >bias_var_total</th>\n",
       "      <th id=\"T_f858a_level0_col5\" class=\"col_heading level0 col5\" >bias_var_beta0_pop</th>\n",
       "      <th id=\"T_f858a_level0_col6\" class=\"col_heading level0 col6\" >bias_var_beta1_pop</th>\n",
       "      <th id=\"T_f858a_level0_col7\" class=\"col_heading level0 col7\" >bias_var_pop_total</th>\n",
       "      <th id=\"T_f858a_level0_col8\" class=\"col_heading level0 col8\" >bias_var_total_pop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f858a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f858a_row0_col0\" class=\"data row0 col0\" >20</td>\n",
       "      <td id=\"T_f858a_row0_col1\" class=\"data row0 col1\" >-15.01%</td>\n",
       "      <td id=\"T_f858a_row0_col2\" class=\"data row0 col2\" >-18.85%</td>\n",
       "      <td id=\"T_f858a_row0_col3\" class=\"data row0 col3\" >-24.71%</td>\n",
       "      <td id=\"T_f858a_row0_col4\" class=\"data row0 col4\" >58.56%</td>\n",
       "      <td id=\"T_f858a_row0_col5\" class=\"data row0 col5\" >0.50%</td>\n",
       "      <td id=\"T_f858a_row0_col6\" class=\"data row0 col6\" >0.05%</td>\n",
       "      <td id=\"T_f858a_row0_col7\" class=\"data row0 col7\" >-2.14%</td>\n",
       "      <td id=\"T_f858a_row0_col8\" class=\"data row0 col8\" >2.69%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f858a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f858a_row1_col0\" class=\"data row1 col0\" >60</td>\n",
       "      <td id=\"T_f858a_row1_col1\" class=\"data row1 col1\" >-4.91%</td>\n",
       "      <td id=\"T_f858a_row1_col2\" class=\"data row1 col2\" >-6.29%</td>\n",
       "      <td id=\"T_f858a_row1_col3\" class=\"data row1 col3\" >-8.66%</td>\n",
       "      <td id=\"T_f858a_row1_col4\" class=\"data row1 col4\" >19.86%</td>\n",
       "      <td id=\"T_f858a_row1_col5\" class=\"data row1 col5\" >0.32%</td>\n",
       "      <td id=\"T_f858a_row1_col6\" class=\"data row1 col6\" >0.04%</td>\n",
       "      <td id=\"T_f858a_row1_col7\" class=\"data row1 col7\" >-0.78%</td>\n",
       "      <td id=\"T_f858a_row1_col8\" class=\"data row1 col8\" >1.14%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f858a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_f858a_row2_col0\" class=\"data row2 col0\" >100</td>\n",
       "      <td id=\"T_f858a_row2_col1\" class=\"data row2 col1\" >-2.86%</td>\n",
       "      <td id=\"T_f858a_row2_col2\" class=\"data row2 col2\" >-3.87%</td>\n",
       "      <td id=\"T_f858a_row2_col3\" class=\"data row2 col3\" >-4.21%</td>\n",
       "      <td id=\"T_f858a_row2_col4\" class=\"data row2 col4\" >10.94%</td>\n",
       "      <td id=\"T_f858a_row2_col5\" class=\"data row2 col5\" >0.34%</td>\n",
       "      <td id=\"T_f858a_row2_col6\" class=\"data row2 col6\" >0.09%</td>\n",
       "      <td id=\"T_f858a_row2_col7\" class=\"data row2 col7\" >0.73%</td>\n",
       "      <td id=\"T_f858a_row2_col8\" class=\"data row2 col8\" >1.16%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f858a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_f858a_row3_col0\" class=\"data row3 col0\" >200</td>\n",
       "      <td id=\"T_f858a_row3_col1\" class=\"data row3 col1\" >-1.73%</td>\n",
       "      <td id=\"T_f858a_row3_col2\" class=\"data row3 col2\" >-1.96%</td>\n",
       "      <td id=\"T_f858a_row3_col3\" class=\"data row3 col3\" >-2.13%</td>\n",
       "      <td id=\"T_f858a_row3_col4\" class=\"data row3 col4\" >5.82%</td>\n",
       "      <td id=\"T_f858a_row3_col5\" class=\"data row3 col5\" >-0.22%</td>\n",
       "      <td id=\"T_f858a_row3_col6\" class=\"data row3 col6\" >-0.08%</td>\n",
       "      <td id=\"T_f858a_row3_col7\" class=\"data row3 col7\" >0.23%</td>\n",
       "      <td id=\"T_f858a_row3_col8\" class=\"data row3 col8\" >0.53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f858a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_f858a_row4_col0\" class=\"data row4 col0\" >400</td>\n",
       "      <td id=\"T_f858a_row4_col1\" class=\"data row4 col1\" >-0.81%</td>\n",
       "      <td id=\"T_f858a_row4_col2\" class=\"data row4 col2\" >-0.88%</td>\n",
       "      <td id=\"T_f858a_row4_col3\" class=\"data row4 col3\" >-0.89%</td>\n",
       "      <td id=\"T_f858a_row4_col4\" class=\"data row4 col4\" >2.59%</td>\n",
       "      <td id=\"T_f858a_row4_col5\" class=\"data row4 col5\" >-0.03%</td>\n",
       "      <td id=\"T_f858a_row4_col6\" class=\"data row4 col6\" >0.05%</td>\n",
       "      <td id=\"T_f858a_row4_col7\" class=\"data row4 col7\" >0.32%</td>\n",
       "      <td id=\"T_f858a_row4_col8\" class=\"data row4 col8\" >0.40%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f858a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_f858a_row5_col0\" class=\"data row5 col0\" >600</td>\n",
       "      <td id=\"T_f858a_row5_col1\" class=\"data row5 col1\" >-0.57%</td>\n",
       "      <td id=\"T_f858a_row5_col2\" class=\"data row5 col2\" >-0.56%</td>\n",
       "      <td id=\"T_f858a_row5_col3\" class=\"data row5 col3\" >-0.72%</td>\n",
       "      <td id=\"T_f858a_row5_col4\" class=\"data row5 col4\" >1.85%</td>\n",
       "      <td id=\"T_f858a_row5_col5\" class=\"data row5 col5\" >-0.05%</td>\n",
       "      <td id=\"T_f858a_row5_col6\" class=\"data row5 col6\" >0.09%</td>\n",
       "      <td id=\"T_f858a_row5_col7\" class=\"data row5 col7\" >0.08%</td>\n",
       "      <td id=\"T_f858a_row5_col8\" class=\"data row5 col8\" >0.22%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x212174f06b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se generan 5.000 muestras para n = 20, 60, 100, 200, 400, 600. Se fija beta_0 = beta_1 = beta_1 = 1, y diseño de errores no-normales y heterocedásticos. \n",
    "# Se reporta el sesto relativo y el sesgo relativo total en la estimación de las varianzas de beta_0, beta_1 y beta_2, comparando White con residuos y White con errores vs. Poblacional.\n",
    "df_11 = white_vcov_correction(n_samples=5000, n_observations_list=[20, 60, 100, 200, 400, 600], beta_0_true=1, beta_1_true=1, beta_2_true=1, design=2)\n",
    "df_11"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
